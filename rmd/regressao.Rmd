# **Regressão** {#regressao}

<br>

## Análise de regressão

<br>

### O que é regressão?

-   Regressão refere-se a uma equação matemática que permite que uma ou mais variável(is) de desfecho (dependentes) seja(m) prevista(s) a partir de uma ou mais variável(is) independente(s). A regressão implica em uma direção de efeito, mas não garante causalidade.[@greenhalgh1997a]

-   Para estimar os efeitos imparciais de um fator de exposição primária sobre uma variável de desfecho, frequentemente constroem-se modelos estatísticos de regressão.[@bandoli2018]

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *modelsummary*[@modelsummary] fornece as funções [*modelsummary*](https://www.rdocumentation.org/packages/modelsummary/versions/1.4.1/topics/modelsummary) e [*modelplot*](https://www.rdocumentation.org/packages/modelsummary/versions/1.4.1/topics/modelplot) para gerar tabelas e gráficos de coeficientes de regressão.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *gtsummary*[@gtsummary] fornece a função [*tbl_regression*](https://www.rdocumentation.org/packages/gtsummary/versions/1.6.3/topics/tbl_regression) para construção da 'Tabela 2' com dados do modelo de regressão.
:::

<br>

### Quais são os algoritmos de regressão?

-   Linear.[@REF]

-   Nào-linear.[@REF]

-   Polinomial.[@REF]

-   Ridge.[@REF]

-   Lasso.[@REF]

<br>

### O que são análises de regressão simples?

-   A análise de regressão simples consiste em modelos estatísticos com 1 variável dependente (desfecho) e 1 variável independente (preditor).[@Hidalgo2013]

-   A equação de regressão simples é expressa como \@ref(eq:regressao-simples), onde $Y$ é a variável dependente, $X$ é a variável independente, $\beta_0$ é o intercepto (constante), $\beta_1$ é o coeficiente de regressão da variável independente e $\epsilon$ representa o erro aleatório do modelo.[@Hidalgo2013]

<br>

\begin{equation}
(\#eq:regressao-simples)
Y = \beta_0 + \beta_1 X + \epsilon
\end{equation}

<br>

### O que são análises de regressão multivariável?

-   A análise multivariável (ou múltiplo) consiste em modelos estatísticos com 1 variável dependente (desfecho) e duas ou mais variáveis independentes.[@Hidalgo2013]

-   A equação de regressão multivariável é expressa como \@ref(eq:regressao-multivariavel), onde $Y$ é a variável dependente, $X_1, X_2, ..., X_n$ são as variáveis independentes, $\beta_0$ é o intercepto (constante), $\beta_1, \beta_2, ..., \beta_n$ são os coeficientes de regressão das variáveis independentes e $\epsilon$ representa o erro aleatório do modelo.[@Hidalgo2013]

<br>

\begin{equation}
(\#eq:regressao-multivariavel)
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n + \epsilon
\end{equation}

<br>

### O que são análises de regressão multivariada?

-   A análise multivariada consiste em modelos estatísticos com 2 ou mais variáveis dependente (desfechos) e duas ou mais variáveis independentes.[@Hidalgo2013]

-   A equação de regressão multivariada é expressa como \@ref(eq:regressao-multivariada), onde $Y_1, Y_2, ..., Y_m$ são as variáveis dependentes, $X_1, X_2, ..., X_n$ são as variáveis independentes, $\beta_{0j}$ é o intercepto (constante) da variável dependente $Y_j$, $\beta_{ij}$ são os coeficientes de regressão das variáveis independentes para a variável dependente $Y_j$ e $\epsilon_j$ representa o erro aleatório do modelo para a variável dependente $Y_j$.[@Hidalgo2013]

<br>

\begin{align}
(\#eq:regressao-multivariada)
Y_1 &= \beta_{01} + \beta_{11} X_1 + \beta_{12} X_2 + \dots + \beta_{1n} X_n + \epsilon_1 \\
Y_2 &= \beta_{02} + \beta_{21} X_1 + \beta_{22} X_2 + \dots + \beta_{2n} X_n + \epsilon_2 \\
&\vdots \\
Y_m &= \beta_{0m} + \beta_{m1} X_1 + \beta_{m2} X_2 + \dots + \beta_{mn} X_n + \epsilon_m
\end{align}

<br>

### O que são análises de regressão linear?

-   .[@REF]

<br>

```{r regressao-linear, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Regressão linear."}
# simulate X
set.seed(123)
X <- rnorm(100, mean = 50, sd = 10)

# simulate Y based on X
Y <- 5 + 2 * X + rnorm(100, mean = 0, sd = 30)

# create a dataframe
data <- data.frame(X, Y)

# create the regression model
model <- lm(Y ~ X, data = data)

ggplot2::ggplot(data, ggplot2::aes(x = X, y = Y)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm",
                       color = "blue",   # cor da linha
                       fill = "grey70",  # cor da área IC95%
                       se = TRUE) +      # mostra o intervalo de confiança
  ggplot2::labs(title = "Regressão linear", 
                x = "Variável Independente (X)", 
                y = "Variável Dependente (Y)") +
  ggplot2::theme_minimal()
```

<br>

### O que são análises de regressão não-linear?

-   .[@REF]

<br>

```{r regressao-nao-linear, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Regressão não-linear."}
# simulate X
set.seed(123)
X <- seq(-10, 10, length.out = 100)

# simulate Y based on a non-linear function of X
Y <- 5 + 2 * X^2 + rnorm(100, mean = 0, sd = 30)

# create a dataframe
data <- data.frame(X, Y)

# create the regression model
model <- lm(Y ~ poly(X, 2), data = data)

# plot the regression line
ggplot2::ggplot(data, ggplot2::aes(x = X, y = Y)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm",
                       formula = y ~ poly(x, 2),
                       color = "blue",
                       fill = "grey70", # cor da área do IC
                       se = TRUE) +     # ativa o intervalo de confiança
  ggplot2::labs(title = "Regressão não-linear", 
                x = "Variável Independente (X)", 
                y = "Variável Dependente (Y)") +
  ggplot2::theme_minimal()
```

<br>

### O que são análises de regressão polinomial?

-   .[@REF]

<br>

```{r regressao-polinomial, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Regressão polinomial."}
# simulate X
set.seed(123)
X <- seq(-10, 10, length.out = 100)

# simulate Y based on a polynomial function of X
Y <- 5 + 2 * X^3 - 0.1 * X^2 + rnorm(100, mean = 0, sd = 30)

# create a dataframe
data <- data.frame(X, Y)

# create the regression model
model <- lm(Y ~ poly(X, 3), data = data)

ggplot2::ggplot(data, ggplot2::aes(x = X, y = Y)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm",
                       formula = y ~ poly(x, 3),
                       color = "blue",      # cor da linha
                       fill = "grey70",     # cor da faixa IC95%
                       se = TRUE) +         # ativa o intervalo de confiança
  ggplot2::labs(title = "Regressão polinomial",
                x = "Variável Independente (X)",
                y = "Variável Dependente (Y)") +
  ggplot2::theme_minimal()
```

<br>

### O que são análises de regressão ridge?

-   .[@REF]

<br>

```{r regressao-ridge, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Regressão ridge."}
# simulate X
set.seed(123)
X <- seq(-10, 10, length.out = 100)
Y <- 5 + 2 * X + rnorm(100, mean = 0, sd = 5)
data <- data.frame(X, Y)

# criar matriz de preditores com intercepto
X_matrix <- model.matrix(~ X, data = data)

# ajustar modelo de regressão ridge
model <- glmnet::glmnet(X_matrix, data$Y, alpha = 0)

# predições (usando lambda padrão)
predicted_Y <- predict(model, newx = X_matrix, s = 0.01)

# adicionar ao dataframe
data$Y_pred <- as.numeric(predicted_Y)

# plotar com ggplot
ggplot2::ggplot(data, ggplot2::aes(x = X, y = Y)) +
  ggplot2::geom_point(color = "gray40") +
  ggplot2::geom_line(ggplot2::aes(y = Y_pred), color = "blue", size = 1) +
  ggplot2::labs(title = "Regressão Ridge",
       x = "Variável Independente (X)",
       y = "Variável Dependente (Y)") +
  ggplot2::theme_minimal()
```

<br>

### O que são análises de regressão logística?

-   .[@REF]

<br>

```{r regressao-logistica, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Regressão logística."}
# simulate X
set.seed(123)
X <- rnorm(100, mean = 50, sd = 10)

# simulate a binary outcome Y based on X
prob <- 1 / (1 + exp(-( -5 + 0.1 * X)))  # logistic function
Y <- rbinom(100, size = 1, prob = prob) # sample Y ~ Bernoulli(prob)

# create a dataframe
data <- data.frame(X, Y)

# create the logistic regression model
model <- glm(Y ~ X, data = data, family = binomial)

# plot the regression line
ggplot2::ggplot(data, ggplot2::aes(x = X, y = Y)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "glm", method.args = list(family = "binomial"), color = "blue") +
  ggplot2::labs(title = "Regressão logística",
       x = "Variável Independente (X)",
       y = "Probabilidade de Sucesso (Y)") +
  ggplot2::theme_minimal()
```

<br>

## Preparação de variáveis para regressão

<br>

### Como preparar as variáveis categóricas para análise de regressão?

-   Variáveis fictícias (*dummy*) compreendem variáveis criadas para introduzir, nos modelos de regressão, informações contidas em outras variáveis que não podem ser medidas em escala numérica.[@suits1957]

-   Variáveis categóricas nominais, com 2 ou mais níveis, devem ser subdivididas em variáveis fictícias dicotômicas para ser usada em modelos de regressão.[@Healy1995]

-   Cada nível da variável categórica nominal será convertido em uma nova variável fictícias dicotômica, tal que a nova variável dicotômica assume valor 1 para a presença do nível correspondente e 0 em qualquer outro caso.[@Healy1995]

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *fastDummies*[@fastDummies] fornece a função [*dummy_cols*](https://www.rdocumentation.org/packages/fastDummies/versions/1.7.3/topics/dummy_columns) para preparar as variáveis categóricas fictícias para análise de regressão.
:::

<br>

### Por que é comum escolher a categoria mais frequente como referência em modelos epidemiológicos?

- Maior estabilidade estatística: a categoria mais frequente costuma gerar estimativas mais estáveis, com menor erro padrão nos coeficientes das demais categorias.[@REF]

- A escolha da referência não altera o ajuste nem o valor predito pelo modelo — apenas muda o ponto de comparação.[@REF]

<br>

## Multicolinearidade

<br>

### O que é multicolinearidade?

-   Multicolinearidade representa a intercorrelação entre as variáveis independentes (explanatórias) de um modelo.[@Kim2019]

<br>

### Como diagnosticar multicolinearidade de forma quantitativa?

- Verifique a existência de multicolinearidade entre as variáveis candidatas.[@Sun1996]

- O Coeficiente de determinação ($R^2$) é uma medida de quão bem as variáveis independentes explicam a variabilidade da variável dependente. Valores próximos a 1 indicam que as variáveis independentes estão fortemente correlacionadas entre si, o que pode indicar multicolinearidade.[@Kim2019]

-  O Fator de Inflação da Variância (*variance inflation factor*, VIF) é uma medida que quantifica o quanto a variância de um coeficiente de regressão é inflacionada devido à multicolinearidade. Valores de VIF maiores que 10 são frequentemente considerados indicativos de multicolinearidade significativa.[@Kim2019]

- O recíproco da VIF é chamado de Tolerância, que mede a proporção da variância de uma variável independente que não é explicada pelas outras variáveis independentes. Valores baixos de Tolerância (geralmente abaixo de 0.1) indicam multicolinearidade.[@Kim2019]

- O número de condições (*Condition Number*) é uma medida que avalia a estabilidade numérica de um modelo de regressão. Valores altos (entre 10 de 30) indicam multicolinearidade, e valores maiores que 30 indicam forte multicolinearidade.[@Kim2019]

<br>

```{r multicolinearidade, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Multicolinearidade entre variáveis candidatas em modelos de regressão multivariável."}
# Set the seed for reproducibility
set.seed(123)

# Generate an independent variable
X1 <- rnorm(100, mean = 50, sd = 10)

# Generate a second variable that is highly correlated with X1
X2 <- 0.8 * X1 + rnorm(100, mean = 0, sd = 5)

# Generate a third variable that is not correlated with X1 or X2
X3 <- rnorm(100, mean = 30, sd = 5)

# Combine the variables into a dataframe
data <- data.frame(X1, X2, X3)

# Create the customized grid plot
GGally::ggpairs(
  data, 
  lower = list(continuous = GGally::wrap("smooth", method = "lm")),
  diag = list(continuous = "densityDiag"),
  upper = list(continuous = "cor")
) +
  ggplot2::theme(
    panel.background = ggplot2::element_blank(),
    panel.grid.major = ggplot2::element_blank(),
    panel.grid.minor = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(color = "black")
  )
```

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *GGally*[@GGally] fornece a função [*ggpairs*](https://www.rdocumentation.org/packages/GGally/versions/2.2.1/topics/ggpairs) para criar uma matriz gráfica de correlações bivariadas.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *car*[@car] fornece a função [*vif*](https://www.rdocumentation.org/packages/car/versions/3.1-3/topics/vif) para calcular o fator de inflação da variância (VIF).
:::

<br>

### O que fazer em caso de multicolinearidade elevada?

- Verifique a transformação (codificação) de variáveis numéricas em categóricas.[@Kim2019]

- Aumente o tamanho da amostra, se possível, para reduzir a multicolinearidade.[@Kim2019]

- Combine níveis de variáveis categóricas com baixa frequência de ocorrência.[@Kim2019]

- Combine variáveis numéricas altamente correlacionadas em uma única variável composta, como a média ou soma das variáveis.[@Kim2019] 

- Considere a exclusão de variáveis altamente correlacionadas do modelo, especialmente se elas não forem essenciais para a análise.[@Kim2019]

- Use técnicas de seleção de variáveis, como seleção passo a passo, para identificar e remover variáveis redundantes.[@Kim2019]

- Use técnicas de regularização, como regressão ridge ou lasso, que podem lidar com multicolinearidade ao penalizar coeficientes de regressão.[@Kim2019]

<br>

## Redução de dimensionalidade

<br>

### Correlação bivariada pode ser usada para seleção de variáveis em modelos de regressão multivariável?

-   Seleção bivariada de variáveis - isto é, aplicação de testes de correlação em pares de variáveis candidatas e variável de desfecho afim de selecionar quais serão incluídas no modelo multivariável - é um dos erros mais comuns na literatura.[@Dales1978; @Sun1996; @heinze2016]

-   A seleção bivariada de variáveis torna o modelo mais suscetível a otimismo no ajuste se as variáveis de confundimento não são adequadamente controladas.[@Dales1978; @Sun1996]

<br>

### Variáveis sem significância estatística devem ser excluídas do modelo final?

-   Eliminar uma variável de um modelo significa anular o seu coeficiente de regressão ($\beta = 0$), mesmo que o valor estimado pelos dados seja outro. Desta forma, os resultados se afasTAM de uma solução de máxima verossimilhança (que tem fundamento teórico) e o modelo resultante é intencionalmente subótimo.[@heinze2016]

-   Os coeficientes de regressão geralmente dependem do conjunto de variáveis do modelo e, portanto, podem mudam de valor ("mudança na estimativa" positiva ou negativa) se uma (ou mais) variável(is) for(em) eliminada(s) do modelo.[@heinze2016]

<br>

### Por que métodos de regressão gradual não são recomendados para seleção de variáveis em modelos de regressão multivariável?

-   Métodos diferentes de regressão gradual podem produzir diferentes seleções de variáveis de um mesmo banco de dados.[@Healy1995]

-   Nenhum método de regressão gradual garante a seleção ótima de variáveis de um banco de dados.[@Healy1995]

-   As regras de término da regressão baseadas em P-valor tendem a ser arbitrárias.[@Healy1995]

<br>

### O que pode ser feito para reduzir o número de variáveis candidatas em modelos de regressão multivariável?

-   Em caso de uma proporção baixa entre o número de participantes e de variáveis, use o conhecimento prévio da literatura para selecionar um pequeno conjunto de variáveis candidatas.[@Sun1996]

-   Colapse categorias com contagem nula (células com valor igual a 0) de variáveis candidatas.[@Sun1996]

-   Use simulações de dados para identificar qual(is) variável(is) está(ão) causando problemas de convergência do ajuste do modelo.[@Sun1996]

-   A eliminação retroativa tem sido recomendada como a abordagem de regressão gradual mais confiável entre aquelas que podem ser facilmente alcançadas com programas de computador.[@heinze2016]

<br>

### Quando devemos forçar uma variável no modelo?

- Sempre que houver base teórica ou evidência prévia forte (por exemplo, idade em estudos de câncer), ou se for a variável de exposição principal.[@Greenland1989]

<br>

## Suposições dos modelos

<br>

### Quais suposições são feitas para regressão?

-   As suposições dos modelos de regressão incluem linearidade, independência, homocedasticidade, normalidade dos resíduos e ausência de multicolinearidade.[@REF]

<br>

### Como avaliar as suposições de uma regressão?

- Usando diagnóstico de regressão (ex.: análise de resíduos, gráficos de valores observados vs. preditos) e comparação com análises estratificadas.[@Greenland1989]

<br>

```{r suposicoes, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Diagnóstico de regressão para avaliar suposições do modelo: linearidade, normalidade dos resíduos, homocedasticidade e alavancagem."}
# Definir semente
set.seed(123)

# Simular dados
n <- 100
x1 <- stats::rnorm(n, mean = 50, sd = 10)
x2 <- stats::rnorm(n, mean = 30, sd = 5)
erro <- stats::rnorm(n, mean = 0, sd = 5)
y <- 10 + 0.5 * x1 - 0.3 * x2 + erro

# Data frame
dados <- data.frame(y = y, x1 = x1, x2 = x2)

# Modelo
modelo <- stats::lm(y ~ x1 + x2, data = dados)

# Dados auxiliares para gráficos
dados_diag <- dados %>%
  dplyr::mutate(
    Ajustado = stats::fitted(modelo),
    Resíduo = stats::resid(modelo),
    Resíduo_pad = stats::rstandard(modelo),
    Raiz_Resíduo_pad = sqrt(abs(Resíduo_pad)),
    Cook = stats::cooks.distance(modelo),
    Alavancagem = stats::hatvalues(modelo)
  )

# Gráfico 1: Resíduos vs Ajustados
g1 <- ggplot2::ggplot(dados_diag, ggplot2::aes(x = Ajustado, y = Resíduo)) +
  ggplot2::geom_point(color = "blue") +
  ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  ggplot2::labs(title = "Resíduos vs Ajustados", subtitle = "--- Resíduo = 0",
                x = "Valores Ajustados", y = "Resíduos") +
  ggplot2::theme_minimal()

# Gráfico 2: QQ Plot
g2 <- ggplot2::ggplot(dados_diag, ggplot2::aes(sample = Resíduo)) +
  ggplot2::stat_qq(color = "darkgreen") +
  ggplot2::stat_qq_line(color = "black", linetype = "dashed") +
  ggplot2::labs(title = "Normal Q-Q", subtitle = "--- Distribuição Normal Teórica",
                x = "Quantis Teóricos", y = "Resíduos") +
  ggplot2::theme_minimal()

# Gráfico 3: Scale-Location
g3 <- ggplot2::ggplot(dados_diag, ggplot2::aes(x = Ajustado, y = Raiz_Resíduo_pad)) +
  ggplot2::geom_point(color = "orange") +
  ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  ggplot2::labs(title = "Scale-Location", subtitle = "--- Referência visual",
                x = "Valores Ajustados", y = "√|Resíduos Padronizados|") +
  ggplot2::theme_minimal()

# Gráfico 4: Distância de Cook
g4 <- ggplot2::ggplot(dados_diag, ggplot2::aes(x = seq_along(Cook), y = Cook)) +
  ggplot2::geom_segment(ggplot2::aes(xend = seq_along(Cook), yend = 0), color = "purple") +
  ggplot2::geom_hline(yintercept = 4/n, linetype = "dashed", color = "black") +
  ggplot2::labs(title = "Distância de Cook", subtitle = "--- Limite sugerido (4/n)",
                x = "Observação", y = "Distância") +
  ggplot2::theme_minimal()

# Gráfico 5: Observado vs Ajustado
lim <- range(c(dados_diag$y, dados_diag$Ajustado))
g5 <- ggplot2::ggplot(dados_diag, ggplot2::aes(x = y, y = Ajustado)) +
  ggplot2::geom_point(color = "darkblue") +
  ggplot2::geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  ggplot2::coord_cartesian(xlim = lim, ylim = lim) +
  ggplot2::labs(title = "Observado vs Ajustado", subtitle = "--- y = x",
                x = "Valores Observados", y = "Valores Ajustados") +
  ggplot2::theme_minimal()

# Gráfico 6: Alavancagem
g6 <- ggplot2::ggplot(dados_diag, ggplot2::aes(x = seq_along(Alavancagem), y = Alavancagem)) +
  ggplot2::geom_point(color = "darkred") +
  ggplot2::geom_hline(yintercept = 2 * mean(dados_diag$Alavancagem), linetype = "dashed", color = "black") +
  ggplot2::labs(title = "Alavancagem", subtitle = "--- 2 × média",
                x = "Observação", y = "Valores de Alavancagem") +
  ggplot2::theme_minimal()

# Organizar os 6 gráficos em 2x3
gridExtra::grid.arrange(g1, g2, g3, g4, g5, g6, nrow = 2)
```

<br>

```{r, echo=FALSE, warning=FALSE, results='asis', eval=knitr::is_html_output()}
cat(readLines("citation.html"), sep = "\n")
```

<br>
