```{=openxml}
<w:p>
  <w:r>
    <w:br w:type="page"/>
  </w:r>
</w:p>
```

# **Inteligência artificial** {#inteligencia-artificial}

<br>

## Inteligência artificial

<br>

### O que é inteligência artificial (IA)?

-   .[@REF]

<br>

### Como ela se relaciona com estatística, ciência de dados e aprendizado de máquina?

-   .[@REF]

<br>

## Inteligência artificial generativa

<br>

### O que é IA generativa?

-   .[@REF]

<br>

### O que são grandes modelos de linguagem (*large language models*, LLM)?

-   .[@REF]

<br>

### Como funcionam modelos os grandes modelos de linguagem?

-   .[@REF]

<br>

```{r llm, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Representação esquemática de um modelo de linguagem grande (LLM)"}
# Reprodutibilidade
set.seed(42)

# 1) Texto e tokenização
texto <- "Os modelos de linguagem grandes (LLMs) aprendem padrões e sugerem o próximo token."

# Tokenizador simples: palavras, números, abreviações e pontuação
tokens <- stringr::str_extract_all(texto, "[\\p{L}\\p{N}]+(?:['’][\\p{L}\\p{N}]+)?|[[:punct:]]", simplify = FALSE)[[1]]
tokens <- tokens[tokens != ""]
tokens_df <- tibble::tibble(idx = seq_along(tokens), token = tokens)

# 2) "Mini-LLM" ilustrativo
#    Embeddings aleatórios + atenção toy + softmax
# Vocabulário = tokens do texto + alguns candidatos de próxima palavra
candidatos <- c("e","então","porque","mas","gato","cachorro","token","modelo",".",",")
vocab <- unique(c(tokens, candidatos))

d <- 32  # dimensão do embedding (toy)
E <- matrix(rnorm(length(vocab) * d), nrow = length(vocab), ncol = d)
rownames(E) <- vocab

# Mapeia cada token ao seu embedding
emb_for <- function(tok) E[as.character(tok), , drop = FALSE]

# Contexto = média dos embeddings dos tokens já vistos
contexto <- apply(do.call(rbind, lapply(tokens, emb_for)), 2, mean)

# Logits ~ similaridade (produto interno) com o contexto
logits <- as.numeric(E %*% contexto)
names(logits) <- vocab

# Softmax com temperatura
softmax <- function(x, temp = 0.8) {
  z <- x / temp
  z <- z - max(z)
  exp_z <- exp(z)
  exp_z / sum(exp_z)
}
probs <- softmax(logits, temp = 0.8)
pred_df <- tibble::tibble(token = names(probs), prob = probs) |>
  dplyr::arrange(dplyr::desc(prob))

top_k <- 10
pred_top <- pred_df |> dplyr::slice(1:top_k) |>
  dplyr::mutate(token = factor(token, levels = rev(token)))

# 3) Atenção "toy"
#    Último token (query) atendendo aos anteriores (keys)
# Para a atenção, criamos Q e K a partir de E (projeções lineares aleatórias)
Wq <- matrix(rnorm(d * d), nrow = d, ncol = d)
Wk <- matrix(rnorm(d * d), nrow = d, ncol = d)

# Keys: um por token da sequência
K_mat <- do.call(rbind, lapply(tokens, function(tk) emb_for(tk) %*% Wk))
# Query: do último token
q_vec <- as.numeric((emb_for(tail(tokens, 1)) %*% Wq)[1, ])

# Pesos de atenção do último token para todos os anteriores
att_logits <- as.numeric(K_mat %*% q_vec) / sqrt(d)
att_w <- softmax(att_logits, temp = 1.0)

att_df <- tibble::tibble(idx = seq_along(tokens), token = tokens, att = att_w)

# 4) Figuras (3 painéis)

# Painel A: sequência de tokens
pal <- scales::hue_pal()(length(unique(tokens)))
tokens_df <- tokens_df |>
  dplyr::mutate(tok_id = as.integer(factor(token)),
                y = 1)

# Painel A: sequência de tokens (sem cores, apenas moldura preta)
g_seq <- ggplot2::ggplot(tokens_df, ggplot2::aes(xmin = idx - 0.5, xmax = idx + 0.5,
                               ymin = 0.5, ymax = 1.5)) +
  ggplot2::geom_rect(fill = "white", color = "black") +
  ggplot2::geom_text(ggplot2::aes(x = idx, y = 1, label = token), size = 3) +
  ggplot2::scale_x_continuous(breaks = tokens_df$idx) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL, limits = c(0.4, 1.6)) +
  ggplot2::labs(title = "A) Sequência de tokens (após tokenização)") +
  ggplot2::theme_minimal(base_size = 12) +
  ggplot2::theme(panel.grid = ggplot2::element_blank(),
        axis.title.x = ggplot2::element_blank(),
        axis.text.x = ggplot2::element_text(size = 8, vjust = 0.5))

# Painel B: atenção do último token para os anteriores
g_att <- ggplot2::ggplot(att_df, ggplot2::aes(x = idx, y = att)) +
  ggplot2::geom_col() +
  ggplot2::geom_text(ggplot2::aes(label = token), nudge_y = 0.02, size = 3, angle = 0) +
  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  ggplot2::scale_x_continuous(breaks = att_df$idx) +
  ggplot2::labs(title = paste0("B) Atenção do último token: \"", tail(tokens,1), "\""),
       x = "posição na sequência", y = "peso de atenção") +
  ggplot2::theme_minimal(base_size = 12)

# Painel C: top-k predições de próximo token (toy)
g_pred <- ggplot2::ggplot(pred_top, ggplot2::aes(x = token, y = prob)) +
  ggplot2::geom_col() +
  ggplot2::coord_flip() +
  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  ggplot2::labs(title = "C) Próximo token (top-10) – distribuição toy",
       x = NULL, y = "probabilidade") +
  ggplot2::theme_minimal(base_size = 12)

# Compor figura
gridExtra::grid.arrange(g_seq, g_att, g_pred, ncol = 1, heights = c(1.2, 1.2, 1.4))
```

<br>

## Inteligência artificial explicável (*eXplainable Artificial Intelligence, XAI)*

<br>

### Quais princípios são utilizados para descrever explicabilidade de IA?

- Explicabilidade: capacidade de revelar e resumir, em termos compreensíveis, os motivos por trás das decisões de um modelo, incluindo métodos post-hoc que explicam resultados após a predição.[@ali2023]

- Interpretabilidade: capacidade de compreender o funcionamento interno do modelo a partir de suas próprias estruturas, por meio de técnicas intrínsecas que tornam o processo decisório inteligível.[@ali2023]

- Transparência: geração de explicações claras e legíveis por humanos sobre as decisões do modelo, essencial para avaliar sua qualidade e resistir a usos adversariais.[@ali2023]

- Equidade: capacidade do modelo de tomar decisões imparciais, sem favorecer ou discriminar grupos, mitigando vieses associados a características sociais, econômicas ou demográficas.[@ali2023]

- Robustez: capacidade do modelo de manter desempenho estável diante de incertezas ou pequenas perturbações nos dados de entrada, incluindo ataques adversariais.[@ali2023]

- Satisfação: grau em que técnicas de explicabilidade aumentam a utilidade, a usabilidade e a aceitação do sistema baseado em aprendizado de máquina.[@ali2023]

- Estabilidade: capacidade de produzir explicações consistentes para entradas semelhantes, evitando variações arbitrárias nas interpretações.[@ali2023]

- Responsabilidade: alinhamento do modelo a valores sociais, éticos e morais, sustentado por transparência, responsabilização, equidade e ética, como base para uma IA responsável.[@ali2023]

<br>

### Por que explicar modelos de IA?

- Para tornar as decisões defensáveis: compreender como e por que um modelo chega a determinado resultado é essencial para avaliar a legitimidade de suas decisões e sustentar seu uso em contextos críticos.[@adadi2018; @vilone2021]

- Para tornar o sistema governável: explicações permitem enxergar o funcionamento interno do modelo, facilitando sua supervisão, auditoria, correção de erros e detecção de vieses ou fragilidades.[@adadi2018; @vilone2021]

- Para aperfeiçoar o desempenho: ao revelar como o modelo utiliza informações, as explicações orientam ajustes que aumentam sua precisão, eficiência e robustez.[@adadi2018; @vilone2021]

- Para ampliar o conhecimento: modelos explicáveis não apenas produzem previsões, mas também ajudam a identificar padrões, relações e hipóteses novas, contribuindo para a geração de conhecimento científico.[@adadi2018; @vilone2021]

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *keras*[\@keras](https://cran.r-project.org/web/packages/keras/index.html) possuu funções para criar, treinar e avaliar modelos de redes neurais.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *tensorflow*[\@tensorflow](https://cran.r-project.org/web/packages/tensorflow/index.html) fornece uma interface para o TensorFlow, uma biblioteca de código aberto amplamente utilizada para aprendizado de máquina e redes neurais.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *torch*[\@torch](https://cran.r-project.org/web/packages/torch/index.html) permite criar e treinar redes neurais com alto desempenho.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *reticulate*[\@reticulate](https://cran.r-project.org/web/packages/reticulate/index.html) integra R e Python em um mesmo ambiente de trabalho, permitindo chamar funções Python a partir de R e facilitar o uso de bibliotecas de IA disponíveis nesse ecossistema.
:::

<br>

## Limitações fundamentais de modelos generativos

<br>

### O que são alucinações em IA generativa?

- Alucinações em IA generativa ocorrem quando um modelo produz saídas linguisticamente plausíveis, porém factualmente incorretas, inexistentes ou não verificáveis, sem que haja erro numérico ou falha no processo de treinamento.[@REF]

<br>

```{r alucinacoes, echo=FALSE, warning=FALSE, message=FALSE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Funcionamento conceitual de modelos de linguagem e origem das alucinações. O painel A ilustra o fluxo abstrato de um LLM até a predição do próximo token; o painel B mostra a escolha baseada em plausibilidade linguística; o painel C evidencia que essa otimização probabilística pode gerar respostas fluentes, porém factualmente falsas."}
# reprodutibilidade
set.seed(42)

# Painel A — Pipeline conceitual do LLM
df_pipe <- data.frame(
  etapa = factor(
    c("Texto\nde entrada", "Tokenização", "Embeddings",
      "Atenção", "Predição do\npróximo token"),
    levels = c("Texto\nde entrada", "Tokenização",
               "Embeddings", "Atenção", "Predição do\npróximo token")
  ),
  x = 1:5,
  y = rep(1, 5)
)

p_pipeline <- ggplot2::ggplot(df_pipe, ggplot2::aes(x, y, label = etapa)) +
  ggplot2::geom_rect(ggplot2::aes(xmin = x - 0.45, xmax = x + 0.45,
                ymin = y - 0.25, ymax = y + 0.25),
            fill = "white", color = "black") +
  ggplot2::geom_text(size = 3.5) +
  ggplot2::geom_segment(ggplot2::aes(x = x + 0.45, xend = x + 0.55,
                   y = y, yend = y),
               arrow = ggplot2::arrow(length = unit(0.15, "cm")),
               linewidth = 0.6) +
  ggplot2::labs(title = "A) Funcionamento conceitual de um LLM") +
  ggplot2::theme_void() +
  ggplot2::theme(plot.title = element_text(face = "bold", size = 12))

# Painel B — Distribuição do próximo token
tokens <- c("descoberta", "modelo", "artigo", "estudo", "teoria")
prob <- c(0.38, 0.27, 0.18, 0.11, 0.06)

df_prob <- data.frame(token = tokens, prob = prob)

p_prob <- ggplot2::ggplot(df_prob, ggplot2::aes(x = reorder(token, prob), y = prob)) +
  ggplot2::geom_col(fill = "#3182BD") +
  ggplot2::coord_flip() +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(
    title = "B) Próximo token mais provável",
    subtitle = "Escolha baseada em plausibilidade linguística",
    x = NULL,
    y = "Probabilidade"
  ) +
  ggplot2::theme_minimal(base_size = 11) +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold"))

# Painel C — Espaço de alucinação
n <- 300
df_space <- data.frame(
  plausibilidade = runif(n),
  veracidade     = runif(n)
)

df_space$regiao <- ifelse(
  df_space$plausibilidade > 0.6 & df_space$veracidade <= 0.4,
  "Alucinação",
  "Outros"
)

exemplos <- data.frame(
  plausibilidade = c(0.85),
  veracidade     = c(0.20),
  texto = "Resposta fluente,\nmas factualmente falsa"
)

p_space <- ggplot2::ggplot(df_space, ggplot2::aes(plausibilidade, veracidade)) +
  ggplot2::geom_point(
    aes(color = regiao),
    alpha = 0.4, size = 2
  ) +
  ggplot2::geom_vline(xintercept = 0.6, linetype = "dashed") +
  ggplot2::geom_hline(yintercept = 0.6, linetype = "dashed") +
  ggplot2::geom_point(
    data = exemplos,
    ggplot2::aes(plausibilidade, veracidade),
    inherit.aes = FALSE,
    shape = 21, fill = "white", size = 4, stroke = 1.2
  ) +
  ggrepel::geom_text_repel(
    data = exemplos,
    aes(plausibilidade, veracidade, label = texto),
    inherit.aes = FALSE,
    size = 3.2
  ) +
  ggplot2::scale_color_manual(
    values = c("Alucinação" = "#D32F2F", "Outros" = "#9E9E9E")
  ) +
  ggplot2::labs(
    title = "C) Alucinação em modelos generativos",
    x = "Plausibilidade linguística",
    y = "Veracidade factual",
    color = NULL
  ) +
  ggplot2::theme_minimal(base_size = 11) +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold"),
        legend.position = "none")

# Figura final (3 painéis)
gridExtra::grid.arrange(
  p_pipeline,
  p_prob,
  p_space,
  ncol = 1,
  heights = c(1, 1.2, 1.6)
)
```

<br>

### Por que modelos generativos alucinam?

- Modelos generativos são treinados para maximizar a probabilidade do próximo token dado um contexto, e não para verificar fatos ou manter correspondência com a realidade externa.[@REF]

- O critério de otimização privilegia a verossimilhança estatística da linguagem, não a veracidade epistemológica do conteúdo.[@REF]

<br>

### Alucinações indicam erro do modelo?

- Alucinações não indicam que o modelo “aprendeu errado”, mas que ele aprendeu exatamente o que foi solicitado: produzir linguagem plausível, não conhecimento verdadeiro.[@REF]

- Do ponto de vista matemático, uma resposta alucinatória pode ser ótima segundo a função de perda, mesmo sendo incorreta no mundo real.[@REF]

<br>

### Qual a diferença entre erro estatístico e alucinação?

- Erros estatísticos decorrem de limitações de generalização, ruído nos dados ou inadequação do modelo ao problema.[@REF]

- Alucinações decorrem da ausência de mecanismos internos de checagem factual e constituem um fenômeno semântico e comunicacional, não um erro numérico.[@REF]

<br>

### Quais tipos de alucinação são mais comuns?

- Alucinação factual: afirmação de fatos inexistentes ou incorretos.[@REF]

- Alucinação de fonte: citação de autores, artigos ou documentos inexistentes.[@REF]

- Alucinação causal: inferência de relações de causa e efeito sem fundamento empírico.[@REF]

<br>

### Por que alucinações são um problema prático?

- Porque a fluência linguística do modelo pode induzir confiança indevida em informações erradas, especialmente em contextos científicos, clínicos, jurídicos e educacionais.[@REF]

<br>

### É possível eliminar completamente as alucinações?

- Até o momento, não. Alucinações são uma consequência estrutural do paradigma generativo probabilístico e podem apenas ser mitigadas, não eliminadas.[@REF]

- Estratégias como *prompting* controlado, ajuste de temperatura, recuperação de informações externas e validação pós-geração reduzem sua frequência, mas não suprimem o fenômeno.[@REF]

<br>

```{r, echo=FALSE, warning=FALSE, results='asis', eval=knitr::is_html_output()}
cat(readLines("citation.html"), sep = "\n")
```

<br>
