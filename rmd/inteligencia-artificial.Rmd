# **Inteligência artificial** {#inteligencia-artificial}

<br>

## Inteligência artificial

<br>

### O que é inteligência artificial (IA)?

-   .[@REF]

<br>

### Qual é a unidade básica de IA?

-   A unidade básica de IA é o neurônio artificial, que simula o comportamento de um neurônio biológico, recebendo entradas, aplicando pesos e uma função de ativação para produzir uma saída.[@REF]

-   A rede neural é uma coleção de neurônios artificiais organizados em camadas, onde cada camada processa informações e passa para a próxima, permitindo o aprendizado de padrões complexos.[@REF]

<br>

### Como ela se relaciona com estatística, ciência de dados e aprendizado de máquina?

-   .[@REF]

<br>

## Neurônios artificiais

<br>

### O que são neurônios artificiais?

-   Neuronios artificiais (ou perceptrons) são modelos matemáticos que imitam o funcionamento dos neurônios biológicos, recebendo entradas, aplicando pesos e uma função de ativação para produzir uma saída.[@mcculloch1943; @rosenblatt1958; @rosenblatt1960]

<br>

```{r neuronio-artificial, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Representação esquemática de um neurônio computacional." }
# ----- Parâmetros -----
act_type <- "sigmoid"   # escolha: "sigmoid", "relu", "tanh"

# abre área de plot sem título e com margens mínimas
graphics::par(mar=c(0,0,0,0))
diagram::openplotmat()

# Raio dos círculos
r_in  <- 0.05
r_sum <- 0.07
r_out <- 0.05

# Posições
pos_input <- matrix(c(
  0.10, 0.70,
  0.10, 0.50,
  0.10, 0.30
), ncol=2, byrow=TRUE)

pos_sum <- c(0.40, 0.50)   # Σ
pos_act <- c(0.65, 0.50)   # centro do gráfico
pos_out <- c(0.90, 0.50)   # y

# Entradas e nós
diagram::textellipse(pos_input[1,], r_in,  r_in,  lab="x1", box.col="lightblue")
diagram::textellipse(pos_input[2,], r_in,  r_in,  lab="x2", box.col="lightblue")
diagram::textellipse(pos_input[3,], r_in,  r_in,  lab="x3", box.col="lightblue")
diagram::textellipse(pos_sum,       r_sum, r_sum, lab="Σ",  box.col="lightgreen")
diagram::textellipse(pos_out,       r_out, r_out, lab="y",  box.col="salmon")

# Função para encurtar setas
encurta <- function(p1, p2, dist) {
  v <- p2 - p1
  v <- v / sqrt(sum(v^2))
  p2 - v * dist
}

# Conexões inputs → Σ
for (i in 1:3) {
  to_enc   <- encurta(pos_input[i,], pos_sum, r_sum)
  from_enc <- encurta(pos_sum, pos_input[i,], r_in)
  diagram::straightarrow(from=from_enc, to=to_enc, arr.pos=0.9, arr.length=0.25, lwd=1.5)
}

# Pesos
graphics::text(0.23, 0.70, "w1")
graphics::text(0.23, 0.52, "w2")
graphics::text(0.23, 0.34, "w3")

# ----- Caixa do mini-gráfico -----
fig_left   <- 0.58; fig_right <- 0.72
fig_bottom <- 0.40; fig_top   <- 0.60

# Pontos das bordas esquerda/direita da caixa
p_left  <- c(fig_left - 0.01, 0.50)
p_right <- c(fig_right + 0.01, 0.50)

# Conectar Σ → gráfico
from_sum <- encurta(p_left, pos_sum, r_sum)
diagram::straightarrow(from=from_sum, to=p_left, arr.pos=1, arr.length=0.25, lwd=1.5)

# Conectar gráfico → y
to_out <- encurta(p_right, pos_out, r_out)
diagram::straightarrow(from=p_right, to=to_out, arr.pos=0.9, arr.length=0.25, lwd=1.5)

# ----- Mini-gráfico no lugar de f(.) -----
op <- graphics::par(no.readonly = TRUE)
graphics::par(fig=c(fig_left, fig_right, fig_bottom, fig_top), new=TRUE, mar=c(0.5,0.5,0.5,0.5))

# Funções de ativação
f_act <- switch(act_type,
  "sigmoid" = function(z) 1/(1+exp(-z)),
  "relu"    = function(z) pmax(0, z),
  "tanh"    = function(z) tanh(z)
)

z <- base::seq(-6, 6, length.out=400)
ylim <- switch(act_type,
  "sigmoid" = c(0, 1),
  "relu"    = c(0, 6),
  "tanh"    = c(-1, 1)
)

graphics::plot(z, f_act(z), type="l", xlab="", ylab="", axes=FALSE, ylim=ylim)
graphics::box()

graphics::par(op)
```

<br>

```{r perceptron-exemplo, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Exemplo de um perceptron (regressão logística) para classificação linear."}
# Reprodutibilidade
set.seed(123)

# Gerar dados sintéticos para um problema de classificação linear
n <- 100
x1 <- runif(n, -1, 1)
x2 <- runif(n, -1, 1)
y <- ifelse(x1 + x2 > 0, 1, 0)
df <- data.frame(x1, x2, y = factor(y))

# Ajustar um modelo perceptron (regressão logística)
ggplot2::ggplot(df, ggplot2::aes(x1, x2, color=y)) +
  ggplot2::geom_point(size=3) +
  ggplot2::geom_abline(intercept=0, slope=-1, linetype="dashed") +
  ggplot2::theme_minimal() +
  ggplot2::labs(title="Perceptron: fronteira de decisão linear")
```

<br>

## Rede neural artificial

<br>

### O que é uma rede neural?

-   .[@REF]

<br>

```{r rede-neural, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Representação esquemática de uma rede neural."}
## ===== configuração geral =====
graphics::par(mar = c(0,0,0,0))
diagram::openplotmat()  # sem título

# raios/estética
r_node <- 0.045   # raio dos neurônios (círculos)
lwd_ar <- 1.4     # espessura das arestas
arr_len <- 0.25   # tamanho da seta

# posições (x fixo por camada; y distribuído verticalmente)
x_in   <- 0.10
x_h1   <- 0.35
x_h2   <- 0.60
x_out  <- 0.85

y_in   <- c(0.60, 0.40)        # 2 entradas
y_h1   <- c(0.70, 0.50, 0.30)  # 3 na primeira oculta
y_h2   <- c(0.70, 0.50, 0.30)  # 3 na segunda oculta
y_out  <- 0.50                 # 1 saída

# pontos (x,y) por camada
pos_in  <- cbind(rep(x_in,  length(y_in)),  y_in)
pos_h1  <- cbind(rep(x_h1,  length(y_h1)),  y_h1)
pos_h2  <- cbind(rep(x_h2,  length(y_h2)),  y_h2)
pos_out <- cbind(x_out, y_out)

## ===== helper: encurtar setas para encostar na borda do círculo =====
encurta <- function(p1, p2, dist) {
  v <- p2 - p1
  v <- v / sqrt(sum(v^2))
  p2 - v * dist
}

## ===== desenhar nós (círculos) =====
# entradas
for (i in seq_len(nrow(pos_in))) {
  diagram::textellipse(pos_in[i,], r_node, r_node, lab = paste0("x", i),
                       box.col = "lightblue")
}

# hidden 1
for (j in seq_len(nrow(pos_h1))) {
  diagram::textellipse(pos_h1[j,], r_node, r_node, lab = paste0("h1", j),
                       box.col = "lightgreen")
}

# hidden 2
for (k in seq_len(nrow(pos_h2))) {
  diagram::textellipse(pos_h2[k,], r_node, r_node, lab = paste0("h2", k),
                       box.col = "khaki1")
}

# saída
diagram::textellipse(pos_out, r_node, r_node, lab = "y",
                     box.col = "salmon")

## ===== conectar camadas (totalmente conectadas) =====
# entradas -> hidden1
for (i in seq_len(nrow(pos_in))) {
  for (j in seq_len(nrow(pos_h1))) {
    from <- encurta(pos_h1[j,], pos_in[i,],  r_node)  # sai da borda do nó origem
    to   <- encurta(pos_in[i,], pos_h1[j,],  r_node)  # chega na borda do nó alvo
    diagram::straightarrow(from = from, to = to, arr.pos = 0.9,
                           arr.length = arr_len, lwd = lwd_ar)
  }
}

# hidden1 -> hidden2
for (j in seq_len(nrow(pos_h1))) {
  for (k in seq_len(nrow(pos_h2))) {
    from <- encurta(pos_h2[k,], pos_h1[j,],  r_node)
    to   <- encurta(pos_h1[j,], pos_h2[k,],  r_node)
    diagram::straightarrow(from = from, to = to, arr.pos = 0.9,
                           arr.length = arr_len, lwd = lwd_ar)
  }
}

# hidden2 -> saída
for (k in seq_len(nrow(pos_h2))) {
  from <- encurta(pos_out, pos_h2[k,], r_node)
  to   <- encurta(pos_h2[k,], pos_out, r_node)
  diagram::straightarrow(from = from, to = to, arr.pos = 0.9,
                         arr.length = arr_len, lwd = lwd_ar)
}

## ===== rótulos de camadas (opcional; pode remover) =====
graphics::text(x_in,  0.85, "Entradas",  cex = 0.9)
graphics::text(x_h1,  0.85, "Camada\nEscondida 1",  cex = 0.9)
graphics::text(x_h2,  0.85, "Camada\nEscondida 2",  cex = 0.9)
graphics::text(x_out, 0.85, "Saída",     cex = 0.9)
```

<br>

### Quais são as funções de ativação mais comuns?

-   As funções de ativação introduzem não-linearidades nas redes neurais, permitindo que aprendam padrões complexos, tais como a sigmoide \@ref(eq:sigmoide), tangente hiperbólica \@ref(eq:tanh) e unidade linear retificada (ReLU) \@ref(eq:relu).[@REF]

\begin{equation}
(\#eq:sigmoide)
f(x) = \frac{1}{1 + e^{-x}}
\end{equation}

<br>

\begin{equation}
(\#eq:tanh)
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}

<br>

\begin{equation}
(\#eq:relu)
f(x) = \max(0, x)
\end{equation}

<br>

```{r funcoes-ativacao, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Gráficos das funções de ativação mais comuns."}
# grid comum
z <- seq(-6, 6, length.out = 400)
df <- data.frame(z = z)

# funções
df$sigmoid <- 1/(1+exp(-df$z))
df$tanh    <- tanh(df$z)
df$relu    <- pmax(0, df$z)

# Sigmoide
p1 <- ggplot2::ggplot(df, ggplot2::aes(x=z, y=sigmoid)) +
  ggplot2::geom_line(size=1, color="blue") +
  ggplot2::geom_hline(yintercept=0, linetype="dashed") +
  ggplot2::geom_vline(xintercept=0, linetype="dashed") +
  ggplot2::labs(x="z", y="f(z)") +
  ggplot2::ggtitle("Sigmoide") +
  ggplot2::theme_minimal(base_size = 12)

# Tanh
p2 <- ggplot2::ggplot(df, ggplot2::aes(x=z, y=tanh)) +
  ggplot2::geom_line(size=1, color="darkgreen") +
  ggplot2::geom_hline(yintercept=0, linetype="dashed") +
  ggplot2::geom_vline(xintercept=0, linetype="dashed") +
  ggplot2::labs(x="z", y="f(z)") +
  ggplot2::ggtitle("Tanh") +
  ggplot2::theme_minimal(base_size = 12)

# ReLU
p3 <- ggplot2::ggplot(df, ggplot2::aes(x=z, y=relu)) +
  ggplot2::geom_line(size=1, color="red") +
  ggplot2::geom_hline(yintercept=0, linetype="dashed") +
  ggplot2::geom_vline(xintercept=0, linetype="dashed") +
  ggplot2::labs(x="z", y="f(z)") +
  ggplot2::ggtitle("ReLU") +
  ggplot2::theme_minimal(base_size = 12)

# Juntar em uma linha
gridExtra::grid.arrange(p1, p2, p3, ncol=3)
```

<br>

## Espaço de decisão

<br>

### O que é espaço de decisão?

-   O espaço de decisão é a região do espaço de entrada onde o modelo classifica as entradas em diferentes categorias. Ele é definido pelas fronteiras de decisão aprendidas pelo modelo durante o treinamento.[@REF]

<br>

### Como ele é visualizado?

-   O espaço de decisão pode ser visualizado graficamente, especialmente em problemas de classificação binária ou multiclasse, onde as regiões correspondem às classes previstas pelo modelo.[@REF]

<br>

```{r espaco-de-decisao, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Comparação do espaço de decisão entre um modelo linear (regressão logística) e um modelo não linear (MLP)."}
## 1) Dados sintéticos (XOR)
base::set.seed(123)

make_xor <- function(n_per_quad = 100, sd = 0.25) {
  # Quatro centros (±1, ±1). Classe 1 nos quadrantes (1,1) e (-1,-1)
  centers <- matrix(c( 1,  1,
                       1, -1,
                      -1,  1,
                      -1, -1), ncol = 2, byrow = TRUE)
  cls <- c(1, 0, 0, 1)
  X <- NULL; y <- NULL
  for (i in seq_len(4)) {
    X <- rbind(X, cbind(stats::rnorm(n_per_quad, mean = centers[i,1], sd = sd),
                        stats::rnorm(n_per_quad, mean = centers[i,2], sd = sd)))
    y <- c(y, rep.int(cls[i], n_per_quad))
  }
  df <- data.frame(x1 = X[,1], x2 = X[,2], y = factor(y))
  df
}

df <- make_xor(n_per_quad = 120, sd = 0.28)

## 2) Ajustar modelos
# (a) Regressão logística (linear)
mod_glm <- stats::glm(y ~ x1 + x2, data = df, family = stats::binomial())

# (b) MLP com 1 camada escondida (size neurônios)
# nnet::nnet requer y numérico 0/1 para classificação; usamos linout=FALSE (logística)
y_num <- base::as.numeric(as.character(df$y))
mod_mlp <- nnet::nnet(x = as.matrix(df[, c("x1","x2")]),
                      y = y_num,
                      size = 6,            # neurônios na escondida
                      decay = 1e-3,        # regularização
                      maxit = 500,
                      linout = FALSE,      # saída logística
                      trace = FALSE)

## 3) Grade para visualizar fronteiras
x1_seq <- base::seq(base::min(df$x1) - 0.5, base::max(df$x1) + 0.5, length.out = 300)
x2_seq <- base::seq(base::min(df$x2) - 0.5, base::max(df$x2) + 0.5, length.out = 300)
grid <- base::expand.grid(x1 = x1_seq, x2 = x2_seq)

# Probabilidades preditas
grid$p_glm <- base::as.numeric(stats::predict(mod_glm, newdata = grid, type = "response"))
grid$p_mlp <- as.numeric(stats::predict(mod_mlp, newdata = as.matrix(grid[,c("x1","x2")])))
grid$cls_glm <- factor(ifelse(grid$p_glm >= 0.5, 1, 0))
grid$cls_mlp <- factor(ifelse(grid$p_mlp >= 0.5, 1, 0))

## 4) Plots com ggplot2
common_points <- ggplot2::geom_point(
  data = df, ggplot2::aes(x = x1, y = x2, color = y),
  size = 1.6, alpha = 0.9
)

# Paleta simples e tema limpo
theme_clean <- ggplot2::theme_minimal(base_size = 12) +
  ggplot2::theme(legend.position = "none",
                 plot.title = ggplot2::element_text(face = "bold"),
                 plot.subtitle = ggplot2::element_text(margin = ggplot2::margin(t = 2)))

# (a) Fronteira GLM (linear)
p_glm <- ggplot2::ggplot() +
  ggplot2::geom_raster(
    data = grid,
    ggplot2::aes(x = x1, y = x2, fill = cls_glm),
    alpha = 0.25, interpolate = TRUE
  ) +
  ggplot2::geom_contour(
    data = grid,
    ggplot2::aes(x = x1, y = x2, z = p_glm),
    breaks = 0.5, color = "black", linewidth = 0.7
  ) +
  common_points +
  ggplot2::scale_fill_manual(values = c("#6BAED6","#FD8D3C")) +
  ggplot2::scale_color_manual(values = c("#3182BD","#E6550D")) +
  ggplot2::labs(title = "Modelo Linear",
                subtitle = "Regressão logística (fronteira linear)",
                x = "x1", y = "x2") +
  theme_clean

# (b) Fronteira MLP (não linear)
p_mlp <- ggplot2::ggplot() +
  ggplot2::geom_raster(
    data = grid,
    ggplot2::aes(x = x1, y = x2, fill = cls_mlp),
    alpha = 0.25, interpolate = TRUE
  ) +
  ggplot2::geom_contour(
    data = grid,
    ggplot2::aes(x = x1, y = x2, z = p_mlp),
    breaks = 0.5, color = "black", linewidth = 0.7
  ) +
  common_points +
  ggplot2::scale_fill_manual(values = c("#6BAED6","#FD8D3C")) +
  ggplot2::scale_color_manual(values = c("#3182BD","#E6550D")) +
  ggplot2::labs(title = "Modelo Não Linear",
                subtitle = "Rede neural (1 camada escondida) — fronteira não linear",
                x = "x1", y = "x2") +
  theme_clean

# Painel lado a lado
gridExtra::grid.arrange(p_glm, p_mlp, ncol = 2)
```

<br>

## Áreas da IA

<br>

### Quais são as principais áreas de aplicação da IA?

-   Percepção: visão computacional, reconhecimento de fala.[@REF]

-   Raciocínio: sistemas especialistas, inferência lógica.[@REF]

-   Aprendizado: aprendizado de máquina e reforço profundo.[@REF]

-   Interação: processamento de linguagem natural, agentes conversacionais.[@REF]

<br>

### Quais são exemplos de aplicação prática?

-   Classificação de imagens.[@REF]

-   Análise de sentimentos.[@REF]

-   Geração de texto.[@REF]

<br>

## Técnicas modernas

<br>

### Quais são as técnicas modernas de IA?

-   Redes neurais profundas (CNN, RNN, Transformers).[@REF]

-   Modelos generativos (GANs, VAEs, LLMs).[@REF]

-   Aprendizado por reforço profundo (Deep Reinforcement Learning).[@REF]

<br>

## IA generativa e grandes modelos de linguagem

<br>

### O que são grandes modelos de linguagem (*large language models*, LLM)?

-   .[@REF]

<br>

### Como funcionam modelos como GPT, BERT e similares?

-   .[@REF]

<br>

```{r llm, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Representação esquemática de um modelo de linguagem grande (LLM)"}
# Reprodutibilidade
set.seed(42)

# 1) Texto e tokenização
texto <- "Os modelos de linguagem grandes (LLMs) aprendem padrões e sugerem o próximo token."

# Tokenizador simples: palavras, números, abreviações e pontuação
tokens <- stringr::str_extract_all(texto, "[\\p{L}\\p{N}]+(?:['’][\\p{L}\\p{N}]+)?|[[:punct:]]", simplify = FALSE)[[1]]
tokens <- tokens[tokens != ""]
tokens_df <- tibble::tibble(idx = seq_along(tokens), token = tokens)

# 2) "Mini-LLM" ilustrativo
#    Embeddings aleatórios + atenção toy + softmax
# Vocabulário = tokens do texto + alguns candidatos de próxima palavra
candidatos <- c("e","então","porque","mas","gato","cachorro","token","modelo",".",",")
vocab <- unique(c(tokens, candidatos))

d <- 32  # dimensão do embedding (toy)
E <- matrix(rnorm(length(vocab) * d), nrow = length(vocab), ncol = d)
rownames(E) <- vocab

# Mapeia cada token ao seu embedding
emb_for <- function(tok) E[as.character(tok), , drop = FALSE]

# Contexto = média dos embeddings dos tokens já vistos
contexto <- apply(do.call(rbind, lapply(tokens, emb_for)), 2, mean)

# Logits ~ similaridade (produto interno) com o contexto
logits <- as.numeric(E %*% contexto)
names(logits) <- vocab

# Softmax com temperatura
softmax <- function(x, temp = 0.8) {
  z <- x / temp
  z <- z - max(z)
  exp_z <- exp(z)
  exp_z / sum(exp_z)
}
probs <- softmax(logits, temp = 0.8)
pred_df <- tibble::tibble(token = names(probs), prob = probs) |>
  dplyr::arrange(dplyr::desc(prob))

top_k <- 10
pred_top <- pred_df |> dplyr::slice(1:top_k) |>
  dplyr::mutate(token = factor(token, levels = rev(token)))

# 3) Atenção "toy"
#    Último token (query) atendendo aos anteriores (keys)
# Para a atenção, criamos Q e K a partir de E (projeções lineares aleatórias)
Wq <- matrix(rnorm(d * d), nrow = d, ncol = d)
Wk <- matrix(rnorm(d * d), nrow = d, ncol = d)

# Keys: um por token da sequência
K_mat <- do.call(rbind, lapply(tokens, function(tk) emb_for(tk) %*% Wk))
# Query: do último token
q_vec <- as.numeric((emb_for(tail(tokens, 1)) %*% Wq)[1, ])

# Pesos de atenção do último token para todos os anteriores
att_logits <- as.numeric(K_mat %*% q_vec) / sqrt(d)
att_w <- softmax(att_logits, temp = 1.0)

att_df <- tibble::tibble(idx = seq_along(tokens), token = tokens, att = att_w)

# 4) Figuras (3 painéis)

# Painel A: sequência de tokens
pal <- scales::hue_pal()(length(unique(tokens)))
tokens_df <- tokens_df |>
  dplyr::mutate(tok_id = as.integer(factor(token)),
                y = 1)

# Painel A: sequência de tokens (sem cores, apenas moldura preta)
g_seq <- ggplot2::ggplot(tokens_df, ggplot2::aes(xmin = idx - 0.5, xmax = idx + 0.5,
                               ymin = 0.5, ymax = 1.5)) +
  ggplot2::geom_rect(fill = "white", color = "black") +
  ggplot2::geom_text(ggplot2::aes(x = idx, y = 1, label = token), size = 3) +
  ggplot2::scale_x_continuous(breaks = tokens_df$idx) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL, limits = c(0.4, 1.6)) +
  ggplot2::labs(title = "A) Sequência de tokens (após tokenização)") +
  ggplot2::theme_minimal(base_size = 12) +
  ggplot2::theme(panel.grid = ggplot2::element_blank(),
        axis.title.x = ggplot2::element_blank(),
        axis.text.x = ggplot2::element_text(size = 8, vjust = 0.5))

# Painel B: atenção do último token para os anteriores
g_att <- ggplot2::ggplot(att_df, ggplot2::aes(x = idx, y = att)) +
  ggplot2::geom_col() +
  ggplot2::geom_text(ggplot2::aes(label = token), nudge_y = 0.02, size = 3, angle = 0) +
  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  ggplot2::scale_x_continuous(breaks = att_df$idx) +
  ggplot2::labs(title = paste0("B) Atenção do último token: \"", tail(tokens,1), "\""),
       x = "posição na sequência", y = "peso de atenção") +
  ggplot2::theme_minimal(base_size = 12)

# Painel C: top-k predições de próximo token (toy)
g_pred <- ggplot2::ggplot(pred_top, ggplot2::aes(x = token, y = prob)) +
  ggplot2::geom_col() +
  ggplot2::coord_flip() +
  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  ggplot2::labs(title = "C) Próximo token (top-10) – distribuição toy",
       x = NULL, y = "probabilidade") +
  ggplot2::theme_minimal(base_size = 12)

# Compor figura
gridExtra::grid.arrange(g_seq, g_att, g_pred, ncol = 1, heights = c(1.2, 1.2, 1.4))
```

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *keras*[\@keras](https://cran.r-project.org/web/packages/keras/index.html) funções para criar, treinar e avaliar modelos de redes neurais.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *torch*[@torch](https://cran.r-project.org/web/packages/torch/index.html) permite criar e treinar redes neurais com alto desempenho.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *reticulate*[@reticulate](https://cran.r-project.org/web/packages/reticulate/index.html) integra R e Python em um mesmo ambiente de trabalho, permitindo chamar funções Python a partir de R e facilitar o uso de bibliotecas de IA disponíveis nesse ecossistema.
:::

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *text2vec*[@text2vec](https://cran.r-project.org/web/packages/text2vec/index.html) fornece ferramentas para modelagem de texto em R, incluindo pré-processamento, vetorização, embeddings e outros modelos úteis para processamento de linguagem natural.
:::

<br>

```{r, echo=FALSE, warning=FALSE, results='asis', eval=knitr::is_html_output()}
cat(readLines("citation.html"), sep = "\n")
```

<br>
