```{=openxml}
<w:p>
  <w:r>
    <w:br w:type="page"/>
  </w:r>
</w:p>
```
# **Paradoxos estatísticos** {#paradoxos-estatisticos}

<br>

## Paradoxos

<br>

### O que são paradoxos estatísticos?

-   Paradoxos podem originar da incompreensão ou mal informação da nossa intuição a respeito do fenômeno.[@meng2018]

<br>

### O que é o paradoxo de Abelson?

-   Um baixo percentual de variância explicada não implica que o fator causal seja irrelevante. Em processos cumulativos, efeitos pequenos podem produzir consequências grandes.[@abelson1985]

-   Esse paradoxo alerta contra o uso ingênuo de medidas como $R^2$ ou “% de explicação” para julgar a importância prática de um fator.[@REF]

<br>

```{r abelson, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE, appendix=TRUE, fig.align='center', layout='Blank', ph=officer::ph_location_fullsize(), results='asis', fig.cap='Simulação do paradoxo de Abelson. Mesmo com uma diferença pequena de habilidade entre dois jogadores (A e B), o jogador B tende a vencer a maioria das tentativas devido ao alto ruído por tentativa. No entanto, a análise por tentativa mostra um baixo valor de $R^2$, indicando que a habilidade explica pouco da variação no desempenho observado. Isso ilustra como um fator causal pode ser importante na prática, mesmo quando a variância explicada é baixa.'}
# reprodutibilidade
set.seed(123)

# simulação dos dados
n_trials   <- 1000
skill_diff <- 0.05
noise_sd  <- 1

# geração dos dados
data <- tibble::tibble(
  Jogador    = rep(c("A", "B"), each = n_trials),
  Tentativa  = rep(seq_len(n_trials), times = 2),
  Habilidade = rep(c(0, skill_diff), each = n_trials),
  Desempenho = Habilidade + rnorm(2 * n_trials, sd = noise_sd)
)

# ajuste dos modelos
model_micro <- lm(Desempenho ~ Habilidade, data = data)

tbl_micro <- model_micro %>%
  gtsummary::tbl_regression(
    intercept = FALSE,
    estimate_fun = ~gtsummary::style_number(.x, digits = 3)
  ) %>%
  gtsummary::add_glance_table(
    include = r.squared,
    fmt_fun = r.squared ~ function(x) gtsummary::style_number(x, digits = 3),
    label = r.squared ~ "R² (variância explicada)"
  ) %>%
  gtsummary::modify_caption(
    "**Modelo micro (por tentativa): Desempenho ~ Habilidade**"
  )

data_macro <- data %>%
  dplyr::mutate(
    bloco = rep(seq_len(10), each = n_trials / 10, times = 2)
  ) %>%
  dplyr::group_by(Jogador, Habilidade, bloco) %>%
  dplyr::summarise(
    Total_bloco = sum(Desempenho),
    .groups = "drop"
  )

model_macro <- lm(Total_bloco ~ Habilidade, data = data_macro)

tbl_macro <- model_macro %>%
  gtsummary::tbl_regression(
    intercept = FALSE,
    estimate_fun = ~gtsummary::style_number(.x, digits = 3)
  ) %>%
  gtsummary::add_glance_table(
    include = r.squared,
    fmt_fun = r.squared ~ function(x) gtsummary::style_number(x, digits = 3),
    label = r.squared ~ "R² (variância explicada)"
  )

gtsummary::tbl_stack(
  tbls = list(tbl_micro, tbl_macro),
  group_header = c(
    "Nível micro — eventos individuais",
    "Nível macro — resultado acumulado"
  )
) %>%
  gtsummary::modify_header(
    label     = "**Variável explicativa**",
    estimate  = "**Estimativa**",
    conf.low  = "**IC 95%**",
    p.value   = "**P-valor**"
  ) %>%
  gtsummary::remove_abbreviation() %>%
  gtsummary::modify_footnote(
    conf.low ~ "IC = Intervalo de confiança"
)
```

<br>

### O que é o paradoxo de Berkson?

-   .[@berkson1946]

<br>

### O que é o paradoxo de Ellsberg?

-   .[@ellsberg1961]

<br>

### O que é o paradoxo de Freedman?

-   .[@freedman1983; @freedman1989]

<br>

### O que é o paradoxo de Hand?

-   .[@hand1992]

<br>

### O que é o paradoxo de Kelley?

-   .[@REF]

<br>

### O que é o paradoxo de Lindley?

-   .[@lindley1957]

<br>

### O que é o paradoxo de Lord?

-   .[@lord1967; @lord1969]

<br>

### O que é o paradoxo de Meng?

-   *Big Data*: "Quanto maior a quantidade de dados, maior a certeza de que vamos nos enganar".[@meng2018]

<br>

### O que é o paradoxo de Proebsting?

-   .[@REF]

<br>

### O que é o paradoxo de Simpson?

-   O paradoxo de Simpson ocorre quando a associação entre duas variáveis $X$ e $Y$ desaparece ou mesmo reverte sua direção quando condicionadas em uma terceira variável $Z$.[@simpson1951; @blyth1972]

-   Para decisão do paradoxo de Simpson pode-se utilizar o conceito de 'back-door', o qual considera os 'caminhos' (isto é, associações) no gráfico acíclio direcionado e assegura que todos as associações espúrias do tratamento $X$ para o desfecho $Y$ nesse diagrama causal sejam interceptados pela variável $Z$.[@pearl2014]

-   Dependendo do contexto em que os dados foram obtidos --- delineamento do estudo, escolha dos instrumentos e dos tipos de variáveis --- a melhor escolha para a análise pode variar entre a análise da população agregada ou da subpopulação desagregada.[@pearl2014]

-   É possível que em alguns contextos nem a análise agregada ou a desagregada podem oferecer a resposta correta, sendo necessário o uso de outras (mais) covariáveis.[@pearl2014]

<br>

```{r simpson, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.cap = "Paradoxo de Simpson representado com dados simulados. Os pontos no gráfico representam observações individuais e as linhas de tendência representam as regressões lineares ajustadas para os dados desagregados da população e agregados por subpopulação."}
# Define global shapes and colors
global_shapes <- c(16, 16)  # Circle and triangle
global_colors <- c("black", "gray50")  # Black and gray

# Function to generate data
generate_data <- function(n, r1, r2) {
  x <- c(rnorm(n / 2, mean = -1), rnorm(n / 2, mean = 1))
  y <- r1 * x + rnorm(n)
  
  # Introduce a third variable z that acts as a confounding variable
  z <- rep(c(0, 1), each = n / 2)
  y[z == 1] <- r2 * x[z == 1] + rnorm(sum(z))
  
  data.frame(x = x, y = y, z = z)
}

# Generate data
set.seed(123)
n <- 100
r1 <- 0.8
r2 <- -0.6
data <- generate_data(n, r1, r2)

# Calculate correlation coefficients
cor_aggregated <- cor(data$x, data$y)
cor_disaggregated <- by(data, data$z, function(df)
  cor(df$x, df$y))

# Define the limits for x and y axes
x_limits <- range(data$x)
y_limits <- range(data$y)

# Plot for aggregated trend
aggregated_plot <- ggplot2::ggplot(data, ggplot2::aes(x = x, y = y, color = factor(z)), shape = global_shapes) +
  ggplot2::geom_point(size = 2, shape = ifelse(data$z == 0, 16, 16)) +
  ggplot2::geom_smooth(
    method = "lm",
    se = FALSE,
    ggplot2::aes(group = 1),
    color = "blue"
  ) +
  ggplot2::labs(
    title = "População agregada",
    x = "X",
    y = "Y",
    color = "Z"
  ) +
  ggplot2::scale_color_manual(values = global_colors) +
  ggplot2::theme_minimal() +
  ggplot2::xlim(x_limits) +
  ggplot2::ylim(y_limits) +
  ggplot2::theme(
    aspect.ratio = 1,
    legend.position = "right",
    plot.title = ggplot2::element_text(hjust = 0.5, size = 10)
  )

# Plot for disaggregated trends
disaggregated_plots <- lapply(unique(data$z), function(z_value) {
  ggplot2::ggplot(data[data$z == z_value, ], ggplot2::aes(x = x, y = y, color = factor(z))) +
    ggplot2::geom_point(
      size = 2,
      shape = ifelse(z_value == 0, 16, 16),
      color = ifelse(z_value == 0, "black", "gray50")
    ) +
    ggplot2::geom_smooth(
      method = "lm",
      se = FALSE,
      ggplot2::aes(group = 1),
      color = "blue"
    ) +
    ggplot2::labs(
      title = paste("Subpopulação desagregada (Z=", z_value, ")", sep = ""),
      x = "X",
      y = "Y"
    ) +
    ggplot2::scale_color_manual(values = global_colors) +
    ggplot2::theme_minimal() +
    ggplot2::xlim(x_limits) +
    ggplot2::ylim(y_limits) +
    ggplot2::theme(
      aspect.ratio = 1,
      plot.title = ggplot2::element_text(size = 10)
    )
})

aggregated_plot <- aggregated_plot + 
  ggplot2::theme(plot.margin = ggplot2::margin(2, 2, 2, 2))

disaggregated_plots <- lapply(disaggregated_plots, function(p) {
  p + ggplot2::theme(plot.margin = ggplot2::margin(2, 2, 2, 2))
})

# Arrange plots
cowplot::plot_grid(
  aggregated_plot,
  cowplot::plot_grid(plotlist = disaggregated_plots, align = "v"),
  ncol = 1,
  align = "v",
  axis = "lr"
)
```

<br>

### O que é o paradoxo de James-Stein?

-   O paradoxo de James-Stein mostra que, ao estimar simultaneamente 3 ou mais médias de variáveis normais independentes (com perda quadrática), o estimador “óbvio” $X_i$ --- que é ótimo para cada média isoladamente --- deixa de ser ótimo no conjunto, existindo estimadores que têm erro médio total menor.[@stein1956;@james_estimation_1961]

-   O resultado é paradoxal porque essa melhoria exige “misturar” as estimativas entre si (como no estimador de James–Stein), introduzindo um viés controlado que reduz o erro global, algo impossível quando $n \leq 2$.[@stein1956;@james_estimation_1961]

<br>

```{r james-stein, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE, appendix=TRUE, fig.align='center', layout='Blank', ph=officer::ph_location_fullsize(), results='asis', fig.cap='Simulação do paradoxo de Stein. Comparação do erro médio quadrático entre o estimador clássico (média amostral) e o estimador de James-Stein para diferentes números de médias sendo estimadas simultaneamente. Estimadores aparentemente piores localmente podem ser melhores globalmente quando o objetivo é reduzir o erro total.'}
# Reprodutibilidade
set.seed(123)

# Função de simulação
simula_stein <- function(n, B = 10000, theta_val = 3) {
  
  theta <- rep(theta_val, n)
  
  james_stein <- function(X) {
    shrink <- 1 - (n - 2) / sum(X^2)
    shrink * X
  }
  
  erro_mle <- numeric(B)
  erro_js  <- numeric(B)
  
  for (b in 1:B) {
    X <- rnorm(n, mean = theta, sd = 1)
    
    erro_mle[b] <- sum((X - theta)^2)
    erro_js[b]  <- sum((james_stein(X) - theta)^2)
  }
  
  data.frame(
    Erro = c(erro_mle, erro_js),
    Estimador = rep(c("Maximum Likelihood Estimate", "James–Stein"), each = B),
    Dimensao = paste("n =", n)
  )
}

# Simulações
df_n2 <- simula_stein(n = 2)
df_n5 <- simula_stein(n = 5)

df <- rbind(df_n2, df_n5)

# Gráfico lado a lado
ggplot2::ggplot(df, ggplot2::aes(x = Erro, fill = Estimador)) +
  ggplot2::geom_density(alpha = 0.5) +
  ggplot2::facet_wrap(~ Dimensao, scales = "free_x") +
  ggplot2::labs(
    title = "Paradoxo de Stein",
    subtitle = "n = 2 (sem paradoxo) vs n = 5 (com paradoxo)",
    x = "Erro quadrático total",
    y = "Densidade",
    fill = NULL
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    legend.position = "bottom",
    legend.direction = "horizontal"
  )
```

<br>

### O que é o paradoxo de Okie?

-   .[@REF]

<br>

### O que é o paradoxo da acurácia?

-   .[@REF]

<br>

### O que é o paradoxo do falso positivo?

-   .[@REF]

<br>

### O que é o paradoxo da caixa de Bertrand?

-   .[@REF]

<br>

### O que é o paradoxo do elevador?

-   .[@de1996]

<br>

### O que é o paradoxo da amizade?

-   .[@feld1991]

<br>

### O que é o paradoxo do menino ou menina?

-   .[@de1996]

<br>

### O que é o paradoxo do aniversário?

-   .[@REF]

<br>

### O que é o paradoxo do teste surpresa?

-   .[@REF]

<br>

### O que é o paradoxo do nó da gravata?

-   .[@REF]

<br>

### O que é o paradoxo de Monty Hall?

-   .[@REF]

<br>

### O que é o paradoxo da Bela Adormecida?

-   .[@REF]

<br>

```{r, echo=FALSE, warning=FALSE, results='asis', eval=knitr::is_html_output()}
cat(readLines("citation.html"), sep = "\n")
```

<br>
