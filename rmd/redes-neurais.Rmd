```{=openxml}
<w:p>
  <w:r>
    <w:br w:type="page"/>
  </w:r>
</w:p>
```

# **Redes neurais** {#redes-meurais}

<br>

## Neurônios artificiais

<br>

### O que são neurônios artificiais?

-   Neuronios artificiais (ou perceptrons) são modelos matemáticos que imitam o funcionamento dos neurônios biológicos, recebendo entradas, aplicando pesos e uma função de ativação para produzir uma saída.[@mcculloch1943; @rosenblatt1958; @rosenblatt1960]

-   A equação geral de um neurônio artificial é dada por \@ref(eq:neuronio), onde $x_i$ são as entradas, $w_i$ os pesos, $b$ o viés e $\phi$ a função de ativação:

\begin{equation}
(\#eq:neuronio)
y = \phi\left(\sum_{i=1}^{d} w_i\,x_i + b\right)
\end{equation}

<br>

```{r neuronio-artificial, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Representação esquemática de um neurônio computacional." }
# ----- Parâmetros -----
act_type <- "sigmoid"   # escolha: "sigmoid", "relu", "tanh"

# abre área de plot sem título e com margens mínimas
graphics::par(mar=c(0,0,0,0))
diagram::openplotmat()

# Raio dos círculos
r_in  <- 0.05
r_sum <- 0.07
r_out <- 0.05

# Posições
pos_input <- matrix(c(
  0.10, 0.70,
  0.10, 0.50,
  0.10, 0.30
), ncol=2, byrow=TRUE)

pos_sum <- c(0.40, 0.50)   # Σ
pos_act <- c(0.65, 0.50)   # centro do gráfico
pos_out <- c(0.90, 0.50)   # y

# Entradas e nós
diagram::textellipse(pos_input[1,], r_in,  r_in,  lab="x1", box.col="lightblue")
diagram::textellipse(pos_input[2,], r_in,  r_in,  lab="x2", box.col="lightblue")
diagram::textellipse(pos_input[3,], r_in,  r_in,  lab="x3", box.col="lightblue")
diagram::textellipse(pos_sum,       r_sum, r_sum, lab="Σ",  box.col="lightgreen")
diagram::textellipse(pos_out,       r_out, r_out, lab="y",  box.col="salmon")

# Função para encurtar setas
encurta <- function(p1, p2, dist) {
  v <- p2 - p1
  v <- v / sqrt(sum(v^2))
  p2 - v * dist
}

# Conexões inputs → Σ
for (i in 1:3) {
  to_enc   <- encurta(pos_input[i,], pos_sum, r_sum)
  from_enc <- encurta(pos_sum, pos_input[i,], r_in)
  diagram::straightarrow(from=from_enc, to=to_enc, arr.pos=0.9, arr.length=0.25, lwd=1.5)
}

# Pesos
graphics::text(0.23, 0.70, "w1")
graphics::text(0.23, 0.52, "w2")
graphics::text(0.23, 0.34, "w3")

# ----- Caixa do mini-gráfico -----
fig_left   <- 0.58; fig_right <- 0.72
fig_bottom <- 0.40; fig_top   <- 0.60

# Pontos das bordas esquerda/direita da caixa
p_left  <- c(fig_left - 0.01, 0.50)
p_right <- c(fig_right + 0.01, 0.50)

# Conectar Σ → gráfico
from_sum <- encurta(p_left, pos_sum, r_sum)
diagram::straightarrow(from=from_sum, to=p_left, arr.pos=1, arr.length=0.25, lwd=1.5)

# Conectar gráfico → y
to_out <- encurta(p_right, pos_out, r_out)
diagram::straightarrow(from=p_right, to=to_out, arr.pos=0.9, arr.length=0.25, lwd=1.5)

# ----- Mini-gráfico no lugar de f(.) -----
op <- graphics::par(no.readonly = TRUE)
graphics::par(fig=c(fig_left, fig_right, fig_bottom, fig_top), new=TRUE, mar=c(0.5,0.5,0.5,0.5))

# Funções de ativação
f_act <- switch(act_type,
  "sigmoid" = function(z) 1/(1+exp(-z)),
  "relu"    = function(z) pmax(0, z),
  "tanh"    = function(z) tanh(z)
)

z <- base::seq(-6, 6, length.out=400)
ylim <- switch(act_type,
  "sigmoid" = c(0, 1),
  "relu"    = c(0, 6),
  "tanh"    = c(-1, 1)
)

graphics::plot(z, f_act(z), type="l", xlab="", ylab="", axes=FALSE, ylim=ylim)
graphics::box()

graphics::par(op)
```

<br>

## Rede neural artificial

<br>

### O que é uma rede neural artificial?

-   Redes neurais artificiais são modelos computacionais compostos por camadas de neurônios artificiais interconectados, nos quais cada camada aplica transformações lineares seguidas de funções não lineares, permitindo a aproximação de relações complexas entre variáveis de entrada e saída.[@REF]

<br>

```{r rede-neural, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Representação esquemática de uma rede neural simples com camada de entrada e saída."}
## ===== configuração geral =====
graphics::par(mar = c(0,0,0,0))
diagram::openplotmat()  # sem título

# raios / estética
r_node  <- 0.045   # raio dos neurônios
lwd_ar  <- 1.4     # espessura das arestas
arr_len <- 0.25    # tamanho da seta

# posições (somente entrada e saída)
x_in  <- 0.15
x_out <- 0.85

y_in  <- c(0.65, 0.50, 0.35)  # 3 entradas
y_out <- 0.50                 # 1 saída

# pontos (x,y)
pos_in  <- cbind(rep(x_in,  length(y_in)),  y_in)
pos_out <- cbind(x_out, y_out)

## ===== helper: encurtar setas =====
encurta <- function(p1, p2, dist) {
  v <- p2 - p1
  v <- v / sqrt(sum(v^2))
  p2 - v * dist
}

## ===== desenhar nós =====
# entradas
for (i in seq_len(nrow(pos_in))) {
  diagram::textellipse(
    pos_in[i,], r_node, r_node,
    lab = paste0("x", i),
    box.col = "lightblue"
  )
}

# saída
diagram::textellipse(
  pos_out, r_node, r_node,
  lab = "y",
  box.col = "salmon"
)

## ===== conectar entrada -> saída =====
for (i in seq_len(nrow(pos_in))) {
  from <- encurta(pos_out, pos_in[i,], r_node)
  to   <- encurta(pos_in[i,], pos_out, r_node)
  diagram::straightarrow(
    from = from,
    to   = to,
    arr.pos = 0.9,
    arr.length = arr_len,
    lwd = lwd_ar
  )
}

## ===== rótulos das camadas =====
graphics::text(x_in,  0.85, "Entradas", cex = 0.9)
graphics::text(x_out, 0.85, "Saída",    cex = 0.9)
```

<br>

::: {.infobox .Rlogo data-latex="{images/Rlogo}"}
O pacote *neuralnet*[@neuralnet] fornece a função [*neuralnet*](https://www.rdocumentation.org/packages/neuralnet/versions/1.44.2/topics/neuralnet) para treinar redes neurais artificiais.
:::

<br>

## Funções de ativação

<br>

### Quais são as funções de ativação mais comuns?

-   As funções de ativação introduzem não-linearidades nas redes neurais, permitindo que aprendam padrões complexos, como sigmoide \@ref(eq:sigmoide), tangente hiperbólica \@ref(eq:tanh) e unidade linear retificada (ReLU) \@ref(eq:relu).[@REF]

\begin{equation}
(\#eq:sigmoide)
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}

<br>

\begin{equation}
(\#eq:tanh)
\tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
\end{equation}

<br>

\begin{equation}
(\#eq:relu)
\operatorname{ReLU}(z) = \max(0, z)
\end{equation}

<br>

- Ao manter gradientes constantes na região positiva, a ReLU favorece estabilidade numérica e eficiência computacional em redes multicamadas.[@REF

- Diferentemente das funções sigmoide e tangente hiperbólica, a ReLU preserva gradientes úteis em regiões amplas do espaço de entrada.[@REF]

- Sem funções de ativação não lineares, uma rede neural profunda se reduz a um modelo linear equivalente.[@REF]

<br>

```{r funcoes-ativacao, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Gráficos das funções de ativação mais comuns."}
# grid comum
z <- seq(-6, 6, length.out = 400)
df <- data.frame(z = z)

# funções
df$sigmoid <- 1/(1+exp(-df$z))
df$tanh    <- tanh(df$z)
df$relu    <- pmax(0, df$z)

# Sigmoide
p1 <- ggplot2::ggplot(df, ggplot2::aes(x=z, y=sigmoid)) +
  ggplot2::geom_line(size=1, color="blue") +
  ggplot2::geom_hline(yintercept=0, linetype="dashed") +
  ggplot2::geom_vline(xintercept=0, linetype="dashed") +
  ggplot2::labs(x="z", y="f(z)") +
  ggplot2::ggtitle("Sigmoide") +
  ggplot2::theme_minimal(base_size = 12)

# Tanh
p2 <- ggplot2::ggplot(df, ggplot2::aes(x=z, y=tanh)) +
  ggplot2::geom_line(size=1, color="darkgreen") +
  ggplot2::geom_hline(yintercept=0, linetype="dashed") +
  ggplot2::geom_vline(xintercept=0, linetype="dashed") +
  ggplot2::labs(x="z", y="f(z)") +
  ggplot2::ggtitle("Tanh") +
  ggplot2::theme_minimal(base_size = 12)

# ReLU
p3 <- ggplot2::ggplot(df, ggplot2::aes(x=z, y=relu)) +
  ggplot2::geom_line(size=1, color="red") +
  ggplot2::geom_hline(yintercept=0, linetype="dashed") +
  ggplot2::geom_vline(xintercept=0, linetype="dashed") +
  ggplot2::labs(x="z", y="f(z)") +
  ggplot2::ggtitle("ReLU") +
  ggplot2::theme_minimal(base_size = 12)

# Juntar em uma linha
gridExtra::grid.arrange(p1, p2, p3, ncol=3)
```

<br>

## Funções de perda

<br>

### O que são funções de perda?

- Funções de perda (*loss functions*) quantificam o erro cometido por um modelo ao comparar suas predições com os valores reais observados.[@REF]

- Funções de perda definem formalmente o objetivo do aprendizado, indicando o que significa “errar pouco” ou “errar muito” em um problema específico.[@REF]

- Durante o treinamento de modelos supervisionados, a função de perda orienta o ajuste dos parâmetros ao medir a discrepância entre saída prevista e desfecho verdadeiro.[@REF]

- Em redes neurais, a minimização da função de perda é realizada por métodos iterativos baseados em gradientes, como a retropropagação do erro.[@REF]

- A escolha da função de perda está intimamente ligada à natureza do problema (regressão, classificação, probabilidade, ranking) e influencia diretamente o espaço de decisão aprendido pelo modelo.[@REF]

<br>

### Quais são as funções de perda mais comuns?

- Erro quadrático médio (Mean Squared Error, MSE \@ref(eq:loss-mse)): Essa função penaliza erros grandes de forma mais severa, sendo adequada quando desvios elevados são indesejáveis e a média do erro quadrático é uma medida relevante de desempenho.[@REF]

<br>

\begin{equation}
(\#eq:loss-mse)
\mathcal{L}_{\mathrm{MSE}}(y, \hat{y})
= \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}

<br>

- Erro absoluto médio (Mean Absolute Error, MAE\@ref(eq:loss-mae)): Essa função atribui peso linear aos erros, tornando-se mais robusta a valores extremos quando comparada ao erro quadrático médio.[@REF]

<br>

\begin{equation}
(\#eq:loss-mae)
\mathcal{L}_{\mathrm{MAE}}(y, \hat{y})
= \frac{1}{n}\sum_{i=1}^{n} \lvert y_i - \hat{y}_i \rvert
\end{equation}

<br>

- Erro quadrático médio logarítmico (Mean Squared Logarithmic Error, MSLE \@ref(eq:loss-msle)): Essa função enfatiza erros relativos, sendo particularmente útil quando diferenças proporcionais são mais relevantes do que diferenças absolutas.[@REF]

<br>

\begin{equation}
(\#eq:loss-msle)
\mathcal{L}_{\mathrm{MSLE}}(y, \hat{y})
= \frac{1}{n}\sum_{i=1}^{n}
\left(\log(1+y_i) - \log(1+\hat{y}_i)\right)^2
\end{equation}

<br>

- Entropia cruzada binária (Binary Cross-Entropy, BCE \@ref(eq:loss-bce)): Essa função mede a discrepância entre probabilidades previstas e observadas, sendo o critério padrão em problemas de classificação binária probabilística.

<br>

\begin{equation}
(\#eq:loss-bce)
\mathcal{L}_{\mathrm{BCE}}(y, \hat{y})
= -\frac{1}{n}\sum_{i=1}^{n}
\left[
y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)
\right]
\end{equation}

<br>

- Entropia cruzada categórica (Categorical Cross-Entropy \@ref(eq:loss-cce)): Essa função generaliza a entropia cruzada para múltiplas classes, penalizando previsões probabilísticas inconsistentes com a classe verdadeira.[@REF]

<br>

\begin{equation}
(\#eq:loss-cce)
\mathcal{L}_{\mathrm{CCE}}(y, \hat{y})
= -\frac{1}{n}\sum_{i=1}^{n}
\sum_{k=1}^{K} y_{ik}\log(\hat{y}_{ik})
\end{equation}

<br>

- Função logística (log-verossimilhança negativa \@ref(eq:loss-logistic)): Essa função expressa o critério de máxima verossimilhança da regressão logística, conectando inferência estatística e aprendizado supervisionado.[@REF]

<br>

\begin{equation}
(\#eq:loss-logistic)
\mathcal{L}_{\mathrm{log}}(y, \hat{p})
= -\sum_{i=1}^{n}
\left[
y_i \log(\hat{p}_i) + (1-y_i)\log(1-\hat{p}_i)
\right]
\end{equation}

<br>

- Função hinge (*Support Vector Machines* \@ref(eq:loss-hinge)): Essa função busca maximizar a margem entre classes, penalizando classificações incorretas ou pouco confiantes em relação à fronteira de decisão.

<br>

\begin{equation}
(\#eq:loss-hinge)
\mathcal{L}_{\mathrm{hinge}}(y, f(x))
= \frac{1}{n}\sum_{i=1}^{n}
\max\left(0,\, 1 - y_i f(x_i)\right)
\end{equation}

<br>

## Espaço de decisão

<br>

### O que é espaço de decisão?

-   O espaço de decisão é a região do espaço de entrada onde o modelo classifica as entradas em diferentes categorias. Ele é definido pelas fronteiras de decisão aprendidas pelo modelo durante o treinamento.[@REF]

<br>

### Como ele é visualizado?

-   O espaço de decisão pode ser visualizado graficamente, especialmente em problemas de classificação binária ou multiclasse, onde as regiões correspondem às classes previstas pelo modelo.[@REF]

<br>

```{r perceptron-exemplo, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Espaço de decisão de um perceptron (regressão logística)."}
# Reprodutibilidade
set.seed(123)

# Gerar dados sintéticos para um problema de classificação linear
n <- 100
x1 <- runif(n, -1, 1)
x2 <- runif(n, -1, 1)
y <- ifelse(x1 + x2 > 0, 1, 0)
df <- data.frame(x1, x2, y = factor(y))

# Ajustar um modelo perceptron (regressão logística)
ggplot2::ggplot(df, ggplot2::aes(x1, x2, color=y)) +
  ggplot2::geom_point(size=3) +
  ggplot2::geom_abline(intercept=0, slope=-1, linetype="dashed") +
  ggplot2::theme_minimal() +
  ggplot2::labs(title="Perceptron: fronteira de decisão linear")
```

<br>

```{r espaco-de-decisao, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Comparação do espaço de decisão entre um modelo linear (regressão logística) e um modelo não linear (MLP)."}
## 1) Dados sintéticos (XOR)
base::set.seed(123)

make_xor <- function(n_per_quad = 100, sd = 0.25) {
  # Quatro centros (±1, ±1). Classe 1 nos quadrantes (1,1) e (-1,-1)
  centers <- matrix(c( 1,  1,
                       1, -1,
                      -1,  1,
                      -1, -1), ncol = 2, byrow = TRUE)
  cls <- c(1, 0, 0, 1)
  X <- NULL; y <- NULL
  for (i in seq_len(4)) {
    X <- rbind(X, cbind(stats::rnorm(n_per_quad, mean = centers[i,1], sd = sd),
                        stats::rnorm(n_per_quad, mean = centers[i,2], sd = sd)))
    y <- c(y, rep.int(cls[i], n_per_quad))
  }
  df <- data.frame(x1 = X[,1], x2 = X[,2], y = factor(y))
  df
}

df <- make_xor(n_per_quad = 120, sd = 0.28)

## 2) Ajustar modelos
# (a) Regressão logística (linear)
mod_glm <- stats::glm(y ~ x1 + x2, data = df, family = stats::binomial())

# (b) MLP com 1 camada escondida (size neurônios)
# nnet::nnet requer y numérico 0/1 para classificação; usamos linout=FALSE (logística)
y_num <- base::as.numeric(as.character(df$y))
mod_mlp <- nnet::nnet(x = as.matrix(df[, c("x1","x2")]),
                      y = y_num,
                      size = 6,            # neurônios na escondida
                      decay = 1e-3,        # regularização
                      maxit = 500,
                      linout = FALSE,      # saída logística
                      trace = FALSE)

## 3) Grade para visualizar fronteiras
x1_seq <- base::seq(base::min(df$x1) - 0.5, base::max(df$x1) + 0.5, length.out = 300)
x2_seq <- base::seq(base::min(df$x2) - 0.5, base::max(df$x2) + 0.5, length.out = 300)
grid <- base::expand.grid(x1 = x1_seq, x2 = x2_seq)

# Probabilidades preditas
grid$p_glm <- base::as.numeric(stats::predict(mod_glm, newdata = grid, type = "response"))
grid$p_mlp <- as.numeric(stats::predict(mod_mlp, newdata = as.matrix(grid[,c("x1","x2")])))
grid$cls_glm <- factor(ifelse(grid$p_glm >= 0.5, 1, 0))
grid$cls_mlp <- factor(ifelse(grid$p_mlp >= 0.5, 1, 0))

## 4) Plots com ggplot2
common_points <- ggplot2::geom_point(
  data = df, ggplot2::aes(x = x1, y = x2, color = y),
  size = 1.6, alpha = 0.9
)

# Paleta simples e tema limpo
theme_clean <- ggplot2::theme_minimal(base_size = 12) +
  ggplot2::theme(legend.position = "none",
                 plot.title = ggplot2::element_text(face = "bold"),
                 plot.subtitle = ggplot2::element_text(margin = ggplot2::margin(t = 2)))

# (a) Fronteira GLM (linear)
p_glm <- ggplot2::ggplot() +
  ggplot2::geom_raster(
    data = grid,
    ggplot2::aes(x = x1, y = x2, fill = cls_glm),
    alpha = 0.25, interpolate = TRUE
  ) +
  ggplot2::geom_contour(
    data = grid,
    ggplot2::aes(x = x1, y = x2, z = p_glm),
    breaks = 0.5, color = "black", linewidth = 0.7
  ) +
  common_points +
  ggplot2::scale_fill_manual(values = c("#6BAED6","#FD8D3C")) +
  ggplot2::scale_color_manual(values = c("#3182BD","#E6550D")) +
  ggplot2::labs(title = "Modelo Linear",
                subtitle = "Regressão logística (fronteira linear)",
                x = "x1", y = "x2") +
  theme_clean

# (b) Fronteira MLP (não linear)
p_mlp <- ggplot2::ggplot() +
  ggplot2::geom_raster(
    data = grid,
    ggplot2::aes(x = x1, y = x2, fill = cls_mlp),
    alpha = 0.25, interpolate = TRUE
  ) +
  ggplot2::geom_contour(
    data = grid,
    ggplot2::aes(x = x1, y = x2, z = p_mlp),
    breaks = 0.5, color = "black", linewidth = 0.7
  ) +
  common_points +
  ggplot2::scale_fill_manual(values = c("#6BAED6","#FD8D3C")) +
  ggplot2::scale_color_manual(values = c("#3182BD","#E6550D")) +
  ggplot2::labs(title = "Modelo Não Linear",
                subtitle = "Rede neural (1 camada escondida) — fronteira não linear",
                x = "x1", y = "x2") +
  theme_clean

# Painel lado a lado
gridExtra::grid.arrange(p_glm, p_mlp, ncol = 2)
```

<br>

## Redes neurais multicamadas

<br>

### O que são redes neurais multicamadas?

<br>

- Redes neurais multicamadas são redes neurais artificiais que contêm uma ou mais camadas escondidas entre a camada de entrada e a camada de saída, permitindo a composição sucessiva de transformações não lineares e, consequentemente, maior capacidade de representação de funções complexas.[@REF]

- Essa estrutura amplia o espaço de decisão do modelo sem exigir especificação explícita de interações ou transformações entre variáveis.[@REF]

- À medida que a profundidade da rede aumenta, passa-se do ajuste de parâmetros para o aprendizado de representações internas dos dados, caracterizando um regime distinto de modelagem conhecido como aprendizado profundo.[@REF]

<br>

```{r rede-neural-multicamadas, echo = FALSE, warning = FALSE, message = FALSE, appendix = TRUE, fig.align = 'center', layout='Blank', ph=officer::ph_location_fullsize(), results = "asis", fig.fullwidth = TRUE, fig.cap = "Representação esquemática de uma rede neural multicamadas com 2 camadas escondidas além das camadas de entrada e saída."}
## ===== configuração geral =====
graphics::par(mar = c(0,0,0,0))
diagram::openplotmat()  # sem título

# raios/estética
r_node <- 0.045   # raio dos neurônios (círculos)
lwd_ar <- 1.4     # espessura das arestas
arr_len <- 0.25   # tamanho da seta

# posições (x fixo por camada; y distribuído verticalmente)
x_in   <- 0.10
x_h1   <- 0.35
x_h2   <- 0.60
x_out  <- 0.85

y_in   <- c(0.60, 0.40)        # 2 entradas
y_h1   <- c(0.70, 0.50, 0.30)  # 3 na primeira oculta
y_h2   <- c(0.70, 0.50, 0.30)  # 3 na segunda oculta
y_out  <- 0.50                 # 1 saída

# pontos (x,y) por camada
pos_in  <- cbind(rep(x_in,  length(y_in)),  y_in)
pos_h1  <- cbind(rep(x_h1,  length(y_h1)),  y_h1)
pos_h2  <- cbind(rep(x_h2,  length(y_h2)),  y_h2)
pos_out <- cbind(x_out, y_out)

## ===== helper: encurtar setas para encostar na borda do círculo =====
encurta <- function(p1, p2, dist) {
  v <- p2 - p1
  v <- v / sqrt(sum(v^2))
  p2 - v * dist
}

## ===== desenhar nós (círculos) =====
# entradas
for (i in seq_len(nrow(pos_in))) {
  diagram::textellipse(pos_in[i,], r_node, r_node, lab = paste0("x", i),
                       box.col = "lightblue")
}

# hidden 1
for (j in seq_len(nrow(pos_h1))) {
  diagram::textellipse(pos_h1[j,], r_node, r_node, lab = paste0("h1", j),
                       box.col = "lightgreen")
}

# hidden 2
for (k in seq_len(nrow(pos_h2))) {
  diagram::textellipse(pos_h2[k,], r_node, r_node, lab = paste0("h2", k),
                       box.col = "khaki1")
}

# saída
diagram::textellipse(pos_out, r_node, r_node, lab = "y",
                     box.col = "salmon")

## ===== conectar camadas (totalmente conectadas) =====
# entradas -> hidden1
for (i in seq_len(nrow(pos_in))) {
  for (j in seq_len(nrow(pos_h1))) {
    from <- encurta(pos_h1[j,], pos_in[i,],  r_node)  # sai da borda do nó origem
    to   <- encurta(pos_in[i,], pos_h1[j,],  r_node)  # chega na borda do nó alvo
    diagram::straightarrow(from = from, to = to, arr.pos = 0.9,
                           arr.length = arr_len, lwd = lwd_ar)
  }
}

# hidden1 -> hidden2
for (j in seq_len(nrow(pos_h1))) {
  for (k in seq_len(nrow(pos_h2))) {
    from <- encurta(pos_h2[k,], pos_h1[j,],  r_node)
    to   <- encurta(pos_h1[j,], pos_h2[k,],  r_node)
    diagram::straightarrow(from = from, to = to, arr.pos = 0.9,
                           arr.length = arr_len, lwd = lwd_ar)
  }
}

# hidden2 -> saída
for (k in seq_len(nrow(pos_h2))) {
  from <- encurta(pos_out, pos_h2[k,], r_node)
  to   <- encurta(pos_h2[k,], pos_out, r_node)
  diagram::straightarrow(from = from, to = to, arr.pos = 0.9,
                         arr.length = arr_len, lwd = lwd_ar)
}

## ===== rótulos de camadas (opcional; pode remover) =====
graphics::text(x_in,  0.85, "Entradas",  cex = 0.9)
graphics::text(x_h1,  0.85, "Camada\nEscondida 1",  cex = 0.9)
graphics::text(x_h2,  0.85, "Camada\nEscondida 2",  cex = 0.9)
graphics::text(x_out, 0.85, "Saída",     cex = 0.9)
```

<br>

```{r, echo=FALSE, warning=FALSE, results='asis', eval=knitr::is_html_output()}
cat(readLines("citation.html"), sep = "\n")
```

<br>
