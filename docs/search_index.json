[["index.html", "Ciência com Perguntas e respostas para pesquisadores e analistas de dados Sobre o livro", " Ciência com Perguntas e respostas para pesquisadores e analistas de dados © 2023 by Arthur de Sá Ferreira https://orcid.org/0000-0001-7014-2002 Atualizado em 10/09/2023 Sobre o livro .boxBorder { border: 2px solid black; padding: 5px; outline-offset: 5px; } Como citar: Ferreira, Arthur de Sá. Ciência com R: Perguntas e respostas para pesquisadores e analistas de dados. Rio de Janeiro: 1a edição, 2023. 78p. doi: 10.5281/zenodo.8320233 A versão online desta obra está licenciada com uma Licença Creative Commons Atribuição-NãoComercial 4.0 Internacional. "],["sobre-o-autor.html", "Sobre o autor", " Sobre o autor Arthur de Sá Ferreira Obtive minha graduação em Fisioterapia pela Universidade Federal do Rio de Janeiro (UFRJ, 1999), Formação em Acupuntura pela Academia Brasileira de Arte e Ciência Oriental (ABACO, 2001), Mestrado em Engenharia Biomédica pela Universidade Federal do Rio de Janeiro (UFRJ, 2002) e Doutorado em Engenharia Biomédica pela Universidade Federal do Rio de Janeiro (UFRJ, 2006). Tenho experiência em docência no ensino superior, atuando com professor da graduação em cursos de Fisioterapia, Enfermagem e Odontologia, entre outros (2001-atual); pós-graduação lato sensu em Fisioterapia (2001-atual) e stricto sensu em Ciências da Reabilitação (2010-atual). Sou professor adjunto do Centro Universitário Augusto Motta (UNISUAM), pesquisador dos Programas de Pós-graduação em Ciências da Reabilitação (PPGCR) e Desenvolvimento Local (PPGDL) e Coordenador do Comitê de Ética em Pesquisa (CEP) (2020-atual). Fundei o Laboratório de Simulação Computacional e Modelagem em Reabilitação (LSCMR), onde desenvolvo projetos de pesquisa principalmente nos seguintes temas: Bioestatística, Modelagem e simulação computacional, Processamento de sinais biomédicos, Movimento funcional humano, Medicina tradicional (chinesa), Distúrbios musculoesqueléticos, Doenças cardiovasculares e Doenças respiratórias. Sou membro efetivo da Associação Brasileira de Pesquisa e Pós-Graduação em Fisioterapia (ABRAPG-FT) (2007-atual), Committee on Publication Ethics (COPE) (2018-atual), Consórcio Acadêmico Brasileiro de Saúde Integrativa (CABSIN) (2019-atual) e Royal Statistical Society (RSS) (2021-atual). Componho o corpo editoral dos periódicos internacionais e nacionais: Scientific Reports, Frontiers in Rehabilitation Sciences, Evidence-Based Complementary and Alternative Medicine, Chinese Journal of Integrative Medicine, Journal of Integrative Medicine, Fisioterapia e Pesquisa. Currículos externos  5432142731317894 0000-0001-7014-2002  F-6831-2012 "],["prefácio.html", "Prefácio", " Prefácio "],["unidade-analise.html", "1 Unidade de análise 1.1 Definição 1.2 Pareamento 1.3 Medidas únicas ou múltiplas", " 1 Unidade de análise 1.1 Definição 1.1.1 O que é uma unidade de análise? A unidade de análise de pesquisas na área de saúde geralmente é o indivíduo. Outras possibilidades são instituições em estudos multicêntricos (ex.: hospitais, clínicas) ou estudos publicados em meta-análises (ex.: ensaios clínicos).1 É fundamental identificar corretamente a unidade de análise para evitar inflação do tamanho da amostra (ex.: medidas bilaterais resultando em o dobro de participantes), violações de suposições dos testes de hipótese (ex.: independência entre medidas e/ou unidade de análise) e resultados espúrios em testes de hipótese (ex.: p-valores menores que aqueles observados se a amostra não estivesse inflada).1,2 1.2 Pareamento 1.2.1 O que é pareamento? Pareamento significa que para cada participante de um grupo (por exemplo, com alguma condição clínica), existe um (ou mais) participantes (por exemplo, grupo controle) que possui características iguais ou similares relativas a algumas variáveis de interesse.3 As variáveis escolhidas para pareamento devem ter relação com as variáveis de desfecho, mas não são de interesse elas mesmas.3 O ajuste por pareamento deve ser incluído nas análises estatísticas mesmo que as variáveis de pareamento não sejam consideradas prognósticas ou confundidores na amostra estudada.3 A ausência de evidência estatística de diferença entre grupos não é considerada pareamento.3 1.3 Medidas únicas ou múltiplas 1.3.1 Como podem ser coletadas as informações da unidade de análise? Da unidade de análise podem ser coletadas informações em medidas únicas, repetidas, seriadas ou múltiplas. 1.3.1.1 Medidas únicas A medida única da pressão arterial sistólica no braço esquerdo resulta em um valor pontual. O valor pontual será considerado representativo da variável para a unidade de análise (ex.: 120 mmHg para o participante #9). Medidas únicas obtidas de diferentes unidades de análise podem ser consideradas independentes se observadas outras condições na coleta de dados. Unidade de análise Pressão arterial, braço esquerdo (mmHg) 1 118 2 113 3 116 4 110 5 111 6 116 7 120 8 111 9 120 10 112 1.3.1.2 Medidas repetidas A medida repetida da pressão arterial no braço esquerdo, resultando em um conjunto de valores pontuais (ex.: 110 mmHg, 118 mmHg e 116 mmHg para o participante #5). As medidas repetidas podem ser tabuladas separadamente, por exemplo para análise da confiabilidade de obtenção dessa medida. Unidade de análise Pressão arterial, braço esquerdo (mmHg) #1 Pressão arterial, braço esquerdo (mmHg) #2 Pressão arterial, braço esquerdo (mmHg) #3 1 114 112 112 2 115 120 113 3 115 110 120 4 117 116 114 5 110 118 116 6 110 120 113 7 118 114 117 8 111 112 119 9 120 112 117 10 110 115 115 As medidas repetidas podem ser agregadas por algum parâmetro — ex.: média, mediana, máximo, mínimo, entre outros—, observando-se a relevância biológica, clínica e/ou metodológica desta escolha. O valor agregado será considerado representativo da variável para a unidade de análise (ex.: média = 115 mmHg para o participante #5). Medidas agregadas obtidas de diferentes unidades de análise podem ser consideradas independentes se observadas outras condições na coleta de dados. Unidade de análise Pressão arterial, braço esquerdo (mmHg) média 1 112.6667 2 116.0000 3 115.0000 4 115.6667 5 114.6667 6 114.3333 7 116.3333 8 114.0000 9 116.3333 10 113.3333 1.3.1.3 Medidas seriadas Medidas seriadas são possivelmente relacionadas e, portanto, dependentes na mesma unidade de análise. Por exemplo, a medida seriada da pressão arterial no braço esquerdo, em intervalos tipicamente regulares (ex.: 114 mmHg, 120 mmHg e 110 mmHg em 1 min, 2 min e 3 min, respectivamente, para o participante #1). Unidade de análise Tempo (min) Pressão arterial, braço esquerdo (mmHg) 1 1 114 1 2 120 1 3 110 2 1 119 2 2 120 2 3 114 3 1 116 3 2 114 3 3 116 4 1 113 Medidas seriadas também agregadas por parâmetros — ex.: máximo, mínimo, amplitude — são consideradas representativas da variação temporal ou de uma característica de interesse (ex.: amplitude = 10 mmHg para o participante #1). Unidade de análise Pressão arterial, braço esquerdo (mmHg) amplitude 1 10 2 6 3 2 4 6 5 1 6 8 7 9 8 10 9 7 10 5 1.3.1.4 Medidas múltiplas A medida de pressão arterial bilateral resulta em um conjunto de valores pontuais (ex.: braço esquerdo = 114 mmHg, braço direito = 118 mmHg para o participante #8). Neste caso, ambos os valores pontuais são considerados representativos daquela unidade de análise. Medidas múltiplas também são possivelmente relacionadas e portanto são dependentes na mesma unidade de análise. Medidas múltiplas podem ser obtidas de modo repetido para análise agregada ou seriada. Unidade de análise Pressão arterial, braço esquerdo (mmHg) Pressão arterial, braço direito (mmHg) 1 117 115 2 120 118 3 112 118 4 112 112 5 116 112 6 112 118 7 115 113 8 114 118 9 119 114 10 112 116 Referências "],["dados-metadados.html", "2 Dados e metadados 2.1 Dados 2.2 Metadados 2.3 Dados perdidos", " 2 Dados e metadados 2.1 Dados 2.1.1 O que são dados? “Tudo são dados”.4 2.1.2 O que são dados primários e secundários? Dados primários são dados originais coletados intencionalmente para uma determinada análise exploratória ou inferencial planejada a priori.5 Dados secundários compreendem dados coletados inicialmente para análises de um estudo, e são subsequentemente utilizados para outras análises.5 2.2 Metadados 2.2.1 O que são metadados? Metadados são informações técnicas relacionadas às variáveis do estudo, tais como rótulos, limites de valores plausíveis, códigos para dados perdidos e unidades de medida.6 Metadados também são informações relacionadas ao delineamento e/ou protocolo do estudo, recrutamento dos participantes, e métodos para realização das medidas.6 2.2.2 O que são dados primários e secundários? Dados primários são dados originais coletados intencionalmente para uma determinada análise exploratória ou inferencial planejada a priori.5 Dados secundários compreendem dados coletados inicialmente para análises de um estudo, e são subsequentemente utilizados para outras análises.5 2.3 Dados perdidos 2.3.1 O que são dados perdidos? Dados perdidos são dados não coletados de um ou mais participantes, para uma ou mais variáveis.7 2.3.2 Qual o problema de um estudo ter dados perdidos? Uma grande quantidade de dados perdidos pode comprometer a integridade científica do estudo, considerando-se que o tamanho da amostra foi estimado para observar um determinado tamanho de efeito mínimo.7 Perda de participantes no estudo por dados perdidos pode reduzir o poder estatístico (erro tipo II).7 Não existe solução globalmente satisfatória para o problema de dados perdidos.7 2.3.3 Quais os mecanismos geradores de dados perdidos? Dados perdidos completamente ao acaso (MCAR), em que os dados perdidos estão distribuídos aleatoriamente nos dados da amostra.8,9 Dados perdidos ao acaso (MAR), em que a probabilidade de ocorrência de dados perdidos é relacionada a outras variáveis medidas.8,9 Dados perdidos não ao acaso (MNAR), em que a probabilidade da ocorrência de dados perdidos é relacionada com a própria variável.8,9 2.3.4 Como identificar o mecanismo gerador de dados perdidos? Por definição, não é possível avaliar se os dados foram perdidos ao acaso (MAR) ou não (MNAR).8 Testes t e regressões logísticas podem ser aplicados para identificar relações entre variáveis com e sem dados perdidos, criando um fator de análise (‘dado perdido’ = 1, ‘dado observado’ = 0).8 O pacote misty10 fornece a função na.test para executar o Little’s Missing Completely at Random (MCAR) test11. 2.3.5 Que estratégias podem ser utilizadas na coleta de dados quando há expectativa de perda amostral? Na expectativa de ocorrência de dados perdidos, recomenda-se ampliar o tamanho da amostra com um % correspondente à tal estimativa - embora ainda não corriga potenciais vieses pela perda.7 2.3.6 Que estratégias podem ser utilizadas na análise de dados quando há dados perdidos? Na ocorrência de dados perdidos, a análise mais comum compreende apenas os ‘casos completos’, com exclusão de participantes com algum dado perdido nas variáveis do estudo. Em casos de grande quantidade de dados perdidos, pode-se perder muito poder estatístico (erro tipo II elevado).7 A análise de dados completos é válida quando pode se argumentar que que a probabiidade de o participante ter dados completos depende apenas das covariáveis e não dos desfechos.9 A análise de dados completos é eficiente quando todos os dados perdidos estão no desfecho, ou quando cada participante com dados perdidos nas covariáveis também possui dados perdidos nos desfechos.9 O pacote stats12 fornece a função complete.cases para identificar os casos completos - isto é, sem dados perdidos - em um banco de dados. Na ocorrência de dados perdidos, a imputação de dados (substituição por dados simulados plausíveis preditos pelos dados presentes) pode ser uma alternativa para manter o erro tipo II estipulado no plano de análise.7 A análise com imputação de dados pode ser útil quando pode-se argumentar que os dados foram perdidos ao acaso (MAR); quando o desfecho foi observado e os dados perdidos estão nas covariáveis; e variáveis auxiliares - preditoras do desfecho e não dos dados perdidos - estão disponíveis.9 Em dados longitudinais co um pequeno número de ‘ondas’ (medidas repetidas) e poucas variáveis, para análise com modelos de regressão univariados, a imputação via especificação condicional completa - também conhecido como imputação multivariada por equações encadeadas (multivaraite imputation by chained equations, MICE) - é eficiente do ponto de vista computacional e possui acurácia e precisão para estimação de parâmetros.8,13 Os pacotes mice14 e miceadds15 fornecem funções para imputação de dados 2.3.7 Que estratégias podem ser utilizadas na redação de estudos em que há dados perdidos? Informar: o número de participantes com dados perdidos; diferenças nas taxas de dados perdidos entre os braços do estudo; os motivos dos dados perdidos; o fluxo de participantes; quaisquer diferenças entre os participantes com e sem dados perdidos; o padrão de ausência (por exemplo, se é aleatória); os métodos para tratamento de dados perdidos das variáveis em análise; os resultados de quaisquer análises de sensibilidade; as implicações dos dados perdidos na interpretação do resultados.16 Referências "],["variaveis-fatores.html", "3 Variáveis e fatores 3.1 Variáveis 3.2 Transformação de variáveis 3.3 Dicotomização de variáveis 3.4 Fatores", " 3 Variáveis e fatores 3.1 Variáveis 3.1.1 O que são variáveis? Variáveis são informações que podem variar entre medidas em diferentes indivíduos ou repetições.17 Variáveis definem características de uma amostra extraída da população, tipicamente observados por aplicação de métodos de amostragem (isto é, seleção) da população de interesse.5 3.1.2 Como são classificadas as variáveis? Quanto à informação: Quantitativa ou qualitativa.5,18–20 Quanto ao conteúdo: contínua (intervalo ou razão; discreta ou contínua), categórica ordinal (numérica discreta ou nominal) ou categórica nominal (multinominal ou dicotômica).5,18–20 Quanto à interpretação: dependente (desfecho), independente (preditora, covariável, confundidora, controle), mediadora, moderadora, modificadora ou auxiliar.5,18–20 3.1.3 Por que é importante classificar as variáveis corretamente? Identificar corrtamente os tipos de variáveis da pesquisa é uma das etapas da escolha dos métodos estatísticos adequados para as análises e reprentações no texto, tabelas e gráficos.19 3.2 Transformação de variáveis 3.2.1 O que é transformação de variáveis contínuas? Transformação significa aplicar uma função matemática à variável medida em sua unidade original.21 A transformação visa atender aos pressupostos dos modelos estatísticos quanto à distribuição da variável, em geral a distribuição Gaussiana.5,21 3.2.2 Por que transformar variáveis? Muitos procedimentos estatísticos supõem que as variáveis - ou seus termos de erro, mais especificamente - são normalmente distribuídas. A violação dessa suposição pode aumentar suas chances de cometer um erro do tipo I ou II.22 Mesmo quando se está usando análises consideradas “robustas” para violações dessas suposições ou testes não paramétricos (que não assumem explicitamente termos de erro normalmente distribuídos), atender a essas questões pode melhorar os resultados das análises (por exemplo, Zimmerman, 1995).22 3.2.3 Quais transformações podem ser aplicadas? Distribuições com assimetria à direita: raiz quadrada, logaritmo natural, logaritmo base 10, transformação inversa.22 Distribuições com assimetria à esquerda: reflexão e raiz quadrada, reflexão e logaritmo natural, reflexão e logaritmo base 10, reflexão e transformação inversa.22 Transformação arco-seno.22 Transformação de Box-Cox.23 O pacote MASS24 fornece a função box.cox para executar a transformação de Box-Cox.23 3.3 Dicotomização de variáveis 3.3.1 É recomendado categorizar ou dicotomizar variáveis contínuas? Categorizar variáveis não é necessário para conduzir análises estatísticas. Ao invés de categorizar (ou dicotomizar), priorize as variáveis contínuas.25–27 Em geral, não existe uma justificativa racional (plausibilidade biológica) para assumir que as categorias artificiais subjacentes existam.25–27 Dicotomização causa perda de informação e consequentemente perda de poder estatístico para detectar efeitos.25 Dicotomização também classifica indivíduos com valores próximos na variável contínua como indivíduos em pontos opostos e extremos, artificialmente sugerindo que são muito diferentes.25 Dicotomização pode diminuir a variabilidade das variáveis.25 Dicotomização pode ocultar não-linearidades presentes na variável contínua.25 A mediana, embora amplamente utilizada, não é um bom parâmetro para dicotomizar variáveis.25 Caso exista um ponto de corte ou limiar verdadeiro que discrimina dois ou mais grupos, identificá-lo ainda pode ser um desafio.28 3.3.2 Quais métodos podem ser usados para dicotomizar variáveis contínuas? Em termos de tabelas de contingência 2x2, os seguintes métodos permitem28 a identificação do limiar verdadeiro: Youden,29 Gini Index,30 estatística qui-quadrado,31 risco relativo32 e kappa33. 3.4 Fatores 3.4.1 O que são fatores? [REF]. Referências "],["questões-fundamentais.html", "4 Pensamento estatístico 4.1 Amostragem 4.2 Aleatorização 4.3 Probabilidade", " 4 Pensamento estatístico 4.1 Amostragem 4.2 Aleatorização 4.3 Probabilidade 4.3.1 O que é probabilidade? Probabilidade pode ser definida como a possibilidade de que um evento ocorrerá.18 Probabilidade é quantificada no intervalo de 0 a 1, de modo que p = 0 representa um evento impossível (com certeza não ocorrrá) e p = 1 representa um evento que certamente ocorrerá.18 Referências "],["distribuicoes-parametros.html", "5 Distribuições e parâmetros 5.1 Distribuições 5.2 Parâmetros", " 5 Distribuições e parâmetros 5.1 Distribuições 5.1.1 O que são distribuições? Uma distribuição estatística é uma função que descreve os valores possíveis ou o intervalo de valores de uma variável (eixo horizontal) e a frequência com que cada valor é observado (eixo vertical).5 5.1.2 Quais caracterísiucas definem uma distribuição? Uma distribuição pode ser definida por modelos matemáticos e caracterizada por sua tendência central, dispersão, simetria, curtose. Em uma distribuição normal, o intervalo de 1 desvio-padrão (±1DP) inclui cerca de 68% dos dados; de 2 desvios-padrão (±2DP) cerca de 95% dos dados; e no intervalo de 3 desvios-padrão (±3DP) cerca de 99% dos dados.18 5.1.3 O que é a distribuição normal? A distribuição normal (ou Gaussiana) é uma distribuição com desvios simétricos positivos e negativos em torno de um valor central.18 Em uma distribuição normal, o intervalo de 1 desvio-padrão (±1DP) inclui cerca de 68% dos dados; de 2 desvios-padrão (±2DP) cerca de 95% dos dados; e no intervalo de 3 desvios-padrão (±3DP) cerca de 99% dos dados.18 5.1.4 O que são distribuições não-normais? .[REF] 5.1.5 Que métodos podem ser utilizados para identificar a normalidade da distribuição? Histogramas.5 Gráficos Q-Q.5 testes de hipótese de Kolmogorov-Smirnov, Shapiro-Wilk, Anderson-Darling.5 5.2 Parâmetros 5.2.1 O que são parâmetros? Parâmetros são informações que definem um modelo teórico, como propriedades de uma coleção de indivíduos.17 Parâmetros definem características de uma população inteira, tipicamente não observados por ser inviável ter acesso a todos os indivíduos que constituem tal população.5 5.2.2 Quais são os parâmetros mais comuns? Parâmetros de tendência central: médial, mediana, moda.18,34 Parâmetros de dispersão: variância, desvio-padrão, amplitude, intervalo interquartil, intervalo de confiança.18,34 Parâmetos de proporção: frequência absoluta, frequência relativa, percentil, quantil.18,34,35 Parâmetros de distribuição: assimetria, curtose.34 Extremos: mínimo, máximo.18 Referências "],["computacao-estatistica.html", "6 Computação estatística 6.1 Por onde começar 6.2 Outros programas", " 6 Computação estatística 6.1 Por onde começar 6.1.1 O que é R? R é um programa de computador com linguagem computacional direcionada para análise estatística.36 6.1.2 O que são scripts? Um script é um arquivo de texto contendo (quase) os mesmos comandos que você digitaria na linha de comando do R. (Quase) refere-se ao fato de que se você estiver usando sink() para enviar a saída para um arquivo, você terá que incluir alguns comandos em print() para obter a mesma saída da linha de comando (CRAN). 6.1.3 Quais práticas são recomendadas na redação de scripts? Use nomes consistentes para as variáveis.37 Defina os tipos de variáveis adequatamente no banco de dados.37 Defina constantes - isto é, variáveis de valor fixo - ao invés de digitar valores.37 Use e cite os pacotes disponíveis para suas análises.37 Controle as versões do script.37,38 Teste o script antes de sua utilização.37 Conduza revisão por pares do código durante a redacão (digitação em dupla).37 6.1.4 O que pode ser compartilhado? Idealmente, todos os scripts, pacotes/biliotecas e dados necessários para outros reproduzirem seus dados.38 Minimamente, partes importantes incluindo implementações de novos algoritmos e dados que permitam reproduzir um resultado importante.38 6.1.5 Como preparar os scripts para compartilhamento? Crie links persistentes para versões do seu script.38 Escolha uma licença apropriada para garantir como outros usarão seus scripts.38 Providencie a documentação sobre seu script (ex.: arquivos README).38 Compartilhar todos os pacotes relacionados à sua análise.39 O pacote formatR40 fornece funções para formatar um ou mais scripts. O pacote pkglite41 permite submeter documentos eletrônicos com todos os pacotes necessários. 6.1.6 Como instalar o R e RStudio? R version 4.3.1 (2023-06-16) RStudio Pacotes We used R version 4.3.142 and the following R packages: bookdown v. 0.3543,44, devtools v. 2.4.545, emo v. 0.0.0.900046, fontawesome v. 0.5.247, kableExtra v. 1.3.4.900048, knitr v. 1.4349–51, remotes v. 2.4.2.152, rmarkdown v. 2.2453–55, tidyverse v. 2.0.056, webshot v. 0.5.557. 6.2 Outros programas 6.2.1 Que outros programas de computador podem ser usados? JASP.58 jamovi.59 G*Power.60,61 Referências "],["tabulacao-dados.html", "7 Tabulação de dados 7.1 Planilhas eletrônicas 7.2 Análise inicia de dados", " 7 Tabulação de dados 7.1 Planilhas eletrônicas 7.1.1 Qual a organização de uma tabela de dados? Cada variável possui sua própria coluna (vertical).62 Cada observação possui sua própria linha (horizontal).62 Cada valor possui sua própria célula (linha, coluna).62 7.1.2 Qual a estrutura básica de uma tabela para análise estatística? Em geral, use apenas uma planilha para conter todas as informações coletadas. Evite múltiplas abas no mesmo arquivo, assim como múltiplos arquivos quando possível.63 Use apenas 1 (uma) linha de cabeçalho para nomear os fatores e variáveis do seu estudo.63 Tipicamente, cada linha representa um participante e cada coluna representa uma variável ou fator do estudo. Estudos com medidas repetidas dos participantes podem conter múltiplas linhas para o mesmo participante (repetindo os dados na mesma coluna, conhecido como formato curto) ou só uma linha para o participante (repetindo os dados em colunas separadas, conhecido como formato longo ).64 V1 V2 V3 V4 \\(x_{1,1}\\) \\(x_{1,2}\\) \\(x_{1,3}\\) \\(x_{1,4}\\) \\(x_{2,1}\\) \\(x_{2,2}\\) \\(x_{2,3}\\) \\(x_{2,4}\\) \\(x_{3,1}\\) \\(x_{3,2}\\) \\(x_{3,3}\\) \\(x_{3,4}\\) \\(x_{4,1}\\) \\(x_{4,2}\\) \\(x_{4,3}\\) \\(x_{4,4}\\) \\(x_{5,1}\\) \\(x_{5,2}\\) \\(x_{5,3}\\) \\(x_{5,4}\\) 7.1.3 O que usar para organizar tabelas para análise computadorizada? Seja consistente em: códigos para as variáveis categóricas; códigos para dados perdidos; nomes das variáveis; identificadores de participantes; nome dos arquivos; formato de datas; uso de caracteres de espaço.63,64 Use recursos para validação de dados antes e durante a digitação de dados.63,64 O pacote data.table65 fornece a função melt para reorganizar a tabela em diferentes formatos. 7.1.4 O que não usar para organizar tabelas para análise computadorizada? Não deixe células em branco: substitua dados perdidos por um código sistemático (ex.: NA [not available]).63 Não inclua análises estatísticas ou gráficos nas tabelas de dados brutos.63 Não utilize cores como informação. Se necessário, crie colunas adicionais - variáveis instrumentais ou auxiliares - para identificar a infomação de modo que possa ser analisada.63 Não use células mescladas. Delete linhas e/ou colunas totalmente em branco (sem unidades de análise e/ou sem variáveis). Recomendado ID Data.Coleta Estado.Civil Numero.Filhos 1 11-09-2023 casado NA 2 12-09-2023 casado 1 3 13-09-2023 casado NA 4 14-09-2023 solteiro NA 5 15-09-2023 casado NA 6 16-09-2023 solteiro 0 7 17-09-2023 solteiro NA 8 18-09-2023 solteiro NA 9 19-09-2023 casado NA 10 20-09-2023 solteiro NA Evite ID Data de Coleta Estado Civil Número de Filhos 1 11-09-2023 casado NA 2 12-09-2023 Casado 1 3 13-09-2023 casado NaN 4 14-09-2023 Solteiro N/A 5 15-09-2023 Casado N.A. 6 16-09-2023 solteiro 0 7 17-09-2023 solteiro 8 18-09-2023 Solteiro na 9 19-09-2023 casado n.a. 10 20-09-2023 Solteiro 999 7.2 Análise inicia de dados 7.2.1 O que é análise inicial de dados? Análise inicial de dados é uma sequência de procedimentos que visam principalmente a transparência e integridade das pré-condições do estudo para conduzir a análise estatística apropriada de modo responsável para responder aos problemas da pesquisa.6 O objetivo da análise inicial de dados é propiciar dados prontos para análise estatística, incluido informações confiáveis sobre as propriedades dos dados.6 A análise inicial de dados pode ser divida nas seguintes etapas: (1) configuração dos metadados; (2) limpeza dos dados; (3) verificação dos dados; (4) relatório inicial dos dados; (5) refinamento e atualização do plano de análise estatística; e (6) documentação e relatório da análise inicial de dados.6 A análise inicial de dados não deve ser confundida com análise exploratória66, nem deve ser utilizada para hipotetizar após os dados serem coletados (conhecido como Hypothesizing After Results are Known, HARKing)67. 7.2.2 Como conduzir uma análise inicial de dados? Desenvolva um plano de análise inicial de dados consistente com os objetivos da pesquisa. Por exemplo, verifique a distribuição e escala das variáveis, procure por observações não-usuais ou improváveis, avalie possíveis padrões de dados perdidos.6 Não altere diretamente os dados de uma tabela obtida de uma fonte. Use scripts para implementar eventuais alterações, de modo a manter o registro de todas as modificações realizadas no banco de dados.6 Use os metadados do estudo para guiar a análise inicial dos dados e compartilhe com os dados para maior transparência e reprodutibilidade.6 Representação gráfica dos dados pode ajudar a identificar características e padrões no banco de dados, tais como suposições e tendências.6 Verifique a frequência e proporção de dados perdidos em cada variável, e depois examine por padrões de dados perdidos simultaneamente por duas ou mais variáveis.6 Verifique a frequência e proporção de dados perdidos em cada variável, e depois examine por padrões de dados perdidos simultaneamente por duas ou mais variáveis.6 7.2.3 Quais problemas podem ser detectados na análise inicial de dados? Registros duplicados, que devem ser excluídos para não inflar a amostra.68 Codificação 0 ou 1 para variáveis dicotômicas para representar a direção esperada da associação entre elas.68 Ordenação cronológica de variáveis com registros temporais (retrospectivos ou prospectivos).68 A distribuição das variáveis para verificação das suposições das análises planejadas.68 Ocorrência de efeitos teto e piso nas variáveis.68 Referências "],["analise-descritiva.html", "8 Análise descritiva 8.1 Análise descritiva", " 8 Análise descritiva 8.1 Análise descritiva 8.1.1 O que é análise descritiva? A análise descritiva utiliza métodos para calcular, descrever e resumir os dados coletados da(s) amostra(s) de modo que sejam interpretadas adequatamente.5 As análises descritivas geralmente compreendem a apresentação quantitativa (numérica) em tabelas e/ou gráficos.5 O pacote dataExplorer69 fornece a função create_report para executar análise explotarória. 8.1.2 Que parâmetros populacionais podem ser estimados para descrição? Parâmetros de tendência central: médial, mediana, moda.18,34 Parâmetros de dispersão: variância, desvio-padrão, amplitude, intervalo interquartil, intervalo de confiança.18,34 Parâmetos de proporção: frequência absoluta, frequência relativa, percentil, quantil.18,34,35 Parâmetros de distribuição: assimetria, curtose.34 Extremos: mínimo, máximo.18 8.1.3 O que são quantis? Quantis são pontos de corte que definem a divisão da amostra em grupos de tamanhos iguais. Portanto, nào se referem aos grupos em si, mas aos valores que os dividem.35 Tercis dividem a amostra em 3 grupos tal que 33,3% das observações ficam em cada grupo.35 Quartis dividem a amostra em 4 grupos tal que 25% das observações ficam em cada grupo.35 Quintis dividem a amostra em 5 grupos tal que 20% das observações ficam em cada grupo.35 Decis dividem a amostra em 10 grupos tal que 10% das observações ficam em cada grupo.35 Centis dividem a amostra em 100 grupos tal que 1% das observações ficam em cada grupo.35 O pacote stats42 fornece a função quantile para executar análise de percentis. Referências "],["tabelas-graficos-fluxogramas.html", "9 Tabelas, Gráficos e Fluxogramas 9.1 Tabelas 9.2 Gráficos 9.3 Fluxogramas", " 9 Tabelas, Gráficos e Fluxogramas 9.1 Tabelas 9.1.1 Por que usar tabelas? Tabelas complementam o texto (e vice-versa), e podem apresentar os dados de modo mais acessível e informativo.70 9.1.2 Como construir a Tabela 1? A Tabela 1 geralmente é utilizada para descrever as características da amostra estudada, possibilitando a análise de ameaças à validade interna e/ou externa ao estudo.71 Inclua na tabela: título ou legenda, uma síntese descritiva (geralmente por meio de parâmetros descritivos), intervalos de confiança e/ou p-valores conforme necessário para adequada interpretação.70,72 O pacote table173 fornece funções para construção da ‘Tabela 1’ 9.1.3 Como construir a Tabela 2? A Tabela 2 pode ser utilizada para apresentar estimativas de mutiplos efeitos ajustados de um mesmo modelo estatístico.74 O pacote table173 fornece funções para construção da ‘Tabela 2’ 9.1.4 Como exportar a tabela para o manuscrito? .[REF] 9.2 Gráficos 9.2.1 O que são gráficos? Gráficos são utilizados para apresentar dados (geralmente em grande quantidade) de modo mais intuitivo e fácil de compreender.75 9.2.2 Que elementos incluir em gráficos? Título, eixos horizontal e vertical com respectivas unidades, escalas em intervalos representativos das variáveis, legenda com símbolos, síntese descritiva dos valores e respectiva margem de erro, conforme necessário para adequada interpretação.75 Os pacotes ggplot276, plotly77 e corrplot78 fornecem funções para construção de gráficos 9.2.3 Para que servem as barras de erro em gráficos? Barras de erro ajudam ao autor a apresentar as informações que descrevem os dados (por exmeplo, em uma análise descritiva) ou sobre as inferências ou conclusões tomadas a partir de dados.79 Barras de erro mais longas representam mais imprecisão (maiores erros), enquanto barras mais curtas representam mais precisão na estimativa.79 Barras de erro descritivas geralmente apresentam a amplitude (mínimo-máximo) ou desvio-padrão.79 Barras de erro inferenciais geralmente apresentam o erro-padrão ou intervalo de confiança (por exemplo, de 95%).79 O cumprimento das barras de erro sugerem graficamente a imprecisão dos dados do estudo, uma vez que o valor verdadeiro da população pode estar em qualquer nível do intervalo da barra.79 9.2.4 Quais são as boas práticas na elaboração de gráficos? O tamanho da amostra total e subgrupos, se houver, deve estar descrito na figura ou na sua legenda.79 Para análise inferencial de figuras, as barras de erro representadas por erro-padrão ou intervalo de confiança são preferíveis à amplitude ou desvio-padrão.79 Evite gráficos de barra e mostre a distribuição dos dados sempre que possível.80 Exiba os pontos de dados em boxplots.80 Use jitter simétrico em gráficos de pontos para permitir a visualização de todos os dados.80 Prefira palhetas de cor adaptadas para daltônicos.80 O pacote ggsci81 fornece palhetas de cores inspiradas em publicações científicas para uso em gráficos 9.2.5 Como exportar o gráfico para o manuscrito? .[REF] 9.3 Fluxogramas 9.3.1 O que são fluxogramas? Fluxogramas são figuras que apresentam as etapas de um estudos. 9.3.2 Que fluxogramas podem ser incluídos? Inclua fluxogramas com o delineamento aplicado no estudo.80 A EQUATOR Network disponibiliza modelos de fluxogramas para os mais diversos delineamentos de estudo.82 O pacote PRISMA202083,84 fornece funções para elaboração do fluxograma de revisões sistemáticas no formato padrão Referências "],["analise-inferencial.html", "10 Análise inferencial 10.1 Análise inferencial 10.2 Tamanho do efeito 10.3 P-valor 10.4 Erros de inferência 10.5 Tipos de testes inferencias", " 10 Análise inferencial 10.1 Análise inferencial 10.1.1 O que é análise inferencial? Na análise inferencial são utilizados dados da(s) amostra(s) para fazer uma inferência válida (isto é, estimativa) sobre os parâmetros populacionais desconhecidos.5 10.1.2 Que hipótese está sendo testada? A hipótese nula (ou H0) é a hipótese sob teste em análises infereciais.18 10.1.3 O que é uma análise ad hoc? .[REF] 10.1.4 O que é uma análise post hoc? .[REF] 10.2 Tamanho do efeito 10.2.1 O que é o tamanho do efeito? .[REF] 10.3 P-valor 10.3.1 O que é o P-valor? .[REF] 10.4 Erros de inferência 10.4.1 O que é erro tipo I? .[REF] 10.4.2 O que é erro tipo II? .[REF] 10.5 Tipos de testes inferencias 10.5.1 O que é um teste paramétrico? Testes paramétricos possuem suposições sobre as características e/ou parâmetros da distribuição dos dados na população.5 Testes paramétricos assumem que: a variável é quantitativa numérica (contínua); os dados foram amostrados de uma população com distribuição normal; a variância da(S) amostra(s) é igual à da população; as amostras foram selecionadas de modo aleatório na população; os valores de cada amostra são independentes entre si.5,18 10.5.2 O que é um teste não paramétrico? Testes não-paramétricos fazem poucas suposições, ou menos rigorosas, sobre as características e/ou parâmetros da distribuição dos dados na população.5,18 Testes não-paramétricos são úteis quando as suposições de normalidade não podem ser sustendadas.18 10.5.3 Por que os testes paramétricos são preferidos? Em geral, testes paramétricos são mais robustos (isto é, possuem menores erros tipo I e II) que seus testes não-paramétricos correspondentes.5 Testes não-paramétricos apresentam menor poder estatítsico (maior erro tipo II) comparados aos testes paramétricos correspondentes.18 Referências "],["analise-comparacao.html", "11 Análise de comparação", " 11 Análise de comparação "],["analise-correlacao.html", "12 Análise de correlação", " 12 Análise de correlação "],["analise-regresaao.html", "13 Análise de regressão 13.1 Multivariável vs. Multivariada 13.2 Seleção de variáveis", " 13 Análise de regressão 13.1 Multivariável vs. Multivariada 13.1.1 O que são as análises de regressão multivariável e multivariada? a análise multivariável consiste em 1 variável dependente e múltiplas variáveis dependentes.85 A análise multivariada consiste em múltiplas variáveis dependentes.85 O pacote modelsummary86 fornece as funç˜es modelsummary e modelplot para gerar tabelas e gráficos de coeficientes de regressão. 13.2 Seleção de variáveis 13.2.1 Correlação bivariada pode ser usada para seleção de variáveis? Seleção bivariada de variáveis - isto é, aplicação de testes de correlação em pares de variáveis candidatas e variável de desfecho afim de selecionar quais serão incluídas no modelo multivariável - é um dos erros mais comuns na literatura.87,88 A seleção bivariada de variáveis torna o modelo mais suscetível a otimismo no ajuste se as variáveis de confundimento não são adequadamente controladas.87,88 13.2.2 O que pode ser feito para reduzir o número de variáveis candidatas a um modelo multivariável? Verifique a existência de multicolinearidade entre as variáveis candidatas.88 Em caso de uma proporção baixa entre o número de participantes e de variáveis, use o conhecimento prévio da literatura para selecionar um pequeno conjunto de variáveis candidatas.88 Colapse categorias com contagem nula (células com valor igual a 0) de variáveis candidatas.88 Use simulações de dados para identificar qual(is) variável(is) está(ão) causando problemas de convergência do ajuste do modelo.88 Referências "],["analise-desempenho-diagnostico.html", "14 Análise de desempenho diagnóstico 14.1 Tabelas 2x2 14.2 Curvas ROC", " 14 Análise de desempenho diagnóstico 14.1 Tabelas 2x2 14.1.1 Como analisar o desempenho diagnóstico em tabelas 2x2? Acurácia.[REF] Sensibilidade.[REF] Especificidade.[REF] Valor preditivo positivo[REF] Valor preditivo negativo.[REF] 14.2 Curvas ROC 14.2.1 Como analisar o desempenho diagnóstico em desfechos com distribuição trimodal na população? Limiares duplos podem ser utilizados para análise de desempenho diagnóstico de testes com distribuição trimodal.89 Referências "],["analise-concordancia-confiabilidade.html", "15 Análise de concordância e confiabilidade 15.1 Concordância 15.2 Confiabilidade", " 15 Análise de concordância e confiabilidade 15.1 Concordância 15.2 Confiabilidade "],["delineamento-estudos.html", "16 Delineamento de estudos 16.1 Classificação", " 16 Delineamento de estudos 16.1 Classificação 16.1.1 Como podem ser classificados os estudos científicos? Estudos científicos podem ser classificados em básicos, observacionais, experimentais, acurácia diagnóstica, propriedades psicométricas, avaliação econômica e revisões de literatura:90–99 Estudos básicos91,96 Genética Celular Experimentos com animais Desenvolvimento de métodos Estudos de simulação computacional97,99 Estudos observacionais91,96 Descritivo Estudo de caso Série de casos Transversal Analítico Transversal Caso-Controle Caso-Controle aninhado Caso-Coorte Coorte prospectiva ou retrospectiva Estudos de acurácia diagnóstica95,98 Transversal Caso-Controle Comparativo Totalmente pareado Parcialmente pareado com subgrupo aleatório Parcialmente pareado com subgrupo não aleatório Não pareado aleatório Não pareado não aleatório Estudos de propriedades psicométricas92,94 Validade Confiabilidade Concordância Estudos quase-experimentais93 Quase-aleatorizado controlado Estimação de variável instrumental Discontinnuidade de regressão Série temporal interrompida controlada Série temporal interrompida Diferença Estudos experimentais91,96 Fases I a IV Aleatorizado controlado Não-aleatorizado controlado Autocontrolado Cruzado Fatorial Campo Comunitário Estudos de avaliação econômica91 Análise de custo Análise de minimização de custo Análise de custo-utilidade Análise de custo-efetividade Análise de custo-benefício Estudos de revisão90 Crítica Narrativa Mapeamento Sistemática Meta-análise Mista Visão geral Sistemática qualitativa Rápida Escopo Estado-da-arte Busca e revisão sistemática Sistematizada Guarda-chuva Referências "],["tamanho-amostral.html", "17 Tamanho da amostra", " 17 Tamanho da amostra "],["reamostragem.html", "18 Reamostragem", " 18 Reamostragem "],["vieses.html", "19 Vieses", " 19 Vieses "],["ensaios-clínicos-aleatorizados.html", "20 Ensaios clínicos aleatorizados 20.1 Características 20.2 Comparação na linha de base 20.3 Análises de comparação 20.4 Ajuste de covariáveis 20.5 Interação", " 20 Ensaios clínicos aleatorizados 20.1 Características 20.1.1 Quais são as características dos ensaios clínnicos randomizados? Quanto à unidade de alocação: individual vs. agrupado.100 Quanto ao número de braços: único vs. multiplos.100 Quanto ao número de centros: único vs. multiplos.100 Quanto ao cegamento: aberto vs. simples-cego vs. duplo-cego vs. tripo-cego vs. quádruplo-cego.100 Quanto à alocação: sem sorteio vs. estratificada (centro apenas) vs. estratificada vs. minimizada vs. estratificada e minimisada.100 20.2 Comparação na linha de base 20.2.1 Quais estratégias podem ser adotadas sobre comparação entre grupos na linha de base em ensaios clínicos randomizados? Na fase de projeto: identifique as variáveis prognósticas do desfecho de acordo com a literatura.101 Na fase de análise: inclua as variáveis prognósticas nos modelos para ajuste.101 20.2.2 Por quê não se deve comparar grupos na linha de base em ensaios clínicos randomizados? Quando a randomização é bem-sucedida, a hipótese nula de difença entre grupos na linha de base é verdadeira.101 Testes de significância estatística na linha de base avaliam a probabilidade de que as diferenças observadas possam ter ocorrido por acaso; no entanto, já sabemos - pelo delineamento do experimento - que quaisquer diferenças são causadas pelo acaso.102 20.3 Análises de comparação 20.3.1 Que modelos podem ser utilizados para comparações? As abordagens compreendem a comparação da variável de desfecho medida entre os momentos antes e depois ou da sua mudança (pre - pós) entre os momentos.103 Se a média da variável é igual entre grupos no início do acompanhamento, ambas abordagens estimam o mesmo efeito. Caso contrário, o efeito será influenciado pela correlação entre as medidas antes e depois. A análise da mudança não controla para desbalanços no início do estudo.103 Uma abordagem recomendada é a análise de covariância (ANCOVA), pois ajusta os valores pós-intervenção aos valores pré-intervenção para cada participante, e não é afetada pelas diferenças entr grupos no início do estudo.103 A análise de covariância (ANCOVA) modelando seja a mudança (pré - pós) quando o desfecho no pós-tratamento parece ser o método mais efetivo considerando-se o viés de estimação dos parâmetros, a precisão das estimativas, a cobertura nominal (isto é, intervalo de confiança) e o poder do teste.104 Análise de variância (ANOVA) e modelos lineares mistos (MLM) são outras opções de métodos, embora apresentem maior variância, menor poder, e cobertura nominal comparados à ANCOVA.104 20.4 Ajuste de covariáveis 20.4.1 Quais variáveis devem ser utilizadas no ajuste de covariáveis? A escolha das características de linha de base pelas quais uma análise é ajustada deve ser determinada pelo conhecimento prévio de uma possível influência no resultado, em vez da evidência de desequilíbrio entre os grupos de tratamento no estudo.101 20.4.2 Quais os benefícios do ajuste de covariáveis? Ajustar por covariáveis ajuda a estimar os efeitos do tratamento para o indívíduo, assim como aumenta a eficiência dos testes para hipótese nula e a validade externa do estudo.105 Incluir a variável de desfecho medida na linha de base como covariável - independentemente da análise ser realizada com a medida pós-tratamento da mesma variável ou a diferença para a linha de base - pode aumentar o poder estatístico do estudo.106 Incluir outras variáveis medidas na linha de base, com potencial para serem desbalanceadas entre grupos após a randomização, diminui a chance de afetar as estimativas de efeito dos tratamentos.106 20.4.3 Quais os riscos do ajuste de covariáveis? Incluir covariáveis que não são prognósticas do desfecho pode reduzir o poder estatístico do estudo.106 Incluir covariáveis com dados perdidos pode reduzir o tamanho amostral e consequentemente o poder estatístico do estudo (análise de casos completos) ou levar a desvios do plano de análise por exclusão de covariáveis prognósticas.106 20.4.4 Como lidar com os dados perdidos em covariáveis? Imputação de dados perdidos de uma variável pela média dos dados observada permite estimativas não enviesadas dos efeitos do tratamento, preserva o erro tipo I e aumenta o poder estatístico comparado à análise de dados completos.106 20.5 Interação 20.5.1 O que é efeito de interação? A interação - representada pelo símbolo ‘*’ - é o termo estatístico empregado para representar a heterogeneidade de um determinado efeito.107 20.5.2 Quando usar o termo de interação? Análise de efeito de interação pode ser usada para testar se o efeito de um tratamento varia entre dois ou mais subgrupos de indivíduos, ou seja, se um efeito é modificado pelo(s) outros(s) efeito(s).107 A interação entre duas (ou mais) variáveis pode ser utilizada para comparar efeitos do tratamento em subgrupos de ensaios clínicos.108 O poder estatístico para detectar efeitos de interação é limitado.108 20.5.3 Por que analisar o efeito de interação? A comparação de subgrupos por meio de testes de significância de hipótese nula separados é enganoso por não testar (comparar) diretamente os tamanhos dos efeitos dos tratamentos.109 Referências "],["meta-analises.html", "21 Meta-análises 21.1 Interpretação", " 21 Meta-análises 21.1 Interpretação 21.1.1 Como avaliar a variação do tamanho do efeito? O interlalo de predição contém informação sobre a variação do tamanho do efeito.110 Se o intervalo de predição não contém a hipótese nula (\\(H_{0}\\)), podemos concluir que (a) o tratamento funciona igualmente bem em todas as populações, ou que ele funciona melhor em algumas populações do que em outras.110 Se o intervalo de predição contém a hipótese nula (\\(H_{0}\\)), podemos concluir que o tratamento pode ser benéfico em algumas populações mas prejudicial em outras, de modo que a estimativa pontual (geralmente a média) torna-se amplamente irrelevante. Nesse caso, é recomendado investigar em que populações o tratamento seria benéfico e onde causaria danos.110 21.1.2 Como avaliar a heterogeneidade entre os estudos? A heterogeneidade - variação não-aleatória - no efeito do tratamento entre os estudos incluídos em uma meta-análise pode ser avalada pelo \\(I^{2}\\).110,111 \\(I^{2}\\) representa qual proporção da variância observada reflete a variância nos efeitos verdadeiros em vez do erro de amostragem.110 \\(I^{2}\\) não depende da quantidade de estudos incluídos na meta-análise. Entretanto, \\(I^{2}\\) aumenta com a quantidade de participantes incluídos nos estudos meta-analisados.111 A heterogeneidade entre estudos é explicada de modo mais confiável utilizando dados de pacientes individuais, uma vez que a direção verdadeira da modificação de efeito não pode ser observado a partir de dados agregados no estudo.112 O pacote metagear113 fornece funções para condução e análise de revisões sistemáticas Referências "],["redacao.html", "22 Redação estatística 22.1 Diretrizes 22.2 Checklists", " 22 Redação estatística 22.1 Diretrizes 22.1.1 Quais diretrizes estão disponíveis? Review of guidance papers on regression modeling in statistical series of medical journals.114 Principles and recommendations for incorporating estimands into clinical study protocol templates.115 How to write statistical analysis section in medical research.116 Recommendations for Statistical Reporting in Cardiovascular Medicine: A Special Report From the American Heart Association.117 Framework for the treatment and reporting of missing data in observational studies: The Treatment And Reporting of Missing data in Observational Studies framework.118 Guidelines for reporting of figures and tables for clinical research in urology.119 Who is in this study, anyway? Guidelines for a useful Table 1.71 Guidelines for Reporting of Statistics for Clinical Research in Urology.120 Reveal, Don’t Conceal: Transforming Data Visualization to Improve Transparency.80 Guidelines for the Content of Statistical Analysis Plans in Clinical Trials.121 Basic statistical reporting for articles published in Biomedical Journals: The ‘’Statistical Analyses and Methods in the Published Literature’’ or the SAMPL Guidelines.122 Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm.123 STRengthening analytical thinking for observational studies: the STRATOS initiative.124 Research methods and reporting.125 How to ensure your paper is rejected by the statistical reviewer.126 22.2 Checklists 22.2.1 O que são checklists? Checklists são listas que podem ser utilizadas por autores, revisores ou editores, durante as fases de planejamento ou apresentação, para redação ou revisão de textos que relatam anaálises estatísticas. 22.2.2 Por que usar checklists? Checklists têm sido recomendados para melhorar o relato das análses realizadas, aumentar a transparência do estudo e reprodutibilidade dos achados.127 Trabalhos acadêmicos que relatam análises de dados devem ser passar por revisão por pares que inclua apreciação da análise estatóstica, e sua adequaçào ao delineamento do estudo e instrumentos utilizados.128 Checklists não são suficientes para garantir a qualidade técnica da pesquisa, mas podem conitribuir para a revisão por pares.128 22.2.3 Quais checklists estão disponíveis? A CHecklist for statistical Assessment of Medical Papers (the CHAMP statement): explanation and elaboration.129 Checklist for clinical applicability of subgroup analysis.130 Evidence‐based statistical analysis and methods in biomedical research (SAMBR) checklists according to design features.131 Referências "],["agradecimentos.html", "Agradecimentos", " Agradecimentos "],["fontes-externas.html", "23 Fontes externas 23.1 RSS 23.2 BMJ 23.3 JAMA 23.4 AHA/ASA 23.5 NPG 23.6 Wiley", " 23 Fontes externas 23.1 RSS Best Practices for Data Visualisation - Royal Statistical Society 23.2 BMJ Statistics - Latest from The BMJ Statistics notes - Latest from The BMJ Statistics and research methods - Latest from The BMJ Research methods &amp; reporting 23.3 JAMA JAMA Guide to Statistics and Methods - JAMA 23.4 AHA/ASA Statistical Reporting Recommendations - AHA/ASA journals Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05 - The American Statistical Association 23.5 NPG Statistics for Biologists - Nature Publising Group 23.6 Wiley Tutorials in Biostatistics Papers - Wiley Online Library "],["referências.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
