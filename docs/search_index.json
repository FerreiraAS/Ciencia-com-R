[["index.html", "Ciência com R Perguntas e respostas para pesquisadores e analistas de dados Capa", " Ciência com R Perguntas e respostas para pesquisadores e analistas de dados © 2023-2025 by Arthur de Sá Ferreira https://orcid.org/0000-0001-7014-2002 Capa A versão online desta obra está licenciada com uma Licença Creative Commons Atribuição-NãoComercial 4.0 Internacional.Ciência com R por Arthur de Sa Ferreira está licenciada sob a \" href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache License 2.0. Atualizado em 11/06/2025 "],["sobre-o-autor.html", "Sobre o autor", " Sobre o autor Arthur de Sá Ferreira Obtive minha Graduação em Fisioterapia pela Universidade Federal do Rio de Janeiro (UFRJ, 1999), Formação em Acupuntura pela Academia Brasileira de Arte e Ciência Oriental (ABACO, 2001), Mestrado em Engenharia Biomédica pela Universidade Federal do Rio de Janeiro (UFRJ, 2002) e Doutorado em Engenharia Biomédica pela Universidade Federal do Rio de Janeiro (UFRJ, 2006). Tenho experiência em docência no ensino superior, atuei com professor da graduação em cursos de Fisioterapia, Enfermagem e Odontologia, entre outros (2001-2018); pós-graduação lato sensu em Fisioterapia (2001-atual) e stricto sensu níveis mestrado e doutorado (2010-atual). Como pesquisador, sou Professor Adjunto do Centro Universitário Augusto Motta (UNISUAM), atuando nos Programas de Pós-graduação em Ciências da Reabilitação (PPGCR; 2009-atual) e Desenvolvimento Local (PPGDL; 2018-atual). Também sou pesquisador do Instituto D’Or de Pesquisa e Ensino (IDOR; 2024-atual). Fundei o Laboratório de Simulação Computacional e Modelagem em Reabilitação (LSCMR) em 2012, onde desenvolvo projetos de pesquisa principalmente nos seguintes temas: Bioestatística, Modelagem e simulação computacional, Processamento de sinais biomédicos, Movimento funcional humano, Medicina tradicional (chinesa), Distúrbios musculoesqueléticos, Doenças cardiovasculares e Doenças respiratórias. Dentre os editais públicos que obtive financimento, destaco o Jovem Cientista do Nosso Estado (JCNE; 2012-2015; 2015-2017) e Cientista do Nosso Estado (2021-atual) pela Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ; e Bolsista Produtividade em Pesquisa nível PQ2 pelo Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq; 2021-atual). Como gestor, estou na Coordenação do Programa de Pós-Graduação stricto sensu em Ciências da Reabilitação (PPGCR; 2016-atual). Atuei como coordenador do Comitê de Ética em Pesquisa (CEP) do Centro Universitário Augusto Motta (UNISUAM; 2020-2024) e como Coordenador do Curso de Graduação em Fisioterapia da Universidade Salgado de Oliveira (UNIVERSO; 2004-2009). Atualmente coordeno o Curso Superior de Tecnologia em Radiologia da Faculdade IDOR de Ciências Médicas (IDOR; 2024-atual). Sou membro efetivo da Associação Brasileira de Pesquisa e Pós-Graduação em Fisioterapia (ABRAPG-FT) (2007-atual), Consórcio Acadêmico Brasileiro de Saúde Integrativa (CABSIN) (2019-atual), Committee on Publication Ethics (COPE) (2018-atual) e Royal Statistical Society (RSS) (2021-atual). Componho o corpo editorial e de revisores de periódicos nacionais e internacionais como Scientific Reports, Frontiers in Rehabilitation Sciences, The Journal of Clinical Hypertension, Chinese Journal of Integrative Medicine, Journal of Integrative Medicine, Brazilian Journal of Physical Therapy, Fisioterapia e Pesquisa. Currículos externos 5432142731317894 0000-0001-7014-2002 F-6831-2012 "],["dedicatória.html", "Dedicatória", " Dedicatória Esta obra é dedicada a todos que, em princípio, buscam conhecimento para melhorar a qualidade da pesquisa científica - seja a sua própria, a de colegas ou a de desconhecidos - mas, em última análise, desejam mesmo prover melhores condições de saúde e desenvolvimento da sociedade. Dedico também ao leitor eventual que chegou aqui por acaso. "],["agradecimentos.html", "Agradecimentos", " Agradecimentos Este trabalho não seria possível sem o apoio e suporte da minha esposa Daniele, minha irmã Mônica, meu pai José Victorino, minha mãe Angela (in memoriam) e meus filhos Giovanna, Victor e Lucas. "],["prefácio.html", "Prefácio", " Prefácio No âmbito da análise estatística de dados, os processos envolvidos são marcados por uma série de escolhas críticas. Estas decisões abrangem considerações metodológicas e ações operacionais que moldam toda a jornada analítica. Deve-se selecionar, cuidadosamente, um delineamento de estudo para enfrentar os desafios únicos colocados por um projeto de pesquisa. Além disso, a escolha de métodos estatísticos adequados para lidar com os dados gerados pelo delineamento escolhido tem um peso importante. Estas decisões necessitam de uma base construída sobre as evidências mais convincentes da literatura existente e na adesão a práticas sólidas de investigação. Interpretar os resultados destas análises não é uma tarefa simples. Confiar apenas na formação educacional convencional, no bom senso e na intuição para decifrar tabelas e gráficos pode revelar-se inadequado. Interpretações errôneas podem gerar consequências indesejáveis, incluindo a utilização de testes diagnósticos imprecisos ou o endosso de tratamentos ineficazes. Este livro emerge do reconhecimento desses desafios. A proposta gira em torno da organização de um compêndio abrangente de métodos e técnicas de ponta, para análise estatística de dados em pesquisa científica, apresentados em formato de perguntas e respostas. Esse formato promove um diálogo direto e objetivo com o leitor, respondendo a dúvidas comumente colocadas por alunos de graduação, pós-graduação, mestrado e doutorado, bem como por pesquisadores. O objetivo geral de cada capítulo é elucidar as questões metodológicas fundamentais: “O que é?”, “Por que usar?”, “Quando usar?”, “Quando não usar?” e “Como fazer?”. Em cada capítulo, diversas questões específicas são propostas e respondidas sistematicamente, permitindo ao leitor uma melhor elaboração do conteúdo e resultado do seu trabalho. Os capítulos foram organizados para seguir uma progressão de conceitos e aplicações. Embora sejam fragmentados para maior clareza instrucional, as referências cruzadas ajudam a mitigar a fragmentação do conteúdo e reforçar a interconexão dos tópicos. O público-alvo compreende pesquisadores, professores, analistas de dados, profissionais e estudantes que regularmente lidam com a tomada de decisões em pesquisa. Os estudantes de pós-graduação encontrarão aqui uma obra repleta de exemplos para adaptar na análise dos dados de seus projetos de pesquisa. Professores de graduação e pós-graduação terão acesso a uma obra didática de referência, direcionada para seus alunos. Pesquisadores e analistas de dados iniciantes descobrirão um valioso acervo de informações e referências para a construção de projetos e manuscritos. Pesquisadores e os cientistas mais experientes podem recorrer às referências e esclarecimentos mais atuais sobre vieses, paradoxos, mitos e mal práticas em pesquisa. E mesmo os leitores não familiarizados ainda com as técnicas de análise de dados em pesquisa terão a oportunidade de apreciar o papel fundamental de colocar e responder suas perguntas na busca do conhecimento científico. Arthur de Sá Ferreira "],["parte-1---fundamentos.html", "Parte 1 - Fundamentos", " Parte 1 - Fundamentos "],["pensamento-probabilístico.html", "Capítulo 1 Pensamento probabilístico 1.1 Experimento 1.2 Espaço amostral e eventos discretos 1.3 Espaço amostral e eventos contínuos 1.4 Probabilidade 1.5 Independência e probabilidade 1.6 Leis dos números anômalos 1.7 Leis dos pequenos números 1.8 Leis dos grandes números 1.9 Teorema central do limite 1.10 Regressão para a média", " Capítulo 1 Pensamento probabilístico 1.1 Experimento 1.1.1 O que é um experimento? Um experimento é um processo de simulação ou medição cujo resultado é chamado de desfecho.1 Tentativa se refere a uma repetição de um experimento aleatório.1 Em um experimento aleatório, o desfecho de cada tentativa é imprevisível.1 1.2 Espaço amostral e eventos discretos 1.2.1 O que é espaço amostral discreto? O espaço amostral \\(S\\) de um experimento aleatório é definido como o conjunto de todos os desfechos possíveis de um experimento.1 Em probabilidade discreta, o espaço amostral \\(S\\) pode ser enumerado e contato.1 Figura 1.1: Exemplos de espaço amostral discreto. Superior: Todas as faces de uma moeda. Inferior: Todas as faces de um dado. 1.2.2 O que é evento discreto? Um evento \\(E\\) é um único desfecho ou uma coleção de desfechos.1 Um evento \\(E\\) é um subconjunto do espaço amostral \\(S\\) de um experimento.1 Figura 1.2: Exemplos de evento de experimento. Superior: 1 lançamento de 1 moeda. Inferior: 1 lançamento de 1 dado. 1.2.3 O que é espaço de eventos discretos? Um espaço de eventos \\(E_{s}\\) também é um subconjunto do espaço amostral \\(S\\) de um experimento.1 A união de dois eventos \\(E_{1} \\cup E_{2}\\) é o conjunto de todos os desfechos que estão em ambos.1 A intersecção de dois eventos \\(E_{1} \\cap E_{2}\\), ou evento conjunto, é o conjunto de todos os desfechos que estão em ambos os eventos.1 O complemento de um evento \\(E^C\\) consiste em todos os desfechos que não estão incluídos no evento \\(E\\).1 Figura 1.3: Espaço de eventos: União dos eventos face = 3 e face = 4 de um dado. 1.3 Espaço amostral e eventos contínuos 1.3.1 O que é espaço amostral contínuo? .REF? 1.3.2 O que é evento contínuo? .REF? 1.3.3 O que é espaço de eventos contínuo? .REF? 1.4 Probabilidade 1.4.1 O que é probabilidade? Com um espaço amostral \\(S\\) finito e não vazio de desfechos igualmente prováveis, a probabilidade \\(P\\) de um evento \\(E\\) é a razão entre o número de desfechos no evento \\(E\\) e o número de desfechos no espaço amostral \\(S\\).1 Um evento \\(E\\) impossível não contém um desfecho e, portanto, nunca ocorre: \\(P(E)=0\\).1,2 Um evento \\(E\\) é certo consiste em qualquer um dos desfechos possíveis e, portanto, sempre ocorre: \\(P(E)=1\\).1 1.4.2 Quais são os axiomas da probabilidade? A probabilidade de um evento é um número real que satisfaz os seguintes axiomas descritos por Andrei Nikolaevich Kolmogorov em 1950:1,2 Axioma I. Probabilidades de um evento \\(E\\) são números não-negativos: \\(P(E) \\geq 0\\). Axioma II. Probabilidade de todos os eventos do espaço amostral \\(A\\) ocorrerem é 100%: \\(P(S)=1\\). Axioma III. A probabilidade de um conjunto k de eventos mutuamente exclusivos é igual a soma da probabilidade de cada evento: \\(P(E_{1} \\cup E_{2} \\cup ... E_{k}) = P(E_{1}) + P(E_{2}) + ... + P(E_{k})\\). Os axiomas possuem as seguintes consequências:1 A soma da probabilidade de dois eventos que dividem o espaço amostral é 100%: \\(P(E)+P(E)^C=1\\). O valor máximo de probabilidade de um evento é 100%: \\(P(S)≤1\\). A probabilidade é uma função não decrescente do número de desfechos de um evento. 1.5 Independência e probabilidade 1.5.1 O que é independência em estatística? Em experimentos aleatórios, é comum assumir que os eventos de tentativas separadas são independentes devido a independência física de eventos e experimentos.1 Se a ocorrência do evento \\(E_{1}\\) não tiver efeito na ocorrência do evento \\(E_{2}\\), os eventos \\(E_{1}\\) e \\(E_{2}\\) são considerados estatisticamente independentes. Eventos são mutuamente exclusivos, ou disjuntos, se a ocorrência de um exclui a ocorrência dos outros.1 Se dois eventos \\(E_{1}\\) e \\(E_{2}\\) são mutuamente exclusivos, então os eventos \\(E_{1}\\) e \\(E_{2}\\) não podem ocorrer ao mesmo tempo e, portanto, são eventos dependentes. Em experimentos independentes, o desfecho de uma tentativa é independente dos desfechos de outras tentativas, passadas e/ou futuras. Uma tentativa em um experimento aleatório é independente se a probabilidade de cada desfecho possível não mudar de tentativa para tentativa.1 Figura 1.4: Esquerda: Evento (face = 4). Direita: Experimentos de 1 lançamento de 1 dado (superior), 3 lançamentos de 1 dado (central), 10 lançamentos de 1 dado (inferior). 1.5.2 O que é probabilidade marginal? Probabilidade marginal é a probabilidade de ocorrência de um evento \\(E\\) independentemente da(s) probabilidade(s) de outro(s) evento(s).1 1.5.3 O que é probabilidade conjunta? Probabilidade conjunta é a probabilidade de ocorrência de dois ou mais eventos independentes \\(E_{1}\\), \\(E_{2}\\), …, \\(E_{k}\\), independentemente da(s) probabilidade(s) de outro(s) evento(s).1 Se a probabilidade conjunta dos eventos é nula (\\(E_{1} \\cup E_{2} = 0\\)), esses dois eventos \\(E_{1}\\) e \\(E_{2}\\) são mutuamente exclusivos ou disjuntos.1 1.5.4 O que é probabilidade condicional? Probabilidade condicional é a probabilidade de ocorrência do evento \\(E_{2}\\) quando se sabe que o evento \\(E_{1}\\) já ocorreu \\(P(E_{2} | E_{1})\\).1 A probabilidade condicional \\(P(E_{2} | E_{1})\\) representa que a ocorrência do evento \\(E_{1}\\) fornece informação sobre a ocorrência do evento \\(E_{2}\\).1 Se a ocorrência do evento \\(E_{1}\\) tiver alguma influência na ocorrência do evento \\(E_{2}\\), então a probabilidade condicional do evento \\(E_{2}\\) dado o evento \\(E_{1}\\) pode ser maior ou menor do que a probabilidade marginal.1 1.6 Leis dos números anômalos 1.6.1 O que é a lei dos números anômalos? A lei dos números anômalos - lei de Benford - é uma distribuição de probabilidade que descreve a frequência de ocorrência do primeiro dígito em muitos conjuntos de dados do mundo real.3 1.7 Leis dos pequenos números 1.7.1 O que é a lei dos pequenos números? A crença exagerada na probabilidade de replicar com sucesso os achados de um estudo, pela tendência de se considerar uma amostra como representativa da população.4 A crença na lei dos pequenos números se refere à tendência de superestimar a estabilidade das estimativas provenientes de estudos com amostras pequenas.5 Quando se percebe um padrão, pode não ser possível identificar se tal padrão é real.6 1.7.2 Quais são as versões da lei dos pequenos números? 1a Lei Forte dos Pequenos Números: “Não há pequenos números suficientes para atender às muitas demandas que lhes são feitas”.6 2a Lei Forte dos Pequenos Números: “Quando dois números parecem iguais, não são necessariamente assim”.7 1.8 Leis dos grandes números 1.8.1 O que é a lei dos grandes números? A lei dos grandes números descreve que, ao realizar o mesmo experimento \\(E\\) um grande número de vezes (\\(n\\)), a média \\(\\mu\\) dos resultados obtidos tende a se aproximar do valor esperado \\(E[\\bar{X}]\\) à medida que mais experimentos forem realizados (\\(n \\to \\infty\\)).REF? De acordo com a lei dos grandes números, a média amostral converge para a média populacional à medida que o tamanho da amostra aumenta.REF? 1.8.2 Quais são as versões da lei dos grandes números? Lei Fraca dos Grandes Números (de Poisson): ““.REF? Lei Fraca dos Grandes Números (de Bernoulli): ““.REF? Lei Forte dos Grandes Números: ““.REF? 1.9 Teorema central do limite 1.9.1 O que é teorema central do limite? O teorema central do limite - equação (1.1) - afirma que, para uma amostra aleatória de tamanho \\(n\\) de uma população com valor esperado igual à média \\(E[\\bar{X_{i}}] = \\mu\\) e variância \\(Var[\\bar{X_{i}}]\\) igual a \\(\\sigma^{2}\\), a distribuição amostral da média de uma variável \\(\\bar{X}\\) se aproxima de uma distribuição normal \\(N\\) com média \\(\\mu\\) e variância \\(\\sigma^{2}/n\\) à medida que \\(n\\) aumenta (\\(n \\to \\infty\\)):8 \\[\\begin{equation} \\tag{1.1} \\sqrt{n}(\\bar{X} - \\mu) \\xrightarrow{n \\to \\infty} N(0, \\sigma^2) \\end{equation}\\] O teorema central do limite demonstra que se o tamanho da amostra \\(n\\) for suficientemente grande, a distribuição amostral das médias obtidas utilizando reamostragem com substituição será aproximadamente normal, com média \\(\\mu\\) e variância \\(\\sigma^{2}/n\\), independentemente da distribuição da população.8 No exemplo abaixo, uma variável aleatória numérica com distribuição uniforme no espaço amostral \\(S=[18;65]\\) tem média \\(\\mu\\) = 38.53 e variância \\(\\sigma^{2}\\) = 172.433. A distribuição amostral da média de 100 amostras de tamanho 5, 50, 500 e 5000 tomadas da população com reposição e igual probabilidade se aproxima de uma distribuição normal com média \\(\\mu\\) = 38.493 e variância \\(\\sigma^{2}\\) = 0.038, independentemente da distribuição da população: Figura 1.5: Esquerda: Histogramas de uma variável aleatória com distribuição uniforme (N = 100). Direita: Histogramas da média de 100 amostras de tamanhos 5, 50, 500 e 5000 tomadas da população com reposição e igual probabilidade. Em outro exemplo, o lançamento de um dado com distribuição uniforme no espaço amostral \\(S=\\{1,2,3,4,5,6\\}\\) tem média \\(\\mu\\) = 3.77 e variância \\(\\sigma^{2}\\) = 3.169. A distribuição amostral da média de 100 amostras de tamanho 5, 50, 500 e 5000 tomadas da população com reposição e igual probabilidade se aproxima de uma distribuição normal com média \\(\\mu\\) = 3.767 e variância \\(\\sigma^{2}\\) = 0.001, independentemente da distribuição da população: Figura 1.6: Esquerda: Histogramas de lançament de 1 dado com distribuição uniforme (N = 100). Direita: Histogramas da média de 100 amostras de tamanhos 5, 50, 500 e 5000 tomadas da população com reposição e igual probabilidade. Mais um exemplo, o lançamento de uma moeda com distribuição uniforme no espaço amostral \\(S=\\{0,1\\}\\) — codificado para \\(sucesso = 1\\) e \\(insucesso = 0\\) — tem média \\(\\mu\\) = 0.44 e variância \\(\\sigma^{2}\\) = 0.249. A distribuição amostral da média de 100 amostras de tamanho 5, 50, 500 e 5000 tomadas da população com reposição e igual probabilidade se aproxima de uma distribuição normal com média \\(\\mu\\) = 0.439 e variância \\(\\sigma^{2}\\) = 0, independentemente da distribuição da população: Figura 1.7: Esquerda: Histogramas de lançament de 1 moeda com distribuição uniforme (N = 100). Direita: Histogramas da média de 100 amostras de tamanhos 5, 50, 500 e 5000 tomadas da população com reposição e igual probabilidade. 1.9.2 Quais as condições de validade do teorema central do limite? As condições de validade do teorema central do limite são:8 As variáveis aleatórias devem ser independentes e identicamente distribuídas (independent and identically distributed ou i.i.d.); As variáveis aleatórias devem ter média \\(\\mu\\) e variância \\(\\sigma^{2}\\) finitas; O tamanho da amostra deve ser suficientemente grande (geralmente, \\(n \\geq 30\\)). 1.9.3 Qual a relação entre a lei dos grandes números e o teorema central do limite? A lei dos grandes números é um precursor do teorema central do limite, pois estabelece que a média da amostra se torna cada vez mais próxima da média populacional (isto é, mais representativa) à medida que o tamanho da amostra aumenta, e o teorema central do limite demonstra que o a distribuição da soma das variáveis aleatórias se aproxima de uma distribuição normal também à medida que o tamanho da amostra aumenta.REF? 1.9.4 Qual a relevância do teorema central do limite para a análise estatística? O teorema central do limite explica porque os testes paramétricos têm maior poder estatístico do que os testes não paramétricos, os quais não requerem suposições de distribuição de probabilidade.8 O teorema central do limite implica que os métodos estatísticos que se aplicam a distibuições normais podem ser aplicados a outras distribuições quando suas suposições são satisfeitas.8 Como o teorema central do limite determina a distribuição amostral \\(Z\\) - equação (1.2) - das médias com tamanho amostral suficientemente grande, a média pode ser padronizada para uma distribuição normal com média 0 e variância 1, \\(N(0,1)\\):8 \\[\\begin{equation} \\tag{1.2} Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\end{equation}\\] Para amostras com \\(n \\geq 30\\), a distribuição amostral Student-t se aproxima da distribuição normal padrão \\(Z\\) e, portanto, as suposições sobre a distribuição populacional não são mais necessárias de acordo com o teorema central do limite. Neste cenário, a suposição de distribuição normal pode ser usada para a distribuição de probabilidade.8 1.10 Regressão para a média 1.10.1 O que é regressão para a média? Regressão para a média9 é um fenômeno estatístico que ocorre quando uma variável aleatória \\(X\\) é medida na mesma unidade de análise em dois ou mais momentos diferentes, \\(X_{1}\\), \\(X_{2}\\), …, \\(X_{t}\\) e \\(X_{t}\\) é mais próximo da média populacional do que \\(X_{1}\\), ou seja, \\(E(X_{t})\\) é mais próxima de \\(E(X)\\) do que \\(E(X_{1})\\) é de \\(E(X)\\).10 O valor real - sem erros aleatório ou sistemático - em geral não é conhecido, mas pode ser estimado pela média de várias observações.10 Regressão para a média pode ocorrer em qualquer pesquisa cujo delineamento envolva medidas repetidas.11 Em medidas repetidas, a média de várias observações é mais próxima da média verdadeira do que qualquer observação individual, pois o erro aleatório é reduzido pela média.10 Valores extremos - em direção ao mínimo ou máximo - em uma medição inicial tendem a ser seguidos por valores mais próximos da média (valor real) na medição subsequente.10 No exemplo abaixo, a 2a medida (dado 2 = 121) é mais próxima da média (valor real = 120) do que a 1a medida (dado 1 = 118): Figura 1.8: Representação gráfica da regressão para a média em medidas repetidas. A segunda medida (dado 2) é mais próxima da média (valor real) do que a primeira medida (dado 1). 1.10.2 Qual a causa da regressão para a média? A regressão para a média pode ser atribuída ao erro aleatório, que é a variação não sistemática nos valores observados em torno de uma média verdadeira (por exemplo, erro de medição aleatório ou variações aleatórias em um participante).10 Regrssão para a média é uma consequência da observação de que dados extremos não se repetem com frequência.11 Deve-se assumir que a regressão para a média ocorreu até que os dados mostrem o contrário.10 1.10.3 Por que detectar o fenômeno de regressão para a média? A regressão para a média pode levar a conclusões errôneas sobre a eficácia de uma intervenção, pois a mudança observada pode ser devida ao erro aleatório e não ao tratamento.11 1.10.4 Com detectar o fenômeno de regressão para a média? O fenômeno de regressão para a média pode ser detectado por meio de gráfico de dispersão da diferença (estudos transversais) ou mudança (estudos longitudinais) versus os valores da 1a medida.10 O pacote regtomean12 fornece as funções cordata para calcular a correlação entre medidas tipo antes–e-depois e meechua_reg para ajustar modelos lineares de regressão. 1.10.5 Como o fenômeno de regressão para a média pode ser evitado? Aloque os participantes de modo aleatório nos grupos de tratamento e controle pode reduzir o fenômeno de regressão para a média.10 Selecione participantes com base em medidas repetidas ao invés de medidas únicas.10 Referências "],["pensamento-estatístico.html", "Capítulo 2 Pensamento estatístico 2.1 População 2.2 Amostra 2.3 Unidade de análise 2.4 Amostragem 2.5 Reamostragem 2.6 Subamostragem e superamostragem", " Capítulo 2 Pensamento estatístico 2.1 População 2.1.1 O que é população? População - ou população-alvo - refere-se ao conjunto completo sobre o qual se pretende obter informações.13 População é metodologicamente delimitada pelos critérios de inclusão e exclusão do estudo.13 Em estudos observacionais, inicialmente as características geográficas e/ou demográficas, por exemplo, definem a população a ser estudada.13 Em estudos analíticos, a população é inicialmente definida pelos objetivos da pesquisa e, posteriormente, as observações são realizadas na amostra.13 2.2 Amostra 2.2.1 O que é amostra? Amostra é uma parte finita da população do estudo.13 Em pesquisa científica, utilizam-se dados de uma amostra de participantes (ou outras unidades de análise) para realizar inferências sobre a população.14 2.2.2 Por que usar dados de amostras? Dados de uma amostra de tamanho suficiente e características representativas podem ser utilizados para inferência sobre uma população.8 Em geral, amostras de tamanhos maiores possuem médias mais próximas da média populacional e menores variâncias.8 2.2.3 O que é a correção de Bessel para variância? Correção de Bessel é um ajuste feito no denominador da fórmula de variância da amostra — ou seja, o número de graus de liberdade — para evitar que a variância amostral seja menor do que a variância populacional.15 A correção de Bessel é feita subtraindo-se 1 do número de observações da amostra, ou seja, \\(n - 1\\).15 2.2.4 Por que a correção de Bessel para variância é importante? A correção de Bessel é importante porque a variância amostral tende a ser menor do que a variância populacional, especialmente em amostras pequenas.15 A correção de Bessel ajuda a garantir que a variância amostral seja uma estimativa mais precisa da variância populacional, o que é fundamental para a validade dos testes estatísticos e das inferências feitas a partir da amostra.15 2.3 Unidade de análise 2.3.1 O que é unidade de análise? A unidade de análise (ou unidade experimental) de pesquisas na área de saúde geralmente é o indivíduo.16 A unidade de análise também pode ser a instituição em estudos multicêntricos (ex.: hospitais, clínicas) ou um estudo publicado em meta-análise (ex.: ensaios clínicos).16 2.3.2 Por que identificar a unidade de análise de um estudo? É fundamental identificar corretamente a unidade de análise para evitar inflação do tamanho da amostra (ex.: medidas bilaterais resultando em o dobro de participantes), violações de suposições dos testes de hipótese (ex.: independência entre medidas e/ou unidade de análise) e resultados espúrios em testes de hipótese (ex.: P-valores menores que aqueles observados se a amostra não estivesse inflada).16,17 2.3.3 Que medidas podem ser obtidas da unidade de análise de um estudo? Da unidade de análise podem ser coletadas informações em medidas únicas, repetidas, seriadas ou múltiplas. 2.4 Amostragem 2.4.1 O que é amostragem? .REF? 2.4.2 Quais métodos de amostragem são usados para obter uma amostra da população? O método de amostragem é geralmente definido pelas condições de viabilidade do estudo, no que diz respeito a acesso aos participantes, ao tempo de execução e aos custos envolvidos, entre outras.13 Não-probabilísticas ou intencionais:13 Bola de neve. Conveniência. Participantes encaminhados. Probabilísticas:13 Simples. Sistemática. Multiestágio. Estratificada. Agregada. 2.4.3 O que é erro de amostragem? .REF? 2.5 Reamostragem 2.5.1 O que é reamostragem? Reamostragem é um procedimento que cria vários conjuntos de dados sorteados a partir de um conjunto de dados real - a amostra da população - sem a necessidade de fazer suposições sobre os dados e suas distribuições.14 O procedimento é repetido várias vezes para usar a variabilidade dos resultados para obter um intervalo de confiança do parâmetro no nível de significância \\(\\alpha\\) pré-estabelecido.14 2.5.2 Por que utilizar reamostragem? Quando se dispõe de dados de apenas 1 amostra, as diversas suposições que são feitas podem não ser atingidas.14 Procedimentos de reamostragem produzem um conjunto de observações escolhidas aleatoriamente da amostra, igualmente representativo da população original.14 Procedimentos de reamostragem permitem estimar o erro-padrão e intervalos de confiança sem a necessidade de tais suposições, sendo, portanto, um conjunto de procedimentos não-paramétricos.14 2.5.3 Quais procedimentos de reamostragem podem ser realizados? Bootstrap: Cada iteração gera uma amostra bootstrap do mesmo tamanho do conjunto de dados original escolhendo aleatoriamente observações reais, uma de cada vez. Cada observação tem chances iguais de ser escolhida a cada vez, portanto, algumas observações serão escolhidas mais de uma vez e outras nem serão escolhidas.14 2.6 Subamostragem e superamostragem 2.6.1 O que é subamostragem? .REF? 2.6.2 O que é superamostragem? .REF? Referências "],["pensamento-metodológico.html", "Capítulo 3 Pensamento metodológico 3.1 Metodologia da pesquisa 3.2 Relação Estatística-Metodologia 3.3 Reprodutibilidade 3.4 Robustez 3.5 Replicabilidade 3.6 Generalização", " Capítulo 3 Pensamento metodológico 3.1 Metodologia da pesquisa 3.1.1 O que é metodologia da pesquisa? A utilização de um vocabulário próprio — incluindo termos frequentemente usados em metodologia, epidemiologia e estatística — facilita a discussão na comunidade científica e melhora a compreensão das publicações.18,19 3.2 Relação Estatística-Metodologia 3.2.1 Qual a relação entre estatística e metodologia da pesquisa? .20 Figura 3.1: Mapa mental da relação entre o pensamento estatístico e o pensamento metodológico. 3.3 Reprodutibilidade 3.3.1 O que é reprodutibilidade? Reprodutibilidade é a habilidade de se obter resultados iguais ou similares quando uma análise ou teste estatístico é repetido.21–23 3.3.2 Por que reprodutibilidade é importante? Analisar a reprodutibilidade pode fornecer evidências a respeito da objetividade e confiabilidade dos achados, em detrimento de terem sido obtidos devido a vieses ou ao acaso.21 A reprodutibilidade não é apenas uma questão metodológica, mas também ética, uma vez que pode envolver mal práticas científicas como fabricação e/ou falsificação de dados.21 Reprodutibilidade pode ser considerada um padrão mínimo em pesquisa científica.22 3.4 Robustez 3.4.1 O que é robustez? .REF? 3.5 Replicabilidade 3.5.1 O que é replicabilidade? Replicabilidade é a habilidade de se obter conclusões iguais ou similares quando um experimento é repetido.22,23 3.6 Generalização 3.6.1 O que é generalização? Generalização refere-se à extrapolação das conclusões do estudo, observados na amostra, para a população.13 Referências "],["paradoxos-e-falácias.html", "Capítulo 4 Paradoxos e falácias 4.1 Paradoxos estatísticos 4.2 Falácias estatísticas", " Capítulo 4 Paradoxos e falácias 4.1 Paradoxos estatísticos 4.1.1 O que são paradoxos estatísticos? Paradoxos podem originar da incompreensão ou mal informação da nossa intuição a respeito do fenômeno.24 4.1.2 O que é o paradoxo de Abelson? .25 4.1.3 O que é o paradoxo de Berkson? .26 4.1.4 O que é o paradoxo de Big Data? “Quanto maior a quantidade de dados, maior a certeza de que vamos nos enganar”.24 4.1.5 O que é o paradoxo de Ellsberg? .27 4.1.6 O que é o paradoxo de Freedman? .28,29 4.1.7 O que é o paradoxo de Hand? .30 4.1.8 O que é o paradoxo de Lindley? .31 4.1.9 O que é o paradoxo de Lord? .32,33 4.1.10 O que é o paradoxo de Proebsting? .REF? 4.1.11 O que é o paradoxo de Simpson? O paradoxo de Simpson ocorre quando a associação entre duas variáveis \\(X\\) e \\(Y\\) desaparece ou mesmo reverte sua direção quando condicionadas em uma terceira variável \\(Z\\).34,35 Para decisão do paradoxo de Simpson pode-se utilizar o conceito de ‘back-door’, o qual considera os ‘caminhos’ (isto é, associações) no gráfico acíclio direcionado e assegura que todos as associações espúrias do tratamento \\(X\\) para o desfecho \\(Y\\) nesse diagrama causal sejam interceptados pela variável \\(Z\\).36 Dependendo do contexto em que os dados foram obtidos — delineamento do estudo, escolha dos instrumentos e dos tipos de variáveis — a melhor escolha para a análise pode variar entre a análise da população agregada ou da subpopulação desagregada.36 É possivel que em alguns contextos nem a análise agregada ou a desagregada podem oferecer a resposta correta, sendo necessário o uso de outras (mais) covariáveis.36 Figura 4.1: Paradoxo de Simpson representado com dados simulados. Os pontos no gráfico representam observações individuais e as linhas de tendência representam as regressões lineares ajustadas para os dados desagregados da população e agregados por subpopulação. 4.1.12 O que é o paradoxo de Stein? .37 4.1.13 O que é o paradoxo de Okie? .REF? 4.1.14 O que é o paradoxo da acurácia? .REF? 4.1.15 O que é o paradoxo do falso positivo? .REF? 4.1.16 O que é o paradoxo da caixa de Bertrand? .REF? 4.1.17 O que é o paradoxo do elevador? .38 4.1.18 O que é o paradoxo da amizade? .39 4.1.19 O que é o paradoxo do menino ou menina? .38 4.1.20 O que é o paradoxo do teste surpresa? .REF? 4.1.21 O que é o paradoxo do nó da gravata? .REF? 4.1.22 O que é o paradoxo da Bela Adormecida? .REF? 4.2 Falácias estatísticas 4.2.1 O que são falácias estatísticas? Falácias estatísticas são erros de raciocínio que ocorrem em situações que envolvem dados e estatísticas. Elas podem ocorrer em qualquer etapa do processo de análise de dados, desde a coleta até a interpretação dos resultados.Elas podem ser intencionais ou não intencionais, e podem ser usadas para manipular, enganar ou confundir as pessoas.REF? As falácias estatísticas podem ser difíceis de detectar, pois muitas vezes são sutis e podem parecer plausíveis à primeira vista. No entanto, é importante estar ciente delas e saber como identificá-las para evitar erros de interpretação e tomada de decisão.REF? 4.2.2 O que é a falácia do jogador? A falácia do jogador é a crença de que eventos independentes têm uma influência sobre eventos futuros. Por exemplo, se uma moeda é lançada várias vezes e cai cara em todas as vezes, a falácia do jogador sugere que a próxima jogada será coroa, pois a moeda “deve” se equilibrar. No entanto, cada lançamento da moeda é independente e não afeta o resultado do próximo lançamento.40 4.2.3 O que é a falácia da mão quente? A falácia da mão quente é a crença de que um jogador que teve sucesso em um jogo de azar terá mais chances de sucesso no futuro. Por exemplo, se uma moeda é lançada várias vezes e cai cara em todas as vezes, a falácia da mão quente sugere que a próxima jogada será cara, pois o jogador está “quente”. No entanto, cada lançamento da moeda é independente e não afeta o resultado do próximo lançamento.40 Referências "],["vieses-metodológicos.html", "Capítulo 5 Vieses metodológicos 5.1 Vieses metodológicos", " Capítulo 5 Vieses metodológicos 5.1 Vieses metodológicos 5.1.1 O que são vieses metodológicos? .REF? "],["parte-2---estatística-básica.html", "Parte 2 - Estatística Básica", " Parte 2 - Estatística Básica "],["medidas-e-instrumentos.html", "Capítulo 6 Medidas e instrumentos 6.1 Escalas 6.2 Medição e Medidas 6.3 Erros de medida 6.4 Instrumentos 6.5 Acurácia e precisão 6.6 Viés e variabilidade", " Capítulo 6 Medidas e instrumentos 6.1 Escalas 6.1.1 O que são escalas? Uma escala de medição grosseira representa um construto de natureza contínua medido por itens tais que diferentes pontuações são agrupadas na mesma categoria no ato da coleta de dados.41 Em escalas grosseiras, erros são introduzidos porque as variações contínunas do constructo são colapsadas em uma mesma categorias ou separadas entre categorias próximas.41 Escalas tipo Likert com 5 categorias tipo “discordo totalmente”, “discordo parcialmente”, “nem concordo nem discordo”, “concordo parcialmente”, e “concordo totalmente” são escalas grosseira porque as diferenças entre as categorias não são iguais. Por exemplo, a diferença entre “discordo totalmente” e “discordo parcialmente” não é a mesma que a diferença entre “concordo parcialmente” e “concordo totalmente”.41 O pacote likert42 fornece a função likert para analisar respostas de instrumentos em escala Likert. O erros em escalas grosseiras é considerado sistemático mas não pode ser corrigido em nível da unidade de análise.41 6.2 Medição e Medidas 6.2.1 O que é medição? Processo empírico, realizado por meio de um instrumento, que estabelece uma correspondência rigorosa e objetiva entre uma observação e uma categoria em um modelo da observação.43 Esse processo tem como objetivo distinguir de maneira substantiva a manifestação observada de outras possíveis manifestações que também possam ser diferenciadas.43 6.2.2 O que são medidas diretas? .REF? 6.2.3 O que são medidas derivadas? .REF? 6.2.4 O que são medidas por teoria? .REF? 6.2.5 O que são medidas únicas? A medida única da pressão arterial sistólica no braço esquerdo resulta em um valor pontual.REF? Medidas únicas obtidas de diferentes unidades de análise podem ser consideradas independentes se observadas outras condições na coleta de dados.REF? O valor pontual será considerado representativo da variável para a unidade de análise (ex.: 120 mmHg para o participante #9). Tabela 6.1: Tabela de dados brutos com medidas únicas. Unidade de análise Pressão arterial, braço esquerdo (mmHg) 1 118 2 113 3 116 4 110 5 111 6 116 7 120 8 111 9 120 10 112 6.2.6 O que são medidas repetidas? As medidas repetidas podem ser tabuladas separadamente, por exemplo para análise da confiabilidade de obtenção dessa medida.REF? A medida repetida da pressão arterial no braço esquerdo resulta em um conjunto de valores pontuais (ex.: 110 mmHg, 118 mmHg e 116 mmHg para o participante #5). Tabela 6.2: Tabela de dados brutos com medidas repetidas. Unidade de análise Pressão arterial, braço esquerdo (mmHg) #1 Pressão arterial, braço esquerdo (mmHg) #2 Pressão arterial, braço esquerdo (mmHg) #3 1 114 112 112 2 115 120 113 3 115 110 120 4 117 116 114 5 110 118 116 6 110 120 113 7 118 114 117 8 111 112 119 9 120 112 117 10 110 115 115 As medidas repetidas podem ser agregadas por algum parâmetro — ex.: média, mediana, máximo, mínimo, entre outros —, observando-se a relevância biológica, clínica e/ou metodológica desta escolha.REF? Medidas agregadas obtidas de diferentes unidades de análise podem ser consideradas independentes se observadas outras condições na coleta de dados.REF? O valor agregado será considerado representativo da variável para a unidade de análise (ex.: média = 115 mmHg para o participante #5). Tabela 6.3: Tabela de dados brutos com medidas repetidas agregadas. Unidade de análise Pressão arterial, braço esquerdo (mmHg) média 1 113 2 116 3 115 4 116 5 115 6 114 7 116 8 114 9 116 10 113 O pacote stats44 fornece a função aggregate para agregar medidas repetidas utilizando uma função personalizada. 6.2.7 O que são medidas seriadas? Medidas seriadas são possivelmente relacionadas e, portanto, dependentes na mesma unidade de análise.REF? Por exemplo, a medida seriada da pressão arterial no braço esquerdo, em intervalos tipicamente regulares (ex.: 114 mmHg, 120 mmHg e 110 mmHg em 1 min, 2 min e 3 min, respectivamente, para o participante #1). Tabela 6.4: Tabela de dados brutos com medidas seriadas não agregadas. Unidade de análise Tempo (min) Pressão arterial, braço esquerdo (mmHg) 1 1 114 1 2 120 1 3 110 2 1 119 2 2 120 2 3 114 3 1 116 3 2 114 3 3 116 4 1 113 Medidas seriadas também agregadas por parâmetros — ex.: máximo, mínimo, amplitude — são consideradas representativas da variação temporal ou de uma característica de interesse (ex.: amplitude = 10 mmHg para o participante #1). Tabela 6.5: Tabela de dados brutos com medidas seriadas não agregadas. Unidade de análise Pressão arterial, braço esquerdo (mmHg) amplitude 1 10 2 6 3 2 4 6 5 1 6 8 7 9 8 10 9 7 10 5 O pacote stats44 fornece a função aggregate para agregar medidas repetidas utilizando uma função personalizada. 6.2.8 O que são medidas múltiplas? Medidas múltiplas também são possivelmente relacionadas e, portanto, são dependentes na mesma unidade de análise. Medidas múltiplas podem ser obtidas de modo repetido para análise agregada ou seriada.REF? A medida de pressão arterial bilateral resulta em um conjunto de valores pontuais (ex.: braço esquerdo = 114 mmHg, braço direito = 118 mmHg para o participante #8). Neste caso, ambos os valores pontuais são considerados representativos daquela unidade de análise. Tabela 6.6: Tabela de dados brutos com medidas múltiplas. Unidade de análise Pressão arterial, braço esquerdo (mmHg) Pressão arterial, braço direito (mmHg) 1 117 115 2 120 118 3 112 118 4 112 112 5 116 112 6 112 118 7 115 113 8 114 118 9 119 114 10 112 116 O pacote stats44 fornece a função aggregate para agregar medidas repetidas utilizando uma função personalizada. 6.3 Erros de medida 6.3.1 O que são erros de medida? .REF? A natureza dos erros de medida são em geral atribuídos aos (1) instrumentos utilizados e variações no protocolo, na medida em que o seu tamanho médio pode ser reduzido por modificações e melhorias nesses instrumentos; e (2) variações genuínas medida em de curto prazo.45 6.3.2 Quais fontes de variabilidade são comumente investigadas? Intra/Entre participantes (isto é, unidades de análise).46 Intra/Entre repetições.46 Intra/Entre observadores.46 6.4 Instrumentos 6.4.1 O que são instrumentos? .REF? 6.5 Acurácia e precisão 6.5.1 O que é acurácia? .REF? 6.5.2 O que é precisão? .REF? Figura 6.1: Acurácia e precisão como propriedades de uma medida. 6.6 Viés e variabilidade 6.6.1 Qual é a relação entre viés e variabilidade? .REF? Figura 6.2: Viés e variabilidade de uma medida. Referências "],["dados-big-data-e-metadados.html", "Capítulo 7 Dados, big data e metadados 7.1 Dados 7.2 Big data 7.3 Metadados", " Capítulo 7 Dados, big data e metadados 7.1 Dados 7.1.1 O que são dados? “Tudo são dados”.47 Dados coletados em um estudo geralmente contêm erros de mensuração e/ou classificação, dados perdidos e são agrupados por alguma unidade de análise.48 7.1.2 Quais são as fontes de dados? Experimentos.REF? Mundo real.REF? Simulação.REF? 7.1.3 O que são dados primários e secundários? Dados primários são dados originais coletados intencionalmente para uma determinada análise exploratória ou inferencial planejada a priori.49 Dados secundários compreendem dados coletados inicialmente para análises de um estudo, e são subsequentemente utilizados para outras análises.49 7.1.4 O que são dados quantitativos e qualitativos? .REF? 7.2 Big data 7.2.1 O que são big data? Big data refere-se a bancos de dados muito grandes com um mecanismo “R” — aleatório (Random), auto-reportado (self-Reported), reportado administrativamente (administratively reported), seletivamente respondido (selectively repondend) — descontrolado ou desconhecido.24 7.3 Metadados 7.3.1 O que são metadados? Metadados são informações técnicas relacionadas às variáveis do estudo, tais como rótulos, limites de valores plausíveis, códigos para dados perdidos e unidades de medida.50 Metadados também são informações relacionadas ao delineamento e/ou protocolo do estudo, recrutamento dos participantes, e métodos para realização das medidas.50 7.3.2 Quais são as recomendações para os metadados de um banco de dados? Utilize rótulos padronizados para variáveis e fatores para facilitar o reuso (reprodutibilidade) do conjuntos de dados e scripts de análise.51 Crie rótulos de variáveis concisos, claros e mutuamente exclusivos.51 Evite muitas letras maiúsculas ou outros caracteres especiais que usam a shift.51 Na existência de versões de instrumentos publicadas em diferentes anos, use o ano de publicação das escalas no rótulo.51 Divida o rótulo da variável ou fator em partes e ordene-as do mais geral para o mais particular geral (ex.: experimento -&gt; repetição -&gt; escala -&gt; item).51 O pacote base52 fornece a função names para declarar o nome de uma variável. O pacote base52 fornece a função labels para declarar o rótulo de uma variável. O pacote units53 fornece a função units para declarar as unidades de medida de uma variável. O pacote units53 fornece a função valid_udunits para listar as opções de unidades de medida de uma variável. O pacote janitor54 fornece a função clean_names para formatar de modo padronizado o nome das variáveis utilizando apenas caracteres, números e o símbolo ‘_’. O pacote Hmisc55 fornece a função contents para criar um objeto com os metadados (nomes, rótulos, unidades, quantidade e níveis das variáveis categóricas, e quantidade de dados perdidos) de um dataframe. Referências "],["dados-perdidos-e-imputados.html", "Capítulo 8 Dados perdidos e imputados 8.1 Dados perdidos 8.2 Dados imputados", " Capítulo 8 Dados perdidos e imputados 8.1 Dados perdidos 8.1.1 O que são dados perdidos? Dados perdidos são dados não coletados de um ou mais participantes, para uma ou mais variáveis.56 O pacote base52 fornece a função is.na para identificar que elementos de um objeto são dados perdidos. 8.1.2 Qual o problema de um estudo ter dados perdidos? Uma grande quantidade de dados perdidos pode comprometer a integridade científica do estudo, considerando-se que o tamanho da amostra foi estimado para observar um determinado tamanho de efeito mínimo.56 Perda de participantes no estudo por dados perdidos pode reduzir o poder estatístico (erro tipo II).56 Não existe solução globalmente satisfatória para o problema de dados perdidos.56 8.1.3 Quais os mecanismos geradores de dados perdidos? Dados perdidos completamente ao acaso (missing completely at random, MCAR), em que os dados perdidos estão distribuídos aleatoriamente nos dados da amostra.57,58 Dados perdidos ao acaso (missing at random, MAR), em que a probabilidade de ocorrência de dados perdidos é relacionada a outras variáveis medidas.57,58 Dados perdidos não ao acaso (missing not at random, MNAR), em que a probabilidade da ocorrência de dados perdidos é relacionada com a própria variável.57,58 8.1.4 Como identificar o mecanismo gerador de dados perdidos em um banco de dados? Por definição, não é possível avaliar se os dados foram perdidos ao acaso (MAR) ou não (MNAR).57 Testes t e regressões logísticas podem ser aplicados para identificar relações entre variáveis com e sem dados perdidos, criando um fator de análise (‘dado perdido’ = 1, ‘dado observado’ = 0).57 O pacote misty59 fornece a função na.test para executar o Little’s Missing Completely at Random (MCAR) test60. O pacote naniar61 fornece a função mcar_test para executar o Little’s Missing Completely at Random (MCAR) test60. 8.1.5 Que estratégias podem ser utilizadas na coleta de dados quando há expectativa de perda amostral? Na expectativa de ocorrência de perda amostral, com consequente ocorrência de dados perdidos, recomenda-se ampliar o tamanho da amostra com um percentual correspondente a tal estimativa (ex.: 10%), embora ainda não corrija potenciais vieses pela perda.56 8.1.6 Que estratégias podem ser utilizadas na análise quando há dados perdidos? Na ocorrência de dados perdidos, a análise mais comum compreende apenas os ‘casos completos’, com exclusão de participantes com algum dado perdido nas variáveis do estudo. Em casos de grande quantidade de dados perdidos, pode-se perder muito poder estatístico (erro tipo II elevado).56 A análise de dados completos é válida quando pode-se argumentar que que a probabilidade de o participante ter dados completos depende apenas das covariáveis e não dos desfechos.58 A análise de dados completos é eficiente quando todos os dados perdidos estão no desfecho, ou quando cada participante com dados perdidos nas covariáveis também possui dados perdidos nos desfechos.58 O pacote base52 fornece a função na.omit para remover dados perdidos de um objeto em um banco de dados. O pacote stats44 fornece a função complete.cases para identificar os casos completos - isto é, sem dados perdidos - em um banco de dados. 8.1.7 Que estratégias podem ser utilizadas na redação de estudos em que há dados perdidos? Informar: o número de participantes com dados perdidos; diferenças nas taxas de dados perdidos entre os braços do estudo; os motivos dos dados perdidos; o fluxo de participantes; quaisquer diferenças entre os participantes com e sem dados perdidos; o padrão de ausência (por exemplo, se é aleatória); os métodos para tratamento de dados perdidos das variáveis em análise; os resultados de quaisquer análises de sensibilidade; as implicações dos dados perdidos na interpretação do resultados.62 8.2 Dados imputados 8.2.1 O que são dados imputados? .REF? 8.2.2 Quando a imputação de dados é indicada? A análise com imputação de dados pode ser útil quando pode-se argumentar que os dados foram perdidos ao acaso (MAR); quando o desfecho foi observado e os dados perdidos estão nas covariáveis; e variáveis auxiliares — preditoras do desfecho e não dos dados perdidos — estão disponíveis.58 Na ocorrência de dados perdidos, a imputação de dados (substituição por dados simulados plausíveis preditos pelos dados presentes) pode ser uma alternativa para manter o erro tipo II estipulado no plano de análise.56 8.2.3 Quais os métodos de imputação de dados? Modelos lineares e logísticos podem ser utilizados para imputar dados perdidos em variáveis contínuas e dicotômicas, respectivamente.63 Os métodos de imputação de dados mais robustos incluem a imputação multivariada por equações encadeadas (multivariate imputation by chained equations, MICE)64 e a correspondência média preditiva (predictive mean matching, PMM)65,66. Os pacotes mice64 e miceadds67 fornecem funções mice e mi.anova para imputação multivariada por equações encadeadas, respectivamente, para imputação de dados. Referências "],["dados-anonimizados-e-sintéticos.html", "Capítulo 9 Dados anonimizados e sintéticos 9.1 Dados anonimizados 9.2 Dados sintéticos", " Capítulo 9 Dados anonimizados e sintéticos 9.1 Dados anonimizados 9.1.1 O que são dados anonimizados? .REF? 9.1.2 Com anonimizar os dados de um banco? .REF? O pacote ids68 fornece a função random_id para criar identificadores aleatórios por criptografia. O pacote hash69 fornece a função hash para criar identificadores por objetos hash. O pacote anonimizer70 fornece a função anonymize para criar uma versão anônima de variáveis em um banco de dados. O pacote digest71 fornece a função digest para criar identificadores por objetos hash criptografados ou não. 9.2 Dados sintéticos 9.2.1 O que são dados sintéticos? .REF? O pacote synthpop72 fornece a função syn para criar bancos de dados sintéticos a partir de um banco de dados real. Referências "],["tabulação-de-dados.html", "Capítulo 10 Tabulação de dados 10.1 Planilhas eletrônicas", " Capítulo 10 Tabulação de dados 10.1 Planilhas eletrônicas 10.1.1 Qual a organização de uma tabela de dados? As informações podem ser organizadas em formato de dados retangulares (ex.: matrizes, tabelas, quadro de dados) ou não retangulares (ex.: listas).REF? Cada variável possui sua própria coluna (vertical).73 Cada observação possui sua própria linha (horizontal).73 Cada valor possui sua própria célula especificada em um par (linha, coluna).73 Cada célula possui seu próprio dado.73 O pacote DataEditR74 fornece a função data_edit para interativamente criar, editar e salvar a tabela de dados. 10.1.2 Qual a estrutura básica de uma tabela para análise estatística? Use apenas 1 (uma) planilha eletrônica para conter todas as informações coletadas. Evite múltiplas abas no mesmo arquivo, assim como múltiplos arquivos quando possível.75 Use apenas 1 (uma) linha de cabeçalho para nomear os fatores e variáveis do seu estudo.75 Tipicamente, cada linha representa um participante e cada coluna representa uma variável ou fator do estudo. Estudos com medidas repetidas dos participantes podem conter múltiplas linhas para o mesmo participante (repetindo os dados na mesma coluna, conhecido como formato curto) ou só uma linha para o participante (repetindo os dados em colunas separadas, conhecido como formato longo ).76 Tabela 10.1: Estrutura básica de uma tabela de dados. V1 V2 V3 V4 \\(x_{1,1}\\) \\(x_{1,2}\\) \\(x_{1,3}\\) \\(x_{1,4}\\) \\(x_{2,1}\\) \\(x_{2,2}\\) \\(x_{2,3}\\) \\(x_{2,4}\\) \\(x_{3,1}\\) \\(x_{3,2}\\) \\(x_{3,3}\\) \\(x_{3,4}\\) \\(x_{4,1}\\) \\(x_{4,2}\\) \\(x_{4,3}\\) \\(x_{4,4}\\) \\(x_{5,1}\\) \\(x_{5,2}\\) \\(x_{5,3}\\) \\(x_{5,4}\\) 10.1.3 O que usar para organizar tabelas para análise computadorizada? Seja consistente em: códigos para as variáveis categóricas; códigos para dados perdidos; nomes das variáveis; identificadores de participantes; nome dos arquivos; formato de datas; uso de caracteres de espaço.75,76 Crie um dicionário de dados (metadados) em um arquivo separado contendo: nome da variável, descrição da variável, unidades de medida e valores extremos possíveis.75 Use recursos para validação de dados antes e durante a digitação de dados.75,76 O pacote data.table77 fornece a função melt.data.table para reorganizar a tabela em diferentes formatos. 10.1.4 O que não usar para organizar tabelas para análise computadorizada? Não deixe células em branco: substitua dados perdidos por um código sistemático (ex.: NA [not available]).75 Não inclua análises estatísticas ou gráficos nas tabelas de dados brutos.75 Não utilize cores como informação. Se necessário, crie colunas adicionais - variáveis instrumentais ou auxiliares - para identificar a informação de modo que possa ser analisada.75 Não use células mescladas. Delete linhas e/ou colunas totalmente em branco (sem unidades de análise e/ou sem variáveis). 10.1.5 O que é recomendado e o que deve ser evitado na organização das tabelas para análise? Tabela 10.2: Formatação recomendada para tabela de dados. ID Data.Coleta Estado.Civil Numero.Filhos 1 12-06-2025 casado NA 2 13-06-2025 casado 1 3 14-06-2025 casado NA 4 15-06-2025 solteiro NA 5 16-06-2025 casado NA 6 17-06-2025 solteiro 0 7 18-06-2025 solteiro NA 8 19-06-2025 solteiro NA 9 20-06-2025 casado NA 10 21-06-2025 solteiro NA Tabela 10.3: Formatação não recomendada para tabela de dados. ID Data de Coleta Estado Civil Número de Filhos 1 12-06-2025 casado NA 2 13-06-2025 Casado 1 3 14-06-2025 casado NaN 4 15-06-2025 Solteiro N/A 5 16-06-2025 Casado N.A. 6 17-06-2025 solteiro 0 7 18-06-2025 solteiro 8 19-06-2025 Solteiro na 9 20-06-2025 casado n.a. 10 21-06-2025 Solteiro 999 Referências "],["variáveis-e-fatores.html", "Capítulo 11 Variáveis e fatores 11.1 Variáveis 11.2 Transformação de variáveis 11.3 Categorização de variáveis contínuas 11.4 Dicotomização de variáveis contínuas 11.5 Fatores", " Capítulo 11 Variáveis e fatores 11.1 Variáveis 11.1.1 O que são variáveis? Variáveis são informações que podem variar entre medidas em diferentes indivíduos e/ou repetições.78 Variáveis definem características de uma amostra extraída da população, tipicamente observados por aplicação de métodos de amostragem (isto é, seleção) da população de interesse.49 11.1.2 Como são classificadas as variáveis? Quanto à informação:49,79–81 Quantitativa Qualitativa Quanto ao conteúdo:49,79–82 Contínua: representam ordem e magnitude entre valores. Contínua (números inteiros) vs. Discreta (números racionais). Intervalo (valor ‘0’ é arbitrário) vs. Razão (valor ‘0’ verdadeiro). Categórica ordinal (numérica ou nominal): representam ordem, mas não magnitude entre valores. Categórica nominal (multinominal ou dicotômica): não representam ordem ou magnitude, apenas categorias. Quanto à interpretação:49,79–81 Dependente (desfecho) Independente (preditora, covariável, confundidora, controle) Mediadora Moderadora Modificadora Auxiliar Indicadora O pacote base52 fornece a função class para identificar qual é o tipo do objeto. O pacote base52 fornece as funções as.numeric e as.character para criar objetos numéricos e categóricos, respectivamente. O pacote base52 fornece as funções as.Date e as.logical para criar objetos em formato de data e lógicos (VERDADEIRO, FALSO), respectivamente. 11.1.3 Por que é importante classificar as variáveis? Identificar corretamente os tipos de variáveis da pesquisa é uma das etapas da escolha dos métodos estatísticos adequados para as análises e representações no texto, tabelas e gráficos.80 11.2 Transformação de variáveis 11.2.1 O que é transformação de variáveis? Transformação significa aplicar uma função matemática à variável medida em sua unidade original.83 A transformação visa atender aos pressupostos dos modelos estatísticos quanto à distribuição da variável, em geral a distribuição gaussiana.49,83 A dicotomização pode ser interpretada como um caso particular de agrupamento.84 11.2.2 Por que transformar variáveis? Muitos procedimentos estatísticos supõem que as variáveis - ou seus termos de erro, mais especificamente - são normalmente distribuídas. A violação dessa suposição pode aumentar suas chances de cometer um erro do tipo I ou II.85 Mesmo quando se está usando análises consideradas robustas para violações dessas suposições ou testes não paramétricos (que não assumem explicitamente termos de erro normalmente distribuídos), atender a essas questões pode melhorar os resultados das análises (por exemplo, Zimmerman, 1995).85 11.2.3 Quais transformações podem ser aplicadas? Distribuições com assimetria à direita:85 Raiz quadrada Logaritmo natural Logaritmo base 10 Transformação inversa Distribuições com assimetria à esquerda:85 Reflexão e raiz quadrada Reflexão e logaritmo natural Reflexão e logaritmo base 10 Reflexão e transformação inversa Transformação arco-seno.85 Transformação de Box-Cox.86 Transformação de escore padrão (Z-score ou padronização). Escala Mínimo-Máximo (0,1). Normalização (normas L1, L2). Diferenciação. Categorização. Dicotomização. O pacote MASS87 fornece a função boxcox para executar a transformação de Box-Cox.86 11.3 Categorização de variáveis contínuas 11.3.1 O que é categorização de uma variável? .REF? 11.3.2 Por que não é recomendado categorizar variáveis contínuas? Nenhum dos argumentos usados para defender a categorização de variáveis se sustenta sob uma análise técnica rigorosa.88 Categorizar variáveis não é necessário para conduzir análises estatísticas. Ao invés de categorizar, priorize as variáveis contínuas.89–91 Em geral, não existe uma justificativa racional (plausibilidade biológica) para assumir que as categorias artificiais subjacentes existam.89–91 Caso exista um ponto de corte ou limiar verdadeiro que discrimine três ou mais grupos independentes, identificar tal ponto de corte ainda é um desafio.92 Categorização de variáveis contínuas aumenta a quantidade de testes de hipótese para comparações pareadas entre os quantis, inflando, portanto, o erro tipo I.93 Categorização de variáveis contínuas requer uma função teórica que pressupõe a homogeneidade da variável dentro dos grupos, levando tanto a uma perda de poder como a uma estimativa imprecisa.93 Categorização de variáveis contínuas pode dificultar a comparação de resultados entre estudos devido aos pontos de corte baseados em dados de um banco usados para definir as categorias.93 O pacote questionr94 fornece a função irec para executar uma interface interativa para codificação de variáveis categóricas. 11.3.3 Quais são as alternativas à categorização de variáveis contínuas? Análise com os dados das variáveis na escala de medida original.88 Análise com modelos de regressão com pesos locais (lowess) tais como splines e polinômios fracionais.88 11.4 Dicotomização de variáveis contínuas 11.4.1 O que são variáveis dicotômicas? Variáveis dicotômicas (ou binárias) podem representar categorias naturais tipo “presente/ausente”, “sim/não”.REF? Variáveis dicotômicas podem representar categorias fictícias, criadas a partir de variáveis multinominais, em que cada nível é convertido em uma variável dicotômica dummy.REF? Dicotomização é considerado um artefato da análise de dados, uma vez que é realizada após a coleta de dados.41 Geralmente são representadas por “1” e “0”.REF? 11.4.2 Quais argumentos são usados para defender a categorização ou dicotomização de variáveis contínuas? O argumento principal para dicotomização de variáveis é que tal procedimento facilita e simplifica a apresentação dos resultados, principalmente para o público em geral.84 Os pesquisadores não conhecem as consequências estatísticas da dicotomização.88 Os pesquisadores não conhecem os métodos adequados de análise não-paramétrica, não-linear e robusta.88 As categorias representam características existentes dos participantes da pesquisa, de modo que as análises devam ser feitas por grupos e não por indivíduos.88 A confiabilidade da(s) variável(eis) medida(s) é baixa e, portanto, categorizar os participantes resultaria em uma medida mais confiável.88 11.4.3 Por que não é recomendado dicotomizar variáveis contínuas? Nenhum dos argumentos usados para defender a dicotomização de variáveis se sustenta sob uma análise técnica rigorosa.88 Dicotomizar variáveis não é necessário para conduzir análises estatísticas. Ao invés de dicotomizar, priorize as variáveis contínuas.89–91 Em geral, não existe uma justificativa racional (plausibilidade biológica) para assumir que as categorias artificiais subjacentes existam.89–91 Dicotomização causa perda de informação e consequentemente perda de poder estatístico para detectar efeitos.88,89 Dicotomização também classifica indivíduos com valores próximos na variável contínua como indivíduos em pontos opostos e extremos, artificialmente sugerindo que são muito diferentes.89 Dicotomização pode diminuir a variabilidade das variáveis.89 Dicotomização pode ocultar não-linearidades presentes na variável contínua.88,89 A média ou a mediana, embora amplamente utilizadas, não são bons parâmetros para dicotomizar variáveis.84,89 Caso exista um ponto de corte ou limiar verdadeiro que discrimine dois grupos independentes, identificar tal ponto de corte ainda é um desafio.92 11.4.4 Quais cenários legitimam a dicotomização das variáveis contínuas? Quando existem dados e/ou análises que suportem a existência - não apenas a suposição ou teorização - de categorias com um ponto de corte claro e com significado entre elas.88 Quando a distribuição da variável contínua é muito assimétrica, de modo que uma grande quantidade de observações está em um dos extremos da escala.88 11.4.5 Quais métodos são usados para dicotomizar variáveis contínuas? Em termos de tabelas de contingência 2x2, os seguintes métodos permitem92 a identificação do limiar verdadeiro: Youden.95 Gini Index.96 Estatística qui-quadrado (\\(\\chi^2\\)).97 Risco relativo (\\(RR\\)).98 Kappa (\\(\\kappa\\)).99. 11.5 Fatores 11.5.1 O que são fatores? Fator é um sinônimo de variável categórica.REF? Na modelagem, fator é sinônimo de variável preditora, em particular quando se refere à modelagem de efeitos fixos e aleatórios – os fatores (variáveis) são fatores fixos ou fatores aleatórios.REF? Fatores são variáveis controladas pelos pesquisadores em um experimento para determinar seu efeito na(s) variável(ies) de resposta. Um fator pode assumir apenas um pequeno número de valores, conhecidos como níveis. Os fatores podem ser uma variável categórica ou baseados em uma variável contínua, mas usam apenas um número limitado de valores escolhidos pelos experimentadores.REF? O pacote base52 fornece a função as.factor para converter uma variável em fator. 11.5.2 O que são níveis de um fator? Níveis de um fator são as possíveis categorias que descrevem um fator.REF? O pacote base52 fornece as funções levels e nlevels para listar os níveis e a quantidade deles em um fator. Referências "],["distribuições-e-parâmetros.html", "Capítulo 12 Distribuições e parâmetros 12.1 Distribuições de probabilidade 12.2 Parâmetros 12.3 Tendência central 12.4 Dispersão 12.5 Proporção 12.6 Distribuição 12.7 Extremos 12.8 Valores discrepantes", " Capítulo 12 Distribuições e parâmetros 12.1 Distribuições de probabilidade 12.1.1 O que são distribuições de probabilidade? Uma distribuição de probabilidade é uma função que descreve os valores possíveis ou o intervalo de valores de uma variável (eixo horizontal) e a frequência com que cada valor é observado (eixo vertical).49 12.1.2 Como representar distribuições de probabilidade? Tabelas de frequência, polígonos de frequência, gráficos de barras, histogramas e boxplots são formas de representar distribuições de probabilidade.100 Tabelas de frequência mostram as categorias de medição e o número de observações em cada uma. É necessário conhecer o intervalo de valores (mínimo e máximo), que é dividido em intervalos arbitrários chamados “intervalos de classe”.100 Se houver muitos intervalos, não haverá redução significativa na quantidade de dados, e pequenas variações serão perceptíveis. Se houver poucos intervalos, a forma da distribuição não poderá ser adequadamente determinada.100 A quantidade de intervalos pode ser determinada pelo método de Sturges, que é dado pela fórmula \\(k = 1 + 3.322 \\times \\log_{10}(n)\\), onde \\(k\\) é o número de intervalos e \\(n\\) é o número de observações.101 A quantidade de intervalos pode ser determinada pelo método de Scott, que é dado pela fórmula \\(h = 3.5 \\times \\text{sd}(x) \\times n^{-1/3}\\), onde \\(h\\) é a largura do intervalo, \\(\\text{sd}(x)\\) é o desvio padrão e \\(n\\) é o número de observações.102 A quantidade de intervalos pode ser determinada pelo método de Freedman-Diaconis, que é dado pela fórmula \\(h = 2 \\times \\text{IQR}(x) \\times n^{-1/3}\\), onde \\(h\\) é a largura do intervalo, \\(\\text{IQR}(x)\\) é o intervalo interquartil e \\(n\\) é o número de observações.103 A largura das classes pode ser determinada dividindo o intervalo total de observações pelo número de classes. Recomenda-se larguras iguais, mas larguras desiguais podem ser usadas quando existirem grandes lacunas nos dados ou em contextos específicos. Os intervalos devem ser mutuamente exclusivos e não sobrepostos, evitando intervalos abertos (ex.: &lt;5, &gt;10).100 Polígonos de frequência são gráficos de linhas que conectam os pontos médios de cada barra do histograma. Eles são úteis para comparar duas ou mais distribuições de frequência.100 Gráficos de barra verticais ou horizontais representam a distribuição de frequências de uma variável categórica. A altura de cada barra é proporcional à frequência da classe. A largura da barra é igual à largura da classe. A área de cada barra é proporcional à frequência da classe. A área total do gráfico de barras é igual ao número total de observações.100 Histogramas representam a distribuição de frequências de uma variável contínua. A altura de cada barra é proporcional à frequência da classe. A largura da barra é igual à largura da classe. A área de cada barra é proporcional à frequência da classe. A área total do histograma é igual ao número total de observações.100 Boxplots representam a distribuição de frequências de uma variável contínua. A linha central divide os dados em duas partes iguais (mediana ou Q2). A caixa inferior representa o primeiro quartil (Q1) e a caixa superior representa o terceiro quartil (Q3). A linha inferior é o mínimo e a linha superior é o máximo. Os valores atípicos são representados por pontos individuais.100 O pacote grDevices104 fornece a função nclass para determinar a quantidade de classes de um histograma com os métodos de Sturge101, Scott102 ou Freedman-Diaconis103. O pacote graphics105 fornece a função hist para criar histogramas. 12.1.3 Quais características definem uma distribuição? Uma distribuição pode ser definida por modelos matemáticos e caracterizada por parâmetros de tendência central, dispersão, simetria e curtose. 12.1.4 Quais são as distribuições mais comuns? Distribuções discretas: Uniforme: resultados (finitos) que são igualmente prováveis.REF? Binomial: número de sucessos em k tentativas.REF? Poisson: número de eventos em um intervalo de tempo fixo.REF? Bernoulli: .REF? Geométrica: número de testes até o 1o sucesso.REF? Binomial negativa: número de testes até o k-ésimo sucesso.REF? Hipergeométrica: número de indivíduos na amostra tomados sem reposição.REF? Distribuições contínuas: Uniforme: resultados que possuem a mesma densidade.REF? Exponencial: tempo entre eventos.REF? Normal: .REF? Normal padrão: .REF? Aproximação binomial: número de sucessos em uma grande quantidade de tentativas.REF? Aproximação Poisson: número de ocorrências em um intervalo de tempo fixo.REF? Qui-quadrado: .REF? t-Student: .REF? Weibull: .REF? Log-normal: .REF? Beta: .REF? Gama: .REF? Logística: .REF? Pareto.REF? 12.1.5 Quais são as funções de uma distribuição? Função de massa de probabilidade (probability mass function, pmf).REF? Função de distribuição cumulativa (cumulative distribution function, cdf).REF? Função quantílicas (quantile function, qf).REF? Função geradora de números aleatórios (random function, rf).REF? O pacote stats44 fornece funções de distribuição de probabilidade (p), funções de densidade (d), funções quantílicas (q) e funções geradores de números aleatórios (r) para as distribuições normal, Student t, binomial, qui-quadrado, uniforme, dentre outras. O pacote ggdist106 fornece a função geom_slabinterval para criar gráficos de distribuição de probabilidade (p) e funções de densidade (d) as distribuições. O pacote ggfortify107 fornece a função ggdistribution para criar gráficos de distribuição de probabilidade (p), funções de densidade (d), funções quantílicas (q) e funções geradores de números aleatórios (r) para as distribuições. 12.1.6 O que é a distribuição normal? A distribuição normal (ou gaussiana) é uma distribuição com desvios simétricos positivos e negativos em torno de um valor central.79 Em uma distribuição normal, o intervalo de 1 desvio-padrão (±1DP) inclui cerca de 68% dos dados; de 2 desvios-padrão (±2DP) cerca de 95% dos dados; e no intervalo de 3 desvios-padrão (±3DP) cerca de 99% dos dados.79 Figura 12.1: Distribuições e funções de probabilidade 12.1.7 Que métodos podem ser utilizados para identificar a normalidade da distribuição? Histogramas.49 Gráficos Q-Q.49 Testes de hipótese nula:49 Kolmogorov-Smirnov Shapiro-Wilk Anderson-Darling 12.1.8 O que são distribuições não-normais? .REF? 12.2 Parâmetros 12.2.1 O que são parâmetros? Parâmetros são informações que definem um modelo teórico, como propriedades de uma coleção de indivíduos.78 Parâmetros definem características de uma população inteira, tipicamente não observados por ser inviável ter acesso a todos os indivíduos que constituem tal população.49 O pacote base52 fornece a função summary para calcular diversos parâmetros descritivos. 12.2.2 O que é uma análise paramétrica? Testes paramétricos possuem suposições sobre as características e/ou parâmetros da distribuição dos dados na população.49 Testes paramétricos assumem que: a variável é quantitativa numérica (contínua); os dados foram amostrados de uma população com distribuição normal; a variância da(S) amostra(s) é igual à da população; as amostras foram selecionadas de modo aleatório na população; os valores de cada amostra são independentes entre si.49,79 Testes paramétricos são baseados na suposição de que os dados amostrais provêm de uma população com parâmetros fixos determinando sua distribuição de probabilidade.8 12.2.3 O que é uma análise não paramétrica? Testes não-paramétricos fazem poucas suposições, ou menos rigorosas, sobre as características e/ou parâmetros da distribuição dos dados na população.49,79 Testes não-paramétricos são úteis quando as suposições de normalidade não podem ser sustentadas.79 12.2.4 Devemos testar as suposições de normalidade? Testes preliminares de normalidade não são necessários para a maioria dos testes paramétricos de comparação, pois eles são robustos contra desvios moderados da normalidade. Normalidade da distribuição deve ser estabelecida para a população.108 12.2.5 Por que as análises paramétricas são preferidas? Em geral, testes paramétricos são mais robustos (isto é, possuem menores erros tipo I e II) que seus testes não-paramétricos correspondentes.49,109 Testes não-paramétricos apresentam menor poder estatístico (maior erro tipo II) comparados aos testes paramétricos correspondentes.79 12.2.6 Que parâmetros podem ser estimados? Parâmetros de tendência central.79,110 Parâmetros de dispersão.79,110,111 Parâmetros de proporção.79,110,112,112 Parâmetros de distribuição.110 Parâmetros de extremos.79 O pacote base52 fornece a função summary para calcular diversos parâmetros descritivos. 12.3 Tendência central 12.3.1 Que parâmetros de tendência central podem ser estimados? Média: aritmética, ponderada, geométrica ou harmônica.79,110,113 Mediana.79,110,114 Moda.79,110,114 A posição relativa das medidas de tendência central (média, mediana e moda) depende da forma da distribuição.114 Em uma distribuição normal, as três medidas são idênticas.114 A média é sempre puxada para os valores extremos, por isso é deslocada para a cauda em distribuições assimétricas.114 A mediana fica entre a média e a moda em distribuições assimétricas.114 A moda é o valor mais frequente e, portanto, se localiza no pico da distribuição assimétrica.114 O pacote base52 fornece a função summary para calcular diversos parâmetros descritivos. 12.3.2 Como escolher o parâmetro de tendência central? A mediana é preferida à média quando existem poucos valores extremos na distribuição, alguns valores são indeterminados, ou há uma distribuição aberta, ou os dados são medidos em uma escala ordinal.114 A moda é preferida quando os dados são medidos em uma escala nominal.114 A média geométrica é preferida quando os dados são medidos em uma escala logarítmica.114 12.4 Dispersão 12.4.1 Que parâmetros de dispersão podem ser estimados? Variância.79,110 Desvio-padrão: Informam sobre a dispersão da população e são, portanto, úteis como preditores da variação em novas amostras.111,115,116 Erro-padrão: Refletem a incerteza na média e sua dependência do tamanho da amostra.111,115 Amplitude.79,110,116 Intervalo interquartil.79,110,116 Intervalo de confiança: Captura a média populacional correspondente ao nível de significância \\(\\alpha\\) pré-estabelecido.79,110,115,117 O pacote base52 fornece a função summary para calcular diversos parâmetros descritivos. O pacote stats44 fornece a função confint para calcular o intervalo de confiança em um nível de significância \\(\\alpha\\). 12.4.2 Como escolher o parâmetro de dispersão? Desvio-padrão é apropriado quando a média é utilizada como parâmetro de tendência central em distribuições simétricas.116 Amplitue ou intervalo interquartil são apropriadas para variáveis ordinais ou distribuições assimétricas.116 12.5 Proporção 12.5.1 Que parâmetros de proporção podem ser estimados? Frequência absoluta.79,110,112 Frequência relativa.79,110,112 Percentil.79,110,112 Quantil: é o ponto de corte que define a divisão da amostra em grupos de tamanhos iguais. Portanto, não se referem aos grupos em si, mas aos valores que os dividem:112 Tercil: 2 valores que dividem a amostra em 3 grupos de tamanhos iguais.112 Quartil: 3 valores que dividem a amostra em 4 grupos de tamanhos iguais.112 Quintil: 4 valores que dividem a amostra em 5 grupos de tamanhos iguais.112 Decil: 9 valores que dividem a amostra em 10 grupos de tamanhos iguais.112 O pacote base52 fornece a função summary para calcular diversos parâmetros descritivos. O pacote base52 fornece a função table para calcular proporções. O pacote stats52 fornece a função quantile para executar análise de percentis. 12.6 Distribuição 12.6.1 Que parâmetros de distribuição podem ser estimados? Assimetria.110 Curtose.110 12.7 Extremos 12.7.1 O que são extremos? Valores extremos podem constituir valores legítimos ou ilegítimos de uma distribuição.118 12.7.2 Que parâmetros extremos podem ser estimados? Mínimo.79 Máximo.79 O pacote base52 fornece a função summary para calcular diversos parâmetros descritivos. 12.8 Valores discrepantes 12.8.1 O que são valores discrepantes (outliers)? Em termos gerais, um valor discrepante - “fora da curva” ou outlier - é uma observação que possui um valor relativamente grande ou pequeno em comparação com a maioria das observações.119 Um valor discrepante é uma observação incomum que exerce influência indevida em uma análise.119 Valores discrepantes são dados com valores altos de resíduos.118 12.8.2 Quais são os tipos de valores discrepantes? Valores discrepantes podem ser categorizados em três subtipos: outliers de erro, outliers interessantes e outliers aleatórios.118 Os valores discrepantes de erro são observações claramente não legítimas, distantes de outros dados devido a imprecisões por erro de mensuração e/ou codificação.118 Os valores discrepantes interessantes não são claramente erros, mas podem refletir um processo/mecanismo potencialmente interessante para futuras pesquisas.118 Os valores discrepantes aleatórios são observações que resultam por acaso, sem qualquer padrão ou tendência conhecida.118 Valores discrepantes podem ser univariados ou multivariados.118 12.8.3 Por que é importante avaliar valores discrepantes? Excluir o valor discrepante implica em reduzir inadequadamente a variância, ao remover um valor que de fato pertence à distribuição considerada.118 Manter os dados inalterados (mantendo o valor discrepante) implica em aumentar inadequadamente a variância, pois a observação não pertence à distribuição que fundamenta o experimento.118 Em ambos os casos, uma decisão errada pode influenciar o erro do tipo I (\\(\\alpha\\) — rejeitar uma hipótese verdadeira) ou o erro do tipo II (\\(\\beta\\) — não rejeitar uma hipótese falsa).118 12.8.4 Como detectar valores discrepantes? Na maioria das vezes, não há como saber de qual distribuição uma observação provém. Por isso, não é possível ter certeza se um valor é legítimo ou não dentro do contexto do experimento.118 Recomenda-se seguir um procedimento em duas etapas: detectar possíveis candidatos a outliers usando ferramentas quantitativas; e gerenciar os outliers, decidindo manter, remover ou recodificar os valores, com base em informações qualitativas.118 A detecção de outliers deve ser aplicada apenas uma vez no conjunto de dados; um erro comum é identificar e tratar os outliers (como remover ou recodificar) e, em seguida, reaplicar o procedimento no conjunto de dados já modificado.118 A detecção ou o tratamento dos outliers não deve ser realizada após a análise dos resultados, pois isso introduz viés nos resultados.118 12.8.5 Quais são os métodos para detectar valores discrepantes? Valores univariados são comumente considerados outliers quando são mais extremos do que a média ± (desvio padrão × constante), podenso essa constante ser 3 (99,7% das observações estão dentro de 3 desvios-padrão da média) ou 3,29 (99,9% estão dentro de 3,29 desvios-padrão).118 Para detectar outliers univariados, recomenda-se o uso da Mediana da Desviação Absoluta (Median Absolute Deviation, MAD), calculado a partir de um intervalo em torno da mediana, multiplicado por uma constante (valor padrão: 1,4826).118,120 Para detectar outliers multivariados, comumente utiliza-se a distância de Mahalanobis, que identifica valores muito distantes do centróide formado pela maioria dos dados (por exemplo, 99%).118 Para detectar outliers multivariados, recomenda-se o Determinante de Mínima Covariância (Minimum Covariance Determinant, MCD), pois possui o maior ponto de quebra possível e utiliza a mediana, que é o indicador mais robusto em presença de outliers.118,121 12.8.6 Como manejar os valores discrepantes? Manter outliers pode ser uma boa decisão se a maioria desses valores realmente pertence à distribuição de interesse. Manter outliers que pertencem a uma distribuição alternativa pode ser problemático, pois um teste pode se tornar significativo apenas por causa de um ou poucos outliers.118 Remover outliers pode ser eficaz quando eles distorcem a estimativa dos parâmetros da distribuição. Remover outliers que pertencem legitimamente à distribuição pode reduzir artificialmente a estimativa do erro.118 Remover outliers leva à perda de observações, especialmente em conjuntos de dados com muitas variáveis, quando outliers univariados são excluídos em cada variável.118 Recodificar outliers evita a perda de uma grande quantidade de dados, mas deve ser baseada em argumentos razoáveis e convincentes.118 Erros de observação e de medição são uma justificativa válida para descartar observações discrepantes.119 12.8.7 Como conduzir análises com valores discrepantes? É importante reportar se existem valores discrepantes e como foram tratados.119 Valores discrepantes na variável de desfecho podem exigir uma abordagem mais refinada, especialmente quando representam uma variação real na variável que está sendo medida.119 Valores discrepantes em uma (co)variável podem surgir devido a um projeto experimental inadequado; nesse caso, abandonar a observação ou transformar a covariável são opções adequadas.119 Valores discrepantes podem ser recodificados usando a Winsorização,122 que transforma os outliers em valores de percentis específicos (como o 5º e o 95º).118 O pacote outliers123 fornece a função outlier para identificar os valores mais distantes da média. O pacote outliers123 fornece a função rm.outlier para remover os valores mais distantes da média detectados por testes de hipótese e/ou substitui-los pela média ou mediana. Referências "],["análise-inicial-de-dados.html", "Capítulo 13 Análise inicial de dados 13.1 Análise inicial de dados", " Capítulo 13 Análise inicial de dados 13.1 Análise inicial de dados 13.1.1 O que é análise inicial de dados? Análise inicial de dados124 é uma sequência de procedimentos que visam principalmente a transparência e integridade das pré-condições do estudo para conduzir a análise estatística apropriada de modo responsável para responder aos problemas da pesquisa.50 O objetivo da análise inicial de dados é propiciar dados prontos para análise estatística, incluindo informações confiáveis sobre as propriedades dos dados.50 A análise inicial de dados pode ser dividida nas seguintes etapas:50 Configuração dos metadados Limpeza dos dados Verificação dos dados Relatório inicial dos dados Refinamento e atualização do plano de análise estatística Documentação e relatório da análise inicial de dados A análise inicial de dados não deve ser confundida com análise exploratória125, nem deve ser utilizada para hipotetizar após os dados serem coletados (conhecido como Hypothesizing After Results are Known, HARKing)126. 13.1.2 Como conduzir uma análise inicial de dados? Desenvolva um plano de análise inicial de dados consistente com os objetivos da pesquisa. Por exemplo, verifique a distribuição e escala das variáveis, procure por observações não-usuais ou improváveis, avalie possíveis padrões de dados perdidos.50 Não altere diretamente os dados de uma tabela obtida de uma fonte. Use scripts para implementar eventuais alterações, de modo a manter o registro de todas as modificações realizadas no banco de dados.50 Use os metadados do estudo para guiar a análise inicial dos dados e compartilhe com os dados para maior transparência e reprodutibilidade.50 Representação gráfica dos dados pode ajudar a identificar características e padrões no banco de dados, tais como suposições e tendências.50 Verifique a frequência e proporção de dados perdidos em cada variável, e depois examine por padrões de dados perdidos simultaneamente por duas ou mais variáveis.50 Verifique a frequência e proporção de dados perdidos em cada variável, e depois examine por padrões de dados perdidos simultaneamente por duas ou mais variáveis.50 Exclusão de dados ad hoc baseada no desfecho pode influenciar os resultados do estudo, portanto os critérios de exclusão de dados antes da análise estatística (descritiva e/ou inferencial) devem ser reportados.127 13.1.3 Quais problemas podem ser detectados na análise inicial de dados? Ocorrência de dados perdidos, que podem ser excluídos ou imputados para não reduzir o poder do estudo.REF? O pacote stats44 fornece a função na.omit para retornar os dados sem os dados perdidos. O pacote stats44 fornece a função complete.cases para identificar os casos completos - isto é, sem dados perdidos - em um banco de dados. Registros duplicados, que devem ser excluídos para não inflar a amostra.128 O pacote base52 fornece a função duplicated para identificar elementos duplcados de um banco de dados. Codificação 0 ou 1 para variáveis dicotômicas para representar a direção esperada da associação entre elas.128 Ordenação cronológica de variáveis com registros temporais (retrospectivos ou prospectivos).128 A distribuição das variáveis para verificação das suposições das análises planejadas.128 Ocorrência de efeitos teto e piso nas variáveis.128 Referências "],["análise-exploratória-de-dados.html", "Capítulo 14 Análise exploratória de dados 14.1 Análise exploratória de dados", " Capítulo 14 Análise exploratória de dados 14.1 Análise exploratória de dados 14.1.1 O que é análise exploratória de dados? Análise exploratória de dados consiste em um processo iterativo de elaboração e interpretação da síntese de dados, tabelas e gráficos, considerando os aspectos teóricos do estudo.125 Análise exploratória deve ser separada da análise inferencial de testes de hipóteses; a decisão sobre os modelos a testar deve ser feita a priori.119 14.1.2 Por que conduzir a análise exploratória de dados? A condução de análise exploratória de dados pode ajudar a identificar padrões e pode orientar trabalhos futuros, mas os resultados não devem ser interpretados como inferências sobre uma população.119 A análise exploratória não deve ser usada para definir as questões e hipóteses científicas do estudo.119 O pacote explore129 fornece a função explore para análise exploratória de um banco de dados. O pacote dataMaid130 fornece a função makeDataReport para criar um relatório de análise exploratória de um banco de dados. O pacote DataExplorer131 fornece a função create_report para criar um relatório de análise exploratória de um banco de dados. O pacote SmartEDA132 fornece a função ExpReport para criar um relatório de análise exploratória de um banco de dados. O pacote gtExtras133 fornece a função gt_plt_summary para criar uma tabela descritiva síntese com histogramas ou gráficos de barra a partir de um banco de dados. O pacote radiant134 fornece a função radiant para executar uma interface interativa para análise exploratória de dados. 14.1.3 Quais etapas constituem a análise exploratória de dados? Cada combinação de problema de pesquisa e delineamento de estudo pode demandar um plano de análise exploratório distinto.119 Verifique a existência e/ou influência de valores discrepantes (“fora da curva” ou outliers):119,124,125 Boxplots Gráficos quantil-quantil (Q-Q) O pacote graphics105 fornece a função boxplot para construção de gráficos boxplot. Verifique a homocedasticidade (homogeneidade da variância):119 Boxplots condicionais (por fator de análise) Análise dos resíduos do modelo de regressão Gráfico resíduos vs. valores ajustados Verifique a normalidade da distribuição dos dados:119,124 Histograma das variáveis (por fator de análise) Histograma dos resíduos da regressão Verifique a existência de grande quantidade de valores nulos (=0):119 Histograma das variáveis (por fator de análise) Verifique a existência de colinearidade entre variáveis independentes de um modelo de regressão:119 Fator de inflação de variância (variance inflation factor, VIF) Coeficiente de correlação de Pearson (\\(r\\)) Gráfico de dispersão entre variáveis Verifique possíveis relações entre as variáveis dependente(s) e independente(s) de um modelo de regressão:119 Gráfico de dispersão entre variáveis independente e dependente Verifique possíveis interações entre as variáveis dependente(s) de um modelo de regressão:119 Gráfico coplot de dispersão entre variáveis dependentes O pacote graphics105 fornece a função coplot para construção de gráficos boxplot condicionais. Verifique por dependência entre variáveis de um modelo de regressão:119 Gráfico de série temporal das variáveis Gráfico de autocorrelação entre as variáveis Referências "],["análise-descritiva.html", "Capítulo 15 Análise descritiva 15.1 Análise descritiva 15.2 Apresentação de resultados numéricos 15.3 Tabelas 15.4 Tabela 1 15.5 Tabela 2 15.6 Gráficos", " Capítulo 15 Análise descritiva 15.1 Análise descritiva 15.1.1 O que é análise descritiva? Análise descritiva é usada para compreendermos algum aspecto de um conjunto de dados, respondendo a perguntas do tipo “quando?”, “onde?”, “quem?”, “o quê?”, “como?” e “e daí?”.49,135 15.1.2 Como apresentar os resultados descritivos? Variáveis categóricas: Reporte valores de frequência absoluta e relativa (n, percentual).136 Organização das tabelas: as variáveis são exibidas em linhas e os grupos são exibidos em colunas.136 Calcule percentagens para as colunas (isto é, entre grupos) e não entre linhas.136 Em caso de dados perdidos, não inclua uma linha com total de dados perdidos, pois distorce as proporções entre colunas e as análises de tabela de contingência. Indique no texto ou em uma coluna separada o total de dados perdidos por variável.136 15.2 Apresentação de resultados numéricos 15.2.1 O que são casas decimais? O número de casas decimais refere-se à quantidade de dígitos que aparecem após a vírgula decimal.137,138 15.2.2 O que são dígitos significativos? O termo “dígitos significativos” é preferido a “algarismos significativos” ou “dígitos efetivos” e não se relaciona com significância estatística.137,138 O número de dígitos significativos é a soma total de dígitos, desconsiderando a vírgula decimal e os zeros à esquerda; os zeros à direita são considerados informativos, salvo exceções.137,138 15.2.3 Como arredondar dados numéricos? Apresentar dados com quantidade excessiva de casas decimais pode dificultar a interpretação e induzir erroneamente uma precisão espúria.137,138 A precisão é determinada pelo grau de arredondamento aplicado, medido em casas decimais ou dígitos significativos.137,138 Tabela 15.1: Quantidade de casas decimais e dígitos significativos. Valor Casas Decimais Dígitos Significativos 0,00789 5 0 0,0456 4 0 45,6 1 2 123,456 3 3 7890,0000 4 4 O arredondamento também introduz erros, uma vez que aumenta a imprecisão (isto é, incerteza) em torno do valor original.137,138 Tabela 15.2: Valores originais, arredondamentos e erros de arredondamento por casas decimais. Valor Casas Decimais Dígitos Significativos 2 Casas decimais [Margem de erro] 1 Casa decimal [Margem de erro] Sem casa decimal [Margem de erro] 0,00789 5 0 0,01 [0,005, 0,015] 0,0 [-0,05, 0,05] 0 [-0,5, 0,5] 0,0456 4 0 0,05 [0,045, 0,055] 0,0 [-0,05, 0,05] 0 [-0,5, 0,5] 45,6 1 2 45,60 [45,595, 45,605] 45,6 [45,55, 45,65] 46 [45,5, 46,5] 123,456 3 3 123,46 [123,455, 123,465] 123,5 [123,45, 123,55] 123 [122,5, 123,5] 7890,0000 4 4 7890,00 [7889,995, 7890,005] 7890,0 [7889,95, 7890,05] 7890 [7889,5, 7890,5] A regra geral é utilizar 2 ou 3 dígitos significativos para tamanhos de efeito e 1 ou 2 dígitos significativos para medidas de variabilidade.138 Regra dos 3 dígitos significativos para proporção de risco: em média, o erro de arredondamento é menor que os 0,5% exigidos, de modo que três dígitos significativos são mais precisos do que o necessário.137 Regra dos 4 dígitos significativos para proporção de risco: divida a proporção de risco por quatro e arredonde para dois dígitos significativos e, em seguida, relate a proporção para esse número de casas decimais.137 15.3 Tabelas 15.3.1 Por que usar tabelas? Tabelas complementam o texto (e vice-versa), e podem apresentar os dados de modo mais acessível e informativo.139 15.3.2 Que informações incluir nas tabelas? Título ou legenda, uma síntese descritiva (geralmente por meio de parâmetros descritivos), intervalos de confiança e/ou P-valores conforme necessário para adequada interpretação.139,140 15.3.3 Quais são os erros mais comuns de preenchimento de tabelas? Erros tipográficos.141 Ausência de rótulos ou unidades nas variáveis.141 Relatar estatísticas incorretamente, tais como rotular variáveis contínuas como porcentagens.141 Estatísticas descritivas de tendência central (ex.: médias) relatadas sem a estatística de dispersão correspondente (ex.: desvio-padrão).141 Desvio-padrão nulo (\\(\\sigma=0\\)).141 Valores porcentuais que não correspondem ao numerador dividido pelo denominador.141 O pacote flextable142 fornece as funções flextable, as_flextable e save_as_docx para criar e salvar tabelas tabelas formatadas em DOCX. O pacote rempsyc143 fornece a função nice_table para criar tabelas formatadas. O pacote table1144 fornece a função table1 para construção de tabelas. O pacote gtsummary145 fornece a função tbl_summary para construção da ‘Tabela 1’ com dados descritivos. 15.4 Tabela 1 15.4.1 O que é a ‘Tabela 1’? A ‘Tabela 1’ descreve as características demográficas, sociais e clínicas da amostra, completa ou agrupada por algum fator, geralmente por meio de parâmetros de tendência central e dispersão.146,147 15.4.2 Qual a utilidade da ‘Tabela 1’? Descrever (conhecer) as características da amostra e dos grupos sendo comparados, quando aplicável.147 Verificar aderência ao protocolo do estudo, incluindo critérios de inclusão/exclusão, tamanho da amostra e perdas amostrais.147 Permitir a replicação do estudo.147 Meta-analisar os dados junto a estudos similares.147 Avaliar a generalização (validade externa) das conclusões do estudo.147 15.4.3 O que é a falácia da ‘Tabela 1’? Falácia da Tabela 1 ocorre pela interpretação errônea dos P-valores na comparação entre grupos, na linha de base, de um ensaio clínico aleatorizado.148 15.4.4 Como construir a ‘Tabela 1’? A Tabela 1 geralmente é utilizada para descrever as características da amostra estudada, possibilitando a análise de ameaças à validade interna e/ou externa ao estudo.109,149 O pacote table1144 fornece a função table1 para construção de tabelas. O pacote gtsummary145 fornece a função tbl_summary para construção da ‘Tabela 1’ com dados descritivos. 15.5 Tabela 2 15.5.1 Qual a utilidade da ‘Tabela 2’? A Tabela 2 mostra associações ajustadas multivariadas com o resultado para variáveis resumidas na Tabela 1.146 15.5.2 O que é a falácia da ‘Tabela 2’? A Tabela 2 pode induzir ao erro de interpretação pelas estimativas de efeitos para covariáveis do modelo também serem utilizados para controlar a confusão da exposição.146,150 Ao apresentar estimativas de efeito ajustadas para covariáveis juntamente com a estimativa de efeito ajustada para a exposição primária, a Tabela 2 sugere implicitamente que todas estas estimativas podem ser interpretadas de forma semelhante, se não de forma idêntica, como estimativa do efeito total.146,150 A falácia da Tabela 2 pode ser evitada limitando-se a tabela a estimativas das medidas primárias do efeito de exposição nos diferentes modelos, com as covariáveis secundárias de “ajuste” relatadas em uma nota de rodapé, juntamente com a forma como foram categorizadas ou modeladas.146 15.5.3 Como construir a ‘Tabela 2’? A Tabela 2 pode ser utilizada para apresentar estimativas de múltiplos efeitos ajustados de um mesmo modelo estatístico.146 O pacote table1144 fornece a função table1 para construção de tabelas. O pacote gtsummary145 fornece a função tbl_summary para construção da ‘Tabela 1’ com dados descritivos. 15.6 Gráficos 15.6.1 O que são gráficos? Gráficos são utilizados para apresentar dados (geralmente em grande quantidade) de modo mais intuitivo e fácil de compreender.151 15.6.2 Que elementos incluir em gráficos? Título, eixos horizontal e vertical com respectivas unidades, escalas em intervalos representativos das variáveis, legenda com símbolos, síntese descritiva dos valores e respectiva margem de erro, conforme necessário para adequada interpretação.151 Os pacotes ggplot2152, plotly153 e corrplot154 fornecem diversas funções para construção de gráficos tais como ggplot, plot_ly e corrplot respectivamente. 15.6.3 Para que servem as barras de erro em gráficos? Barras de erro ajudam ao autor a apresentar as informações que descrevem os dados (por exemplo, em uma análise descritiva) ou sobre as inferências ou conclusões tomadas a partir de dados.115,117 Barras de erro mais longas representam mais imprecisão (maiores erros), enquanto barras mais curtas representam mais precisão na estimativa.117 Barras de erro descritivas geralmente apresentam a amplitude (mínimo-máximo) ou desvio-padrão.117 Barras de erro inferenciais geralmente apresentam o erro-padrão ou intervalo de confiança no nível de significância \\(\\alpha\\) pré-estabelecido.115,117 Barras de erro com desvio-padrão são úteis para descrever a variabilidade dos dados, enquanto as barras de erro com erro padrão da média são úteis para descrever a precisão do parâmetro estimado (média) e sua relação com o tamanho da amostra.115 Barras de erro com intervalo de confiança são úteis para fornecer uma estimativa da incerteza da estimativa do parâmetro populacional.115 O comprimento das barras de erro sugere graficamente a imprecisão dos dados do estudo, uma vez que o valor verdadeiro da população pode estar em qualquer nível do intervalo da barra.117 De modo contraintuitivo, um espaço entre as barras não garante significância, nem a sobreposição a descarta—depende do tipo de barra.115 Para amostras pequenas é preferível apresentar os dados brutos, uma vez que as barras de erro não serão muito informativas.115 15.6.4 Quais são as boas práticas na elaboração de gráficos? O tamanho da amostra total e subgrupos, se houver, deve estar descrito na figura ou na sua legenda.117 Para análise inferencial de figuras, as barras de erro representadas por erro-padrão ou intervalo de confiança no nível de significância \\(\\alpha\\) pré-estabelecido são preferíveis à amplitude ou desvio-padrão.115,117 Evite gráficos de barra e mostre a distribuição dos dados sempre que possível.155 Exiba os pontos de dados em boxplots.155 Use jitter simétrico em gráficos de pontos para permitir a visualização de todos os dados.155 Prefira palhetas de cor adaptadas para daltônicos.155 O pacote ggsci156 fornece palhetas de cores tais como pal_lancet, pal_nejm e pal_npg inspiradas em publicações científicas para uso em gráficos. O pacote grDevices104 fornece a função dev.new para controlar diversos aspectos do gráfico, tais como tamanho e resolução. 15.6.5 Como exportar figuras em formato TIFF? .REF? O pacote tiff157 fornece a função writeTIFF para exportar gráficos em formato TIFF. Referências "],["análise-inferencial.html", "Capítulo 16 Análise inferencial 16.1 Raciocínio inferencial 16.2 Hipóteses científicas 16.3 Hipóteses estatísticas 16.4 Testes de hipóteses 16.5 Poder do teste 16.6 Inferência visual 16.7 Interpretação de análise inferencial 16.8 Erros de inferência", " Capítulo 16 Análise inferencial 16.1 Raciocínio inferencial 16.1.1 O que é análise inferencial? Na análise inferencial são utilizados dados da(s) amostra(s) para fazer uma inferência válida (isto é, estimativa) sobre os parâmetros populacionais desconhecidos.49 No paradigma de Jerzy Neyman e Egon Pearson, um teste de hipótese científica envolve a tomada de decisão sobre hipóteses nulas (\\(H_{0}\\)) e alternativa (\\(H_{1}\\)) concorrentes e mutuamente exclusivas.158 16.1.2 Quais são os tipos de raciocínio inferencial? Inferência dedutiva: Uma dada hipótese inicial é utilizada para prever o que seria observado caso tal hipótese fosse verdadeira.159 Inferência indutiva: Com base nos dados observados, avalia-se qual hipótese é mais defensável (isto é, mais provável).159 16.1.3 Quais são as questões fundamentais da análise inferencial? A direção do efeito160 A magnitude do efeito160 A importância do efeito160 16.2 Hipóteses científicas 16.2.1 O que é hipótese científica? Hipótese científica é uma ideia que pode ser testada.158 Definir claramente os problemas e os objetivos da pesquisa são o ponto de partida de todos os estudos científicos.48 16.2.2 Quais são as fontes de ideias para gerar hipóteses científicas? Revisão das práticas atuais.161 Desafio a ideias aceitas.161 Conflito entre ideias divergentes.161 Variações regionais, temporais e populacionais.161 Experiências dos próprios pesquisadores.161 Imaginação sem fronteiras ou limites convencionais.161 16.3 Hipóteses estatísticas 16.3.1 O que é hipótese nula? A hipótese nula (\\(H_{0}\\)) é uma expressão que representa o estado atual do conhecimento (status quo), em geral a não existência de um determinado efeito.110 16.3.2 O que é hipótese alternativa? A hipótese alternativa (\\(H_{1}\\)) é uma expressão que contém as situações que serão testadas, de modo que um resultado positivo indique alguma ação a ser conduzida.110 16.3.3 Qual hipótese está sendo testada? A hipótese nula (\\(H_{0}\\)) é a hipótese sob teste em análises inferenciais.79 Pode-se concluir sobre rejeitar ou não rejeitar a hipótese nula (\\(H_{0}\\)).79 Não se conclui sobre a hipótese alternativa (\\(H_{1}\\)).110 Para testar a hipótese nula, deve-se selecionar o nível de significância crítica (P-valor de corte); a probabilidade de rejeitarmos uma hipótese nula verdadeira (\\(\\alpha\\)); e a probabilidade de não rejeitarmos uma hipótese nula falsa (\\(\\beta\\)).158 16.4 Testes de hipóteses 16.4.1 Quais são os tipos de teste de hipóteses? Teste (clássico) de significância da hipótese nula.162 Teste de mínimos efeitos.162 Teste de equivalência.162 Teste de inferioridade.162 Teste de não-inferioridade.REF? Teste de superioridade.REF? 16.4.2 O que é uma família de hipóteses? .REF? 16.4.3 O que são testes ad hoc e post hoc? .REF? 16.4.4 Como ajustar a análise inferencial para hipóteses múltiplas? .REF? O pacote stats44 fornece a função p.adjust para ajustar o P-valor utilizando diversos métodos. 16.4.5 O que são testes unicaudais e bicaudais? .REF? 16.4.6 O que reportar após um teste de hipótese? P-valores, como estimativa da significância estatística.163 Tamanho do efeito, como estimativa de significância substantiva (clínica).163 16.5 Poder do teste 16.5.1 O que é poder do teste? Poder do teste é a probabilidade de rejeitar corretamente a hipótese nula (\\(H_{0}\\)) quando esta é falsa.158 Poder do teste pode ser calculado como (\\(1 - \\beta\\)).158 16.5.2 O que é análise de poder do teste? Poder é a probabilidade de que um dado tamanho de efeito será observado em um experimento futuro sob um conjunto de hipóteses - tamanho de efeito real e erro tipo I - para um dado tamanho de amostra.164 O objetivo geral da análise de poder ao projetar um estudo é escolher um tamanho de amostra que controle os 2 tipos de erros de inferência estatística: tipo I (\\(\\alpha\\), resultado falso-positivo) e tipo II (\\(\\beta\\), resultado falso-negativo).164 Numericamente, o poder de um estudo é calculado como \\(1-\\beta\\) e reportado em valor percentual.164 16.5.3 Quando realizar a análise de poder do teste? Na fase de projeto de pesquisa: a análise de poder para determinar o tamanho da amostra objetiva que o tamanho da amostra permita uma probabilidade razoável de detectar um efeito significativo pré-especificado.164 Após a coleta de dados: a análise de poder objetiva informar estudos futuros a respeito do tamanho da amostra necessário para a detecção de um efeito significativo pré-especificado.164 O pacote pwr165 fornece a função pwr.2p.test para cálculo do poder do teste de proporção balanceado (2 amostras com mesmo número de participantes). O pacote pwr165 fornece a função pwr.2p2n.test para cálculo do do poder do teste de proporção não balanceado (2 amostras com diferente número de participantes). O pacote pwr165 fornece a função pwr.anova.test para cálculo do poder do teste de análise de variância balanceado (3 ou mais amostras com mesmo número de participantes). O pacote pwr165 fornece a função pwr.chisq.test para cálculo do poder do teste de qui-quadrado \\(\\chi^2\\). O pacote pwr165 fornece a função pwr.f2.test para cálculo do poder do teste com modelo linear geral. O pacote pwr165 fornece a função pwr.norm.test para cálculo do poder do teste de média de uma distribuição normal com variância conhecida. O pacote pwr165 fornece a função pwr.p.test para cálculo do poder do teste de proporção (1 amostra). O pacote pwr165 fornece a função pwr.r.test para cálculo do do poder to teste de correlação (1 amostra). O pacote pwr165 fornece a função pwr.t.test para cálculo do poder do teste t de diferença de 1 amostra, 2 amostras dependentes ou 2 amostras independentes (grupos balanceados). O pacote pwr165 fornece a função pwr.t2n.test para cálculo do poder do teste t de diferença de 2 amostras independentes (grupos não balanceados). O pacote longpower166 fornece a função power.mmrm para calcular o poder de testes com análises por modelo de regressão linear misto. O pacote Superpower167 fornece a função power.ftest para calcular o poder do teste por análise de testes F. O pacote Superpower167 fornece a função power_oneway_between para calcular o poder do teste por análise de variância (ANOVA) de 1 fator entre-sujeitos. O pacote Superpower167 fornece a função power_oneway_within para calcular o poder do teste por análise de variância (ANOVA) de 1 fator intra-sujeitos. O pacote Superpower167 fornece a função power_oneway_ancova para calcular o poder do teste por análise de covariância (ANCOVA). O pacote Superpower167 fornece a função power_twoway_between para calcular o poder do teste por análise de covariância (ANOVA) de 2 fatores entre-sujeitos. O pacote Superpower167 fornece a função power_threeway_between para calcular o poder do teste por análise de covariância (ANOVA) de 3 fatores entre-sujeitos. O pacote InteractionPoweR168 fornece a função power_interaction para calcular o poder do teste por análise de efeito de interações. 16.5.4 Por que a análise de poder do teste post hoc é inadequada? A análise do poder é teoricamente incorreta, uma vez que a probabilidade calculada \\(1-\\beta\\) expressa a probabilidade de um evento futuro, o que não é mais relevante quando o evento de interesse já ocorreu.136,164 16.5.5 O que pode ser realizado ao invés da análise de poder? Após a coleta e análise de dados, recomenda-se realizar a análise e interpretação dos resultados a partir do tamanho do efeito e do seu intervalo de confiança no nível de significância \\(\\alpha\\) pré-estabelecido.164 16.6 Inferência visual 16.6.1 O que é inferência visual? Inferência visual consiste na interpretação de dados apresentados em gráficos.169 Para inferência visual, recomenda-se a apresentação dos dados em gráficos com estimativas de tendência central e seu intervalo (preferencialmete intervalo de confiança no nível de significância \\(\\alpha\\) pré-estabelecido).169 16.6.2 Por que usar intervalos de confiança para inferência visual? Intervalos de confiança fornecem estimativas pontuais e intervalares na mesma unidade de medida da variável.169 Existe uma relação entre o intervalo de confiança e o valor de P obtido pelo teste de significância de hipótese nula, em que ambos consideram o mesmo nível de significância \\(\\alpha\\) pré-estabelecido.169 16.6.3 Como interpretar intervalos de confiança em uma figura? Identifique o que as tendências centrais e as barras de erro representam. Qual é a variável dependente? É expressa em unidades originais ou é padronizada ? A figura mostra intervalos de confiança, erro-padrão ou desvio-padrão? Qual é o desenho experimental?169 Faça uma interpretação substantiva dos valores de tendência central e dos intervalos de confiança.169 O intervalo de confiança é uma faixa de valores plausíveis para a tendência central. Valores fora do intervalo são relativamente implausíveis, no nível de significância \\(\\alpha\\) pré-estabelecido.169 Qualquer valor fora do intervalo de confiança, quando considerado como hipótese nula (\\(H_{0}\\)), equivale a \\(P &lt; \\alpha\\) pré-estabelecido (bicaudal).169 Qualquer valor dentro do intervalo, quando considerado como hipótese nula (\\(H_{0}\\)), equivale a \\(P &gt; \\alpha\\) pré-estabelecido (bicaudal).169 16.7 Interpretação de análise inferencial 16.7.1 Como interpretar uma análise inferencial? Testes de hipótese nula (\\(H_{0}\\)) vs. alternativa (\\(H_{1}\\)) a partir de um nível de significância (\\(\\alpha\\)) pré-especificado.170 P-valor como evidência estatística sobre (\\(H_{0}\\)).170 Estimação de intervalos de confiança de um nível de significância (\\(\\alpha\\)) pré-especificado bicaudal (\\(IC_{1-\\alpha/2}\\)) ou unicaudal (\\(IC_{1-\\alpha}\\)).170 Análise Bayesiana.170 16.7.2 O que são resultados ‘positivos’ e ‘negativos’ ou inconclusivos em teste de hipótese? Resultados ‘positivos’ compreendem um P-valor dentro da zona crítica estatisticamente significativa (ex.: \\(P&lt;0,05\\) ou outro ponto de corte) e sugerem que os autores rejeitem a hipótese nula \\(H_{0}\\), confirmando assim sua hipótese científica.171 Resultados ‘negativos’ ou inconclusivos compreendem um P-valor fora da zona crítica estatisticamente significativa (ex.: \\(P \\geq 0,05\\) ou outro ponto de corte) e sugerem que os autores não rejeitem a hipótese nula \\(H_{0}\\) porque o efeito observado é nulo (logo, negativo), ou porque o estudo não possui poder suficiente para detectá-lo, não permitindo portanto afirmar a hipótese científica (logo, inconclusivo).171 16.7.3 Qual a importância de resultados ‘negativos’? Conhecer resultados negativos contribui com uma visão mais ampla do campo de estudo junto aos resultados positivos.172 Resultados negativos permitem um melhor planejamento das pesquisas futuras e pode aumentar suas chances de sucesso.172 16.7.4 Resultados inconclusivos: Ausência de evidência ou evidência de ausência? Em estudos (geralmente com amostras grandes), resultados estatisticamente significativos (com P-valores menores do limiar pré-estabelecido, \\(P&lt;\\alpha\\)) podem não ser clinicamente relevantes.173 Em estudos (geralmente com amostras pequenas), resultados estatisticamente não significativos (com P-valores iguais ou maiores do limiar pré-estabelecido, \\(P \\geq \\alpha\\)) não devem ser interpretados como evidência de inexistência do efeito.173 Geralmente é razoável aceitar uma nova conclusão apenas quando há dados a seu favor (‘resultados positivos’). Também é razoável questionar se apenas a ausência de dados a seu favor (‘resultados negativos’) justifica suficientemente a rejeição de tal conclusão.173 16.8 Erros de inferência 16.8.1 O que são erros de inferência estatística? Um erro de inferência é a tomada de decisão incorreta, seja a favor ou contra a hipótese nula \\(H_{0}\\).158 16.8.2 O que são erros Tipo I e Tipo II? Erro Tipo I significa a rejeição de uma hipótese nula (\\(H_{0}\\)) quando esta é verdadeira.158 Erro Tipo II significa a não rejeição de uma hipótese nula (\\(H_{0}\\)) quando esta é falsa.158 Tabela 16.1: Tabela de erros tipos I e II de inferência estatística. Hipótese nula \\(H_{0}\\) é falsa Hipótese nula \\(H_{0}\\) é verdadeira Hipótese nula \\(H_{0}\\) foi rejeitada Decisão correta Decisão incorreta (erro tipo I) Hipótese nula \\(H_{0}\\) não foi rejeitada Decisão incorreta (erro tipo II) Decisão correta 16.8.3 O que são erros Tipo S e Tipo M? Erro Tipo S (do inglês sign) significa a identificação errônea da direção - positiva ou negativa - do efeito observado.174,175 Tabela 16.2: Tabela de erro tipo S de inferência estatística. Sinal positivo Sinal negativo Sinal positivo Decisão correta Decisão incorreta (erro tipo S) Sinal negativo Decisão incorreta (erro tipo S) Decisão correta Erro Tipo M (do inglês magnitude) significa a identificação errônea - em geral, exagerada - da magnitude do efeito observado.174,175 Tabela 16.3: Tabela de erro tipo M de inferência estatística. Magnitude alta Magnitude baixa Magnitude alta Decisão correta Decisão incorreta (erro tipo M) Magnitude baixa Decisão incorreta (erro tipo M) Decisão correta Referências "],["análise-robusta.html", "Capítulo 17 Análise robusta 17.1 Raciocínio inferencial robusto", " Capítulo 17 Análise robusta 17.1 Raciocínio inferencial robusto 17.1.1 O que é análise robusta? Análise robusta é uma abordagem estatística que busca fornecer resultados confiáveis mesmo quando as suposições clássicas dos modelos estatísticos são violadas, como normalidade e homocedasticidade. Ela utiliza métodos que são menos sensíveis a outliers e outras irregularidades nos dados.176 17.1.2 Por que usar análise robusta? Métodos clássicos como ANOVA e regressão por mínimos quadrados assumem normalidade e homocedasticidade — suposições frequentemente violadas na prática. Violações dessas suposições podem comprometer os resultados, reduzindo o poder estatístico, distorcendo os intervalos de confiança e obscurecendo as reais diferenças entre grupos.176 Testar previamente as suposições não é suficiente: testes de homocedasticidade têm baixo poder e não garantem segurança analítica.176 Métodos estatísticos robustos oferecem uma solução mais segura e eficaz, lidando melhor com dados não ideais.176 17.1.3 Quando usar análise robusta? Em alguns casos, os métodos robustos confirmam os resultados clássicos; em outros, revelam interpretações completamente diferentes. A única forma de saber o impacto real dos métodos robustos é usá-los e comparar com os métodos tradicionais.176 17.1.4 O que é Winsorização? Winsorização é uma técnica que substitui os valores extremos (outliers) por valores menos extremos, preservando a estrutura dos dados. Isso é feito definindo limites superior e inferior e substituindo os valores que ultrapassam esses limites pelos próprios limites.176 O pacote WRS2177 fornece as funções winmean e winvar para calcular a média e variância Winsorizadas. O pacote WRS2177 fornece a função yuen para realizar o teste de comparação de Yuen de médias Winsorizadas para amostras independentes ou dependentes. O pacote WRS2177 fornece a função wincor para calcular a correlação Winsorizada. O pacote WRS2177 fornece as funções t1way, t2way e t3way para realizar testes de comparação de médias Winsorizadas para análise de variância para 1, 2 ou 3 fatores, respectivamente. Referências "],["análise-preditiva.html", "Capítulo 18 Análise preditiva 18.1 Predição via modelagem", " Capítulo 18 Análise preditiva 18.1 Predição via modelagem 18.1.1 O que são predições? .REF? O pacote ggeffects178 fornece a função predict_response para calcular valores preditos marginais ou ajustados das variáveis de desfecho. O pacote ggeffects178 fornece a função test_response para testar valores preditos marginais ou ajustados das variáveis de desfecho. Referências "],["análise-causal.html", "Capítulo 19 Análise causal 19.1 Causalidade", " Capítulo 19 Análise causal 19.1 Causalidade 19.1.1 O que é análise causal? Análise causal é usada para explicar a relação entre causa e efeito em um conjunto de dados, respondendo a perguntas do tipo “por quê?”.135 Análise causal implica em contrafactual, no sentido de que a análise causal é baseada na comparação entre o que realmente aconteceu e o que teria acontecido se uma ou mais variáveis tivessem sido diferentes.135 O pacote dagitty179 fornece a função dagitty para criar um objeto grafo a partir de uma descrição textual. O pacote ggdag180 fornece a função ggdag para criar figuras de grafos. O pacote performance181 fornece a função check_dag para criar, verificar e visualizar os modelos em grafos. Referências "],["tamanho-do-efeito-e-p-valor.html", "Capítulo 20 Tamanho do efeito e P-valor 20.1 Tamanho do efeito 20.2 Efeito fixo 20.3 Efeito aleatório 20.4 Efeito principal 20.5 Efeito de modificação 20.6 Efeito de interação 20.7 Efeito de mediação 20.8 Efeitos brutos e padronizados 20.9 P-valor 20.10 P-haking", " Capítulo 20 Tamanho do efeito e P-valor 20.1 Tamanho do efeito 20.1.1 O que é o tamanho do efeito? Tamanho do efeito quantifica a magnitude de um efeito real da análise, expressando uma importância descritiva dos resultados.182 20.1.2 Quais são os tipos de tamanho do efeito? Diferenças padronizadas entre grupos:163,182 Cohen’s d Glass’s \\(\\Delta\\) Razão de chances (\\(RC\\) ou \\(OR\\)) Risco relativo ou razão de risco (\\(RR\\)) Medidas de associação:163,182 Coeficiente de correlação de Pearson (\\(r\\)), ponto-bisserial (\\(r_{s}\\)), Spearman (\\(\\rho\\)), Kendall (\\(\\tau\\)), Cramér (\\(V\\)) e \\(\\phi\\). Coeficiente de determinação (\\(R^2\\)) 20.1.3 Como converter um tamanho de efeito em outro? .182 O pacote effectsize183 fornece diversas funções para conversão de diferentes estimativas de tamanhos de efeito. 20.1.4 Como interpretar um tamanho do efeito? Tamanhos de efeito podem ser comparadores entre diferentes estudos.163 O pacote effectsize183 fornece a função rules para criar regras de interpretação de tamanhos de efeito. O pacote effectsize183 fornece a função interpret para interpretar os tamanhos de efeito com base em uma lista de regras pré-definidas. O pacote pwr165 fornece a função cohen.ES para obter os tamanhos de efeito “pequeno”, “médio” e “grande” para diversos testes de hipóteses. 20.2 Efeito fixo 20.2.1 O que é efeito fixo? .REF? 20.3 Efeito aleatório 20.3.1 O que é efeito aleatório? .REF? 20.4 Efeito principal 20.4.1 O que é efeito principal? .184 20.5 Efeito de modificação 20.5.1 O que é um modificador de efeito? .184 20.5.2 O que é efeito de modificação? .184 20.6 Efeito de interação 20.6.1 O que é efeito de interação? A interação - representada pelo símbolo * - é o termo estatístico empregado para representar a heterogeneidade de um determinado efeito.185 .184 Figura 20.1: Análise de efeito de interação (direta) entre grupos e tempo. Retas paralelas sugerem ausência de efeito de interação. Figura 20.2: Análise de efeito de interação (inversa) entre grupos e tempo. Retas paralelas sugerem ausência de efeito de interação. O pacote nlme186 fornece a função nlme para ajustar um modelo de regressão misto não linear. O pacote mmrm187 fornece a função mmrm para ajuste de um modelo de regressão misto linear. O pacote emmeans188 fornece a função emmeans para calcular as médias marginais dos fatores e suas combinações de um modelo de regressão misto linear. 20.7 Efeito de mediação 20.7.1 O que é um mediador de efeito? .189 .184 20.7.2 O que é efeito de mediação? .189 .184 20.7.3 O que é efeito direto? .189 .184 20.7.4 O que é efeito indireto? .189 .184 20.7.5 O que é efeito total? .189 .184 20.8 Efeitos brutos e padronizados 20.8.1 O que é efeito bruto? .190 .191 20.8.2 O que é efeito padronizado? .190 .191 20.9 P-valor 20.9.1 O que é significância estatística? A expressão “significância estatística”192 ou “evidência estatística de significância” sugere apenas que um experimento merece ser repetido, uma vez que um baixo P-valor (calculado a partir dos dados, modelos e demais suposições do estudo) sugere ser improvável que os dados coletados sejam coletados no contexto de que a hipótese nula \\(H_{0}\\) assumida é verdadeira.193 20.9.2 Como justificar o nível de significância estatística de um teste? .REF? O pacote Superpower167 fornece a função optimal_alpha para calcular e justificar o nível de significância \\(\\alpha\\) por balanço dos erros tipo I e II. O pacote Superpower167 fornece a função ANOVA_compromise para calcular e justificar o nível de significância \\(\\alpha\\) por balanço dos erros tipo I e II em análise de variância (ANOVA). 20.9.3 O que é o P-valor? P-valor é a probabilidade, assumindo-se um dado modelo estatístico, de que um efeito calculado a partir dos dados seria igual ou mais extremo do que o seu valor observado.194 P-valor é uma variável aleatória que possui distribuição uniforme quando a hipótese nula \\(H_{0}\\) é verdadeira.195 20.9.4 Como interpretar o P-valor? P-valores abaixo de um nível de significância estatística pré-especificado representam que um experimento merece ser repetido, com a rejeição da hipótese nula \\(H_{0}\\)) justificada apenas quando experimentos adicionais frequentemente reportem igualmente resultados positivos (rejeição da hipótese nula \\(H_{0}\\)).170 P-valor resulta da coleta e análise de dados, e assim quantifica a plausibilidade dos dados observados sob a hipótese nula \\(H_{0}\\).196 P-valores podem indicar quantitativamente a incompatibilidade entre os dados obtidos e o modelo estatístico especificado a priori (geralmente constituído pela hipótese nula \\(H_{0}\\)).194 P-valores menores/maiores do que o nível de significância estatístico pré-estabelecido não devem ser utilizados como única fonte de informação para tomada de decisão em ciência.194 20.9.5 O que o P-valor não é? P-valor não representa a probabilidade de que a hipótese nula \\(H_{0}\\)) seja verdadeira, nem a probabilidade de que os dados tenham sido produzidos pelo acaso.194 P-valor não mede o tamanho do efeito ou a relevância da sua observação.194 P-valor sozinho não provê informação suficiente sobre a evidência sobre um modelo teórico. A sua interpretação correta requer uma descrição ampla sobre o delineamento, métodos e análises estatísticas aplicados no estudo.194 Evidência estatística de significância não provê informação sobre a magnitude do efeito observado e não necessariamente implica que o efeito é robusto.127,195 20.9.6 Qual a origem do ‘P&lt;0,05’? .REF? 20.9.7 Quais são os complementos ou alternativas ao P-valor? Intervalos de confiança, credibilidade ou predição.194 Razão de verossimilhança.194 Métodos Bayesianos, fator Bayes.194 20.10 P-haking 20.10.1 O que é P-haking? .REF? Referências "],["parte-3---estatística-aplicada.html", "Parte 3 - Estatística Aplicada", " Parte 3 - Estatística Aplicada "],["computação-estatística.html", "Capítulo 21 Computação estatística 21.1 Programas de computador 21.2 Scripts computacionais 21.3 Pacotes 21.4 Manuscritos reproduzíveis 21.5 Compartilhamento", " Capítulo 21 Computação estatística 21.1 Programas de computador 21.1.1 O que é R? R é um programa de computador de código aberto com linguagem computacional direcionada para análise estatística.197,198 R version 4.5.0 (2025-04-11) está disponível gratuitamente em Comprehensive R Archive Network (CRAN).199 21.1.2 Por que usar R? R é o software de maior abrangência de métodos estatísticos, possui sintaxe que permite análises estatísticas reproduzíveis e está disponível gratuitamente no Comprehensive R Archive Network (CRAN).23,199 21.1.3 O que é RStudio? RStudio é um ambiente de desenvolvimento integrado (integrated development environment, IDE) desenvolvido visando a reprodutibilidade e a simplicidade para a criação e disseminação de conhecimento.198,200 O ambiente do RStudio é dividido em paineis: Source/Script editor: para edição de R scripts.198 Console: para execução de códigos simples, .198 Environments: para visualização de objetos criados durante a sessão de trabalho.198 Output: para visualização de gráficos criados durante a sessão de trabalho.198 Figura 21.1: Interface do RStudio. Fonte: https://docs.posit.co/ide/user/ As principais características do RStudio incluem um ambiente de edição com abas para acesso rápido a arquivos, comandos e resultados; histórico de comandos previamente utilizados; ferramentas para visualização de bancos de dados e elaboração de scripts e gráficos e tabelas.198,200 RStudio está disponível gratuitamente em Posit. O pacote learnr201 fornece tutoriais interativos para RStudio. 21.1.4 Que programas de computador podem ser usados para análise estatística com R? JASP.202 jamovi.203 BlueSky. Os pacotes jmv204 e jmvconnect205 fornecem funções para análise descritiva e inferencial com interface com jamovi. 21.2 Scripts computacionais 21.2.1 O que são R scripts? “Scripts são dados”.206 Scripts permitem ao usuário se concentrar nas tarefas mais importantes da computação e utilizar pacotes ou bibliotecas para executar as funções mais básicas com maior eficiência.206 Um script é um arquivo de texto contendo (quase) os mesmos comandos que você digitaria na linha de comando do R. O “quase” refere-se ao fato de que se você estiver usando sink() para enviar a saída para um arquivo, você terá que incluir alguns comandos em print() para obter a mesma saída da linha de comando. 21.3 Pacotes 21.3.1 O que são pacotes? Pacotes são conjuntos de scripts programados pela comunidade e compartilhados para uso público.198 Os pacotes ficam armazenados no Comprehensive R Archive Network (CRAN) e podem ser instalados diretamente no RStudio.198,199 Na mais recente atualização deste livro, o [Comprehensive R Archive Network (CRAN) possui 382517 pacotes disponíveis.198,199 Os pacotes disponíveis podem ser encontrados em R PACKAGES DOCUMENTATION.207 O pacote utils208 fornece a função install.packages para instalar os pacotes no computador. O pacote utils208 fornece a função library para carregar os pacotes instalados no computador. O pacote utils208 fornece a função require para indicar se o pacote requisitado está disponível. O pacote utils208 fornece a função installed.packages para listar os pacotes instalados no computador. O pacote utils208 fornece a função update.packages para atualizar os pacotes instalados no computador. 21.3.2 Quais práticas são recomendadas na redação de scripts? Use nomes consistentes para as variáveis.209 Defina os tipos de variáveis adequadamente no banco de dados.209 Defina constantes - isto é, variáveis de valor fixo - ao invés de digitar valores.209 Use e cite os pacotes disponíveis para suas análises.209 Controle as versões do script.209,210 Teste o script antes de sua utilização.209 Conduza revisão por pares do código durante a redação (digitação em dupla).209 O pacote formatR211 fornece a função tidy_source para formatar um R script. O pacote styler212 fornece a função style_file para formatar um R script. O pacote lintr213 fornece a função lint para verificar a adesão de um script a um determinado estilo, identificando erros de sintaxe e possíveis problemas semânticos. 21.4 Manuscritos reproduzíveis 21.4.1 O que são manuscritos reproduzíveis? Manuscritos reproduzíveis - manuscritos executáveis ou relatórios dinâmicos - permitem a produção de um manuscrito completo a partir da integração do banco de dados da(s) amostra(s), do(s) script(s) de análise estatística (incluindo comentários para sua interpretação), dos pacotes ou bibliotecas utilizados, das fontes e referências bibliográficas citadas, além dos demais elementos textuais (tabelas, gráficos) - todos gerados dinamicamente.206 21.4.2 Por que usar manuscritos reproduzíveis? No processo tradicional de redação científica há muitas etapas de copiar e colar não reproduzíveis envolvidas. Documentos dinâmicos combinam uma ferramenta de processamento de texto com o R script que produz o texto/tabela/figura a ser incorporado no manuscrito.23 Ao trabalhar com relatórios dinâmicos, é possível extrair o mesmo script usado para análise estatística. Os documentos podem ser compilados em vários formatos de saída e salvos como DOCX, PPTX e PDF.23 Muitos erros de análise poderiam ser evitados com a adoção de boas práticas de programação em manuscritos reproduzíveis.214 O pacote rmarkdown215 fornece as funções render para criar manuscritos reprodutíveis a partir de arquivos .Rmd. O pacote officedown216 fornece as funções rdocx_document e rpptx_document para criar arquivos DOCX e PPTX, respectivamente, com o conteúdo criado no manuscrito reprodutível. O pacote bookdown217 fornece as funções gitbook, pdf_book, epub_book e html_document2 para criar documentos reprodutíveis em diversos formatos (Git, PDF, EPUB e HTML, respectivamente). 21.4.3 O que é RMarkdown? RMarkdown215 é uma ferramenta que permite a integração de texto, código e saída em um único documento. O RMarkdown é uma extensão do Markdown, que é uma linguagem de marcação simples e fácil de aprender, que é usada para formatar texto. O RMarkdown permite a inclusão de blocos de código R, Python, SQL, C++, entre outros, e a saída desses blocos de código é incorporada ao documento final. O RMarkdown é uma ferramenta poderosa para a criação de relatórios dinâmicos, que podem ser facilmente atualizados com novos dados ou análises. O RMarkdown é amplamente utilizado na comunidade científica para a criação de relatórios de pesquisa, artigos científicos, apresentações, livros, entre outros. O trabalho com RMarkdown215 permite um fluxo de dados totalmente transparente, desde o conjunto de dados coletados até o manuscrito finalizado. Todos os aspectos do fluxo de dados podem ser incorporados em blocos de R script (chunk), exibindo tanto o R script quando o respectivo texto, tabelas e figuras formatadas no estilo científico de interesse.218 O RMarkdown215 foi projetado especificamente para relatórios dinâmicos onde a análise é realizada em R e oferece uma flexibilidade incrível por meio de uma linguagem de marcação.23 21.4.4 Como manuscritos reprodutíveis contribuem para a ciência? O compartilhamento de bancos de dados e seus scripts de análise estatística permitem a adoção de práticas reprodutíveis, tais como a reanálise dos dados.219 O pacote projects220 fornece a função setup_projects para criar um projeto com arquivos organizados em diretórios. 21.4.5 Como contribuir para a reprodutibilidade? Disponibilize publicamente os bancos de dados, respeitando as considerações éticas vigentes (ex.: autorização dos participantes e do Comitê de Ética em Pesquisa) e internacionalmente.23 Produza manuscritos reprodutíveis - manuscritos executáveis ou relatórios dinâmicos - que permitem a integração do banco de dados da(s) amostra(s), do(s) script(s) de análise estatística (incluindo comentários para sua interpretação), dos pacotes ou bibliotecas utilizados, das fontes e referências bibliográficas citadas, além dos demais elementos textuais (tabelas, gráficos) - todos gerados dinamicamente.206 O pacote rmarkdown215 fornece a função render para criar manuscritos reprodutíveis a partir de arquivos .Rmd. O pacote bookdown217 fornece as funções gitbook, pdf_book, epub_book e html_document2 para criar documentos reprodutíveis em diversos formatos (Git, PDF, EPUB e HTML, respectivamente). 21.5 Compartilhamento 21.5.1 Por que compartilhar scripts? Compartilhar o script — principalmente junto aos dados — pode facilitar a replicação direta do estudo, a detecção de eventuais erros de análise, a detecção de pesquisas fraudulentas.221 21.5.2 O que pode ser compartilhado? Idealmente, todos os scripts, pacotes/bibliotecas e dados necessários para outros reproduzirem seus dados.210 Minimamente, partes importantes incluindo implementações de novos algoritmos e dados que permitam reproduzir um resultado importante.210 21.5.3 Como preparar dados para compartilhamento? .REF? 21.5.4 Como preparar scripts para compartilhamento? Providencie a documentação sobre seu script (ex.: arquivo README).210 Inclua a versão dos pacotes usados no seu script por meio de um script inicial para instalação de pacotes (ex.: ‘instalar.R’).214 Documente em um arquivo README os arquivos disponíveis e os pré-requisitos necessários para executar o código (ex.: pacotes e respectivas versões). Uma lista de configurações (hardware e software) que foram usadas para rodar o código pode ajudar na reprodução dos resultados.22 Use endereços de arquivos relativos.214 Crie links persistentes para versões do seu script.210 Defina uma semente para o gerador de números aleatórios em scripts com métodos computacionais que dependem da geração de números pseudoaleatórios.22 O pacote base52 fornece a função set.seed para especificar uma semente para reprodutibilidade de computações que envolvem números aleatórios. Escolha uma licença apropriada para garantir os direitos de criação e como outros poderão usar seus scripts.210 Teste o script em uma nova sessão antes de compartilhar.214 Cite todos os pacotes relacionados à sua análise.222 O pacote utils208 fornece a função citation para citar o programa R e os pacotes da sessão atual. O pacote grateful223 fornece a função cite_packages para citar os pacotes utilizados em um projeto R. Inclua a informação da sessão em que os scripts foram rodados.214 O pacote utils208 fornece a função sessionInfo para descrever as características do programa, pacotes e plataforma da sessão atual. 21.5.5 O que incluir no arquivo README? Título do trabalho.22 Autores do trabalho.22 Principais responsáveis pela escrita do script e quaisquer outras pessoas que fizeram contribuições substanciais para o desenvolvimento do script.22 Endereço de e-mail do autor ou contribuidor a quem devem ser direcionadas dúvidas, comentários, sugestões e bugs sobre o script.22 Lista de configurações nas quais o script foi testado, tais com nome e versão do programa, pacotes e plataforma.22 Referências "],["seleção-de-testes.html", "Capítulo 22 Seleção de testes 22.1 Multiverso de análises estatísticas 22.2 Escolha de testes para análise inferencial", " Capítulo 22 Seleção de testes 22.1 Multiverso de análises estatísticas 22.1.1 Por que escolher o teste é um problema? Analisar a mesma hipótese com o mesmo banco de dados pode resultar em diferenças substanciais nas estimativas estatísticas e nas conclusões.224 As decisões para especificação das análises estatísticas podem ser tão minuciosas que muitas vezes nem sequer são registadas como decisões e, assim, podem impactar na reprodutibilidade do estudo.224 22.2 Escolha de testes para análise inferencial 22.2.1 Como selecionar os testes para a análise estatística inferencial? .225 .226 .227 .228 .229 .230 .231 .232 Referências "],["testes-estatísticos.html", "Capítulo 23 Testes estatísticos 23.1 Testes de Qui-quadrado (\\(\\chi^2\\)) 23.2 Teste exato de Fisher", " Capítulo 23 Testes estatísticos 23.1 Testes de Qui-quadrado (\\(\\chi^2\\)) Code # carrega os pacotes library(&quot;dplyr&quot;) library(&quot;gtsummary&quot;) # tabela 2x2 tbl_cross &lt;- # banco de dados trial %&gt;% # cria a tabela de contingência gtsummary::tbl_cross( row = trt, col = response, statistic = &quot;{n}&quot;, digits = 0, percent = &quot;cell&quot;, margin = c(&quot;row&quot;, &quot;column&quot;), missing = &quot;no&quot;, missing_text = &quot;Dados perdidos&quot;, margin_text = &quot;Total&quot; ) %&gt;% # calcula o p-valor do teste gtsummary::add_p( test = &quot;chisq.test&quot;, pvalue_fun = function(x) style_pvalue(x, digits = 3) ) %&gt;% gtsummary::modify_header( p.value = &quot;**P-valor**&quot; ) %&gt;% # calcula o tamanho do efeito gtsummary::modify_table_styling( rows = NULL, footnote = as.character(rstatix::cramer_v(trt, response)) ) %&gt;% # formata o título em negrito gtsummary::bold_labels() %&gt;% # cria título da tabela gtsummary::modify_caption( &quot;Teste Qui-quadrado (com correção de Yates)&quot; ) # exibe a tabela require(huxtable) tbl_cross %&gt;% gtsummary::as_hux_table() Teste Qui-quadrado (com correção de Yates) Tumor Response 0 1 Total P-valor Chemotherapy Treatment Drug A672895 Drug B653398 Total13261193 Code # carrega os pacotes library(&quot;dplyr&quot;) library(&quot;gtsummary&quot;) # tabela 2x2 tbl_cross &lt;- # banco de dados trial %&gt;% # cria a tabela de contingência gtsummary::tbl_cross( row = trt, col = response, statistic = &quot;{n}&quot;, digits = 0, percent = &quot;cell&quot;, margin = c(&quot;row&quot;, &quot;column&quot;), missing = &quot;no&quot;, missing_text = &quot;Dados perdidos&quot;, margin_text = &quot;Total&quot; ) %&gt;% # calcula o p-valor do teste gtsummary::add_p( test = &quot;chisq.test.no.correct&quot;, pvalue_fun = function(x) style_pvalue(x, digits = 3) ) %&gt;% gtsummary::modify_header( p.value = &quot;**P-valor**&quot; ) %&gt;% # calcula o tamanho do efeito gtsummary::modify_table_styling( rows = NULL, footnote = as.character(rstatix::cramer_v(trt, response)) ) %&gt;% # formata o título em negrito gtsummary::bold_labels() %&gt;% # cria título da tabela gtsummary::modify_caption( &quot;Teste Qui-quadrado (sem correção de Yates)&quot; ) # exibe a tabela require(huxtable) tbl_cross %&gt;% gtsummary::as_hux_table() Teste Qui-quadrado (sem correção de Yates) Tumor Response 0 1 Total P-valor Chemotherapy Treatment Drug A672895 Drug B653398 Total13261193 23.2 Teste exato de Fisher Code # carrega os pacotes library(&quot;dplyr&quot;) library(&quot;gtsummary&quot;) # tabela 2x2 tbl_cross &lt;- # banco de dados trial %&gt;% # cria a tabela de contingência gtsummary::tbl_cross( row = trt, col = response, statistic = &quot;{n}&quot;, digits = 0, percent = &quot;cell&quot;, margin = c(&quot;row&quot;, &quot;column&quot;), missing = &quot;no&quot;, missing_text = &quot;Dados perdidos&quot;, margin_text = &quot;Total&quot; ) %&gt;% # calcula o p-valor do teste gtsummary::add_p( test = &quot;fisher.test&quot;, pvalue_fun = function(x) style_pvalue(x, digits = 3) ) %&gt;% gtsummary::modify_header( p.value = &quot;**P-valor**&quot; ) %&gt;% # calcula o tamanho do efeito gtsummary::modify_table_styling( rows = NULL, footnote = as.character(rstatix::cramer_v(trt, response)) ) %&gt;% # formata o título em negrito gtsummary::bold_labels() %&gt;% # cria título da tabela gtsummary::modify_caption( &quot;Teste exato de Fisher&quot; ) # exibe a tabela require(huxtable) tbl_cross %&gt;% gtsummary::as_hux_table() Teste exato de Fisher Tumor Response 0 1 Total P-valor Chemotherapy Treatment Drug A672895 Drug B653398 Total13261193 "],["plano-de-análise.html", "Capítulo 24 Plano de análise 24.1 Plano de análise estatística", " Capítulo 24 Plano de análise 24.1 Plano de análise estatística 24.1.1 O que é plano de análise estatística? .REF? "],["descrição.html", "Capítulo 25 Descrição 25.1 Análise de descrição 25.2 Estimação", " Capítulo 25 Descrição 25.1 Análise de descrição 25.1.1 O que é análise de descrição de dados? A análise descritiva utiliza métodos para calcular, descrever e resumir os dados coletados da(s) amostra(s) de modo que sejam interpretadas adequadamente.49 As análises descritivas geralmente compreendem a apresentação quantitativa (numérica) em tabelas e/ou gráficos.49 O pacote explore129 fornece a função explore para análise exploratória de um banco de dados. O pacote dataMaid130 fornece a função makeDataReport para criar um relatório de análise exploratória de um banco de dados. O pacote DataExplorer131 fornece a função create_report para criar um relatório de análise exploratória de um banco de dados. O pacote SmartEDA132 fornece a função ExpReport para criar um relatório de análise exploratória de um banco de dados. O pacote esquisse233 fornece a função esquisser para executar uma interface interativa para visualização de dados. 25.2 Estimação 25.2.1 O que é estimativa? Estimativa é o valor de uma variável de interesse calculado a partir de uma amostra.REF? 25.2.2 O que é estimativa pontual? Estimativa pontual é o valor único de uma variável de interesse calculado a partir de uma amostra.REF? 25.2.3 O que é estimativa intervalar? Estimativa intervalar é um intervalo de valores de uma variável de interesse calculado a partir de uma amostra.REF? 25.2.4 O que é estimativa de parâmetro? Estimativa de parâmetro é o valor de uma variável de interesse calculado a partir de uma amostra que representa o valor da população.REF? Referências "],["comparação.html", "Capítulo 26 Comparação 26.1 Análise inferencial de comparação", " Capítulo 26 Comparação 26.1 Análise inferencial de comparação 26.1.1 O que é análise de comparação de dados? .REF? O pacote cocor234 fornece as funções cocor.indep.groups, cocor.dep.groups.overlap e cocor.dep.groups.nonoverlap para comparar 2 coeficientes de correlação entre grupos independentes, grupos sobrepostos ou independentes, respectivamente.234 Referências "],["correlação.html", "Capítulo 27 Correlação 27.1 Análise inferencial de correlação 27.2 Coeficientes de correlação", " Capítulo 27 Correlação 27.1 Análise inferencial de correlação 27.1.1 O que é covariância? .REF? 27.1.2 O que é correlação? .REF? 27.1.3 Qual é a interpretação das medidas de correlação? Os valores de correlação estão no intervalo \\([-1; 1]\\).82,235,236 Valores de correlação positivos representam uma relação direta entre as variáveis, tal que valores maiores de uma variável estão associados a valores maiores de outra variável.235,236 Valores de correlação negativos representam uma relação indireta (ou inversa) entre as variáveis, tal que valores maiores (menores) de uma variável estão associados a valores maiores (menores) de outra variável.235,236 Valores de correlação próximos de \\(0\\) representam a inexistência de relação entre as variáveis.235,236 Figura 27.1: Exemplo de diferentes forças e direção de correlação entre duas variáveis X e Y. 27.1.4 Quais precauções devem ser tomadas na interpretação de medidas de correlação? Tamanhos de efeito grande (ou qualquer outro) não representam necessariamente uma relação causa-efeito entre as variáveis.235 Tamanhos de efeito grande (ou qualquer outro) não representam necessariamente uma relação de concordância ou confiabilidade entre as variáveis.235 Uma escala de medição com representação agregada do constructo na coleta de dados pode subestimar o tamanho do efeito da correlação \\(r\\) em de cerca de 13% e do coeficiente de determinação \\(R^2\\) de cerca de 30%.41 Neste caso, a correlação desatenuada \\(r_{x&#39;y&#39;}\\) pode ser calculada pela equação (27.1), utilizando a correlação observada \\(r_{xy}\\) e os fatores de correção \\(r_{xx&#39;}\\) e \\(r_{yy&#39;}\\) para o número de intervalos nas variáveis X e Y, respectivamennte:41 \\[\\begin{equation} \\tag{27.1} r_{x&#39;y&#39;} = \\dfrac{r_{xy}}{r_{xx&#39;}r_{yy&#39;}} \\end{equation}\\] O pacote psychmeta237 fornece a função correct_r_coarseness para calcular o coeficiente de correlação desatenuado (\\(r_{x&#39;y&#39;}\\)). O pacote psychmeta237 fornece a função correct_r para calcular o coeficiente de correlação em escala restrita e/ou com erro de mensuração (\\(r_{x&#39;y&#39;}\\)). Os coeficientes de correlação possuem suposições que, se violadas, podem levar a interpretações equivocadas. Nestes cenários, visualizar os dados e as relações entre as variáveis pode contribuir com a interpretação e utilidade dos coeficientes de correlação.238 O quarteto de Anscombe é um conjunto de quatro bancos de dados bivariados que possuem a mesma média, variância, correlação e regressão linear (até a 2a casa decimal), mas que são visualmente diferentes e, assim, demonstram a importância da análise gráfica da correlação.238 Tabela 27.1: Quarteto de Anscombe. ID x1 x2 x3 x4 y1 y2 y3 y4 1 10 10 10 8 8.04 9.14 7.46 6.58 2 8 8 8 8 6.95 8.14 6.77 5.76 3 13 13 13 8 7.58 8.74 12.74 7.71 4 9 9 9 8 8.81 8.77 7.11 8.84 5 11 11 11 8 8.33 9.26 7.81 8.47 6 14 14 14 8 9.96 8.10 8.84 7.04 7 6 6 6 8 7.24 6.13 6.08 5.25 8 4 4 4 19 4.26 3.10 5.39 12.50 9 12 12 12 8 10.84 9.13 8.15 5.56 10 7 7 7 8 4.82 7.26 6.42 7.91 11 5 5 5 8 5.68 4.74 5.73 6.89 Tabela 27.1: Análise descritiva do Quarteto de Anscombe demostrando os conjuntos de dados bivariados com parâmetros quase idênticos. X1Y1 X2Y2 X3Y3 X4Y4 Observações 11.00 11.00 11.00 11.00 Média x 9.00 9.00 9.00 9.00 Média y 7.50 7.50 7.50 7.50 Variância x 11.00 11.00 11.00 11.00 Variância y 4.13 4.13 4.12 4.12 Correlação 0.82 0.82 0.82 0.82 Coeficiente angular 0.50 0.50 0.50 0.50 Coeficiente linear 3.00 3.00 3.00 3.00 Coeficiente de determinação 0.67 0.67 0.67 0.67 Figura 27.2: Gráfico de dispersão do Quarteto de Anscombe para representação gráfica de conjuntos de dados bivariados com parâmetros quase idênticos e relações muito distintas. O pacote anscombiser239 fornece a função anscombise para gerar bancos de dados que compartilham os mesmos valores de parâmetros do Quarteto de Anscombe. 27.2 Coeficientes de correlação 27.2.1 Quais coeficientes podem ser usados em análises de correlação? Coeficiente de correlação de Pearson (\\(r\\)).235,236 O coeficiente de correlação de Pearson (\\(r\\)) avalia a força e direção da relação linear entre duas variáveis quantitativas.235,236 Tipo: paramétrico.235,236 Hipóteses:236 Nula (\\(H_{0}\\)): \\(r=0\\) Alternativa (\\(H_{1}\\)): \\(r≠0\\) Tamanho do efeito:235,236 Coeficiente de correlação de Pearson (\\(r\\)) O pacote stats44 fornece a função cor.test para calcular o coeficiente de correlação de Pearson (\\(r\\)). O pacote correlation240 do projeto easystats241 fornece a função correlation para calcular o coeficiente de correlação de Pearson (\\(r\\)). Coeficiente de correlação ponto-bisserial (\\(r_{s}\\)).235 O coeficiente de correlação ponto-bisserial (\\(r_{s}\\)) avalia a força e direção da relação linear entre uma variável quantitativa e outra dicotômica.235 Tipo: paramétrico.235 Hipóteses:235 Nula (\\(H_{0}\\)): \\(r_{s}=0\\) Alternativa (\\(H_{1}\\)): \\(r_{s}≠0\\) Tamanho do efeito:235 Coeficiente de correlação ponto-bisserial (\\(r_{s}\\)) O pacote stats44 fornece a função cor.test para calcular o coeficiente de correlação ponto-bisserial (\\(r_{s}\\)). O pacote correlation240 do projeto easystats241 fornece a função correlation para calcular o coeficiente de correlação ponto-bisserial (\\(r_{s}\\)). Coeficiente de correlação de Spearman (\\(\\rho\\)).235,236 O coeficiente de correlação de Spearman (\\(\\rho\\)) avalia a força e direção da relação monotônica entre duas variáveis quantitativas.235,236 O coeficiente de correlação de Spearman (\\(\\rho\\)) pode ser também definida como a correlação de Pearson (\\(r\\)) entre as classificações (ranks) das duas variáveis quantitativas.235,236 Tipo: não-paramétrico.235,236 Hipóteses:235,236 Nula (\\(H_{0}\\)): \\(\\rho=0\\) Alternativa (\\(H_{1}\\)): \\(\\rho≠0\\) Tamanho do efeito:235,236 Coeficiente de correlação de Spearman (\\(\\rho\\)) O pacote stats44 fornece a função cor.test para calcular o coeficiente de correlação de Spearman (\\(\\rho\\)). O pacote correlation240 do projeto easystats241 fornece a função correlation para calcular o coeficiente de correlação de Spearman (\\(\\rho\\)). Coeficiente de Kendall (\\(\\tau\\)).235,236 O coeficiente Kendall \\(\\tau\\) avalia a força e direção da relação monotônica entre duas variáveis quantitativas ou qualitativas.235,236 O coeficiente Kendall \\(\\tau\\) é definido como a proporção de todos os pares concordantes menos a proporção de todos os pares discordantes.235,236 Tipo: não-paramétrico.235,236 Hipóteses:235,236 Nula (\\(H_{0}\\)): \\(\\tau=0\\) Alternativa (\\(H_{1}\\)): \\(\\tau≠0\\) Tamanho do efeito:235,236 Kendall \\(\\tau\\) O pacote stats44 fornece a função cor.test para calcular o coeficiente Kendall \\(\\tau\\). O pacote correlation240 do projeto easystats241 fornece a função correlation para calcular o coeficiente coeficiente Kendall \\(\\tau\\). Coeficiente de Cramér (\\(V\\)).REF? O coeficiente Cramér (\\(V\\)) avalia a força e direção da relação entre duas variáveis qualitativas.REF? Tipo: não-paramétrico.REF? Hipóteses:REF? Nula (\\(H_{0}\\)): \\(V=0\\) Alternativa (\\(H_{1}\\)): \\(V≠0\\) Tamanho do efeito:REF? Coeficiente Cramer (\\(V\\)) Coeficiente de Sheperd \\(\\phi\\).REF? O coeficiente Phi (\\(\\phi\\)) avalia a força e direção da relação entre duas variáveis dicotômicas.REF? Tipo: não-paramétrico.REF? Hipóteses:REF? Nula (\\(H_{0}\\)): \\(\\phi=0\\) Alternativa (\\(H_{1}\\)): \\(\\phi≠0\\) Tamanho do efeito:REF? Coeficiente Phi (\\(\\phi\\)) O pacote correlation240 do projeto easystats241 fornece a função correlation para calcular o coeficiente coeficiente Sheperd \\(\\phi\\). O pacote corrplot154 fornece a função cor.mtest para calcular os P-valores e intervalos de confiança da matriz de correlação. O pacote corrplot154 fornece a função corrplot para visualização da matriz de correlação. Referências "],["redes.html", "Capítulo 28 Redes 28.1 Análise de redes", " Capítulo 28 Redes 28.1 Análise de redes 28.1.1 O que é análise de rede? .REF? O pacote cooccur242 fornece a função cooccur para criar calcular a coocorrência de objetos em um banco de dados. Referências "],["associação.html", "Capítulo 29 Associação 29.1 Análise inferencial de associação 29.2 Associação bivariada 29.3 Associação multivariada", " Capítulo 29 Associação 29.1 Análise inferencial de associação 29.1.1 O que é análise de associação? .REF? 29.2 Associação bivariada 29.2.1 O que são análises de associação bivariada? .REF? 29.2.2 Quais testes podem ser usados para análises de associação bivariada? Teste Qui-quadrado (\\(\\chi^2\\)).243,244 O teste qui-quadrado (\\(\\chi^2\\)) avalia uma hipótese global se a relação entre duas variáveis e/ou fatores é independente ou associada.244 O teste qui-quadrado é utilizado para comparar a distribuição de uma variável categórica em uma amostra ou grupo com a distribuição em outro. Se a distribuição da variável categórica não for muito diferente nos diferentes grupos, pode-se concluir que a distribuição da variável categórica não está relacionada com a variável dos grupos. Pode-se também concluir que a variável categórica e os grupos são independentes.244 Tipo: não paramétrico.243,244 Suposições:243,244 As variáveis são ordinais ou categóricas nominais, de modo que as células representem frequência. Os níveis dos fatores (variáveis categóricas) são mutuamente exclusivos. Tamanho de amostra grande e adequado porque é baseado em uma abordagem de aproximação. Menos de 20% das células com frequências esperadas &lt; 5 Nenhuma célula com frequência esperada &lt; 1. Hipóteses:244 Nula (\\(H_{0}\\)): independente (sem associação) Alternativa (\\(H_{1}\\)): não independente (associação) Tamanho do efeito:244 Phi (\\(\\phi\\)), para tabelas de contingência 2x2 Razão de chances (\\(RC\\) ou \\(OR\\)), para tabelas de contingência 2x2 Cramer V (\\(V\\)), para tabelas de contingência NxM O pacote gtsummary145 fornece a função tbl_cross para criar uma tabela NxM. Teste Exato de Fisher (\\(\\chi^2\\)).243,244 O teste exato de Fisher avalia a hipótese nula de independência aplicando a distribuição hipergeométrica dos números nas células da tabela.244 Hipóteses:243,244 Nula (\\(H_{0}\\)): independente (sem associação) Alternativa (\\(H_{1}\\)): não independente (associação) Tamanho do efeito:243,244 Phi (\\(\\phi\\)), para tabelas de contingência 2x2 Razão de chances (\\(RC\\) ou \\(OR\\)), para tabelas de contingência 2x2 Cramer V (\\(V\\)), para tabelas de contingência NxM O pacote gtsummary145 fornece a função tbl_cross para criar uma tabela NxM. 29.3 Associação multivariada 29.3.1 O que são análises de associação multivariada? .REF? 29.3.2 Quais testes podem ser usados para análises de associação multivariada? .REF? Referências "],["modelos.html", "Capítulo 30 Modelos 30.1 Modelos estatísticos 30.2 Suposições dos modelos 30.3 Avaliação de modelos 30.4 Comparação de modelos", " Capítulo 30 Modelos 30.1 Modelos estatísticos 30.1.1 O que são modelos? Modelos estatísticos são representações matemáticas de fenômenos naturais ou artificiais que descrevem a relação entre variáveis.REF? O pacote equatiomatic245 fornece a função extract_eq para extrair a equação dos modelos em formato LaTeX para visualização. 30.2 Suposições dos modelos 30.2.1 Quais suposições são feitas para modelagem? .REF? 30.2.2 Como avaliar as suposições de um modelo? .REF? O pacote performance181 fornece a função check_model para analisar a colinearidade entre variáveis, a normalidade da distribuição das variáveis e a heteroscedasticidade. 30.3 Avaliação de modelos 30.3.1 O que é qualidade de ajuste de um modelo? .REF? 30.3.2 Como avaliar a qualidade de ajuste de um modelo? .REF? O pacote performance181 fornece a função model_performance para calcular as métricas de ajuste da regressão adequadas ao modelo pré-especificado. O pacote performance181 fornece a função compare_performance para comparar o desempenho e a qualidade do ajuste de diversos modelos de regressão pré-especificados. 30.4 Comparação de modelos 30.4.1 Como comparar modelos de aprendizagem de máquina? .REF? O pacote correctR246 fornece funções para comparar o desempenho e a qualidade do ajuste de diversos modelos de aprendizagem de máquina em amostras correlacionadas. Referências "],["regressão.html", "Capítulo 31 Regressão 31.1 Análise de regressão 31.2 Preparação de variáveis para regressão 31.3 Redução de dimensionalidade para regressão", " Capítulo 31 Regressão 31.1 Análise de regressão 31.1.1 O que é regressão? Regressão refere-se a uma equação matemática que permite que uma ou mais variável(is) de desfecho (dependentes) seja(m) prevista(s) a partir de uma ou mais variável(is) independente(s). A regressão implica em uma direção de efeito, mas não garante causalidade.171 Para estimar os efeitos imparciais de um fator de exposição primária sobre uma variável de desfecho, frequentemente constroem-se modelos estatísticos de regressão.150 O pacote modelsummary247 fornece as funções modelsummary e modelplot para gerar tabelas e gráficos de coeficientes de regressão. O pacote gtsummary145 fornece a função tbl_regression para construção da ‘Tabela 2’ com dados do modelo de regressão. 31.1.2 Quais são os algoritmos de regressão? Linear.REF? Polinomial.REF? Ridge.REF? Lasso.REF? 31.1.3 O que são análises de regressão simples? A análise de regressão simples consiste em modelos estatísticos com 1 variável dependente (desfecho) e 1 variável independente (preditor).248 31.1.4 O que são análises de regressão multivariável? A análise multivariável (ou múltiplo) consiste em modelos estatísticos com 1 variável dependente (desfecho) e duas ou mais variáveis independentes.248 31.1.5 O que são análises de regressão multivariada? A análise multivariada consiste em modelos estatísticos com 2 ou mais variáveis dependente (desfechos) e duas ou mais variáveis independentes.248 31.1.6 O que são análises de regressão linear? .REF? 31.1.7 O que são análises de regressão não-linear? .REF? 31.1.8 O que são análises de regressão logística? .REF? 31.2 Preparação de variáveis para regressão 31.2.1 Como preparar as variáveis categóricas para análise de regressão? Variáveis fictícias (dummy) compreendem variáveis criadas para introduzir, nos modelos de regressão, informações contidas em outras variáveis que não podem ser medidas em escala numérica.249 Variáveis categóricas nominais, com 2 ou mais níveis, devem ser subdivididas em variáveis fictícias dicotômicas para ser usada em modelos de regressão.250 Cada nível da variável categórica nominal será convertido em uma nova variável fictícias dicotômica, tal que a nova variável dicotômica assume valor 1 para a presença do nível correspondente e 0 em qualquer outro caso.250 O pacote fastDummies251 fornece a função dummy_cols para preparar as variáveis categóricas fictícias para análise de regressão. 31.3 Redução de dimensionalidade para regressão 31.3.1 Correlação bivariada pode ser usada para seleção de variáveis em modelos de regressão multivariável? Seleção bivariada de variáveis - isto é, aplicação de testes de correlação em pares de variáveis candidatas e variável de desfecho afim de selecionar quais serão incluídas no modelo multivariável - é um dos erros mais comuns na literatura.196,252,253 A seleção bivariada de variáveis torna o modelo mais suscetível a otimismo no ajuste se as variáveis de confundimento não são adequadamente controladas.252,253 31.3.2 Variáveis sem significância estatística devem ser excluídas do modelo final? Eliminar uma variável de um modelo significa anular o seu coeficiente de regressão (\\(\\beta = 0\\)), mesmo que o valor estimado pelos dados seja outro. Desta forma, os resultados se afasTAM de uma solução de máxima verossimilhança (que tem fundamento teórico) e o modelo resultante é intencionalmente subótimo.196 Os coeficientes de regressão geralmente dependem do conjunto de variáveis do modelo e, portanto, podem mudam de valor (“mudança na estimativa” positiva ou negativa) se uma (ou mais) variável(is) for(em) eliminada(s) do modelo.196 31.3.3 Por que métodos de regressão gradual não são recomendados para seleção de variáveis em modelos de regressão multivariável? Métodos diferentes de regressão gradual podem produzir diferentes seleções de variáveis de um mesmo banco de dados.250 Nenhum método de regressão gradual garante a seleção ótima de variáveis de um banco de dados.250 As regras de término da regressão baseadas em P-valor tendem a ser arbitrárias.250 31.3.4 O que pode ser feito para reduzir o número de variáveis candidatas em modelos de regressão multivariável? Verifique a existência de multicolinearidade entre as variáveis candidatas.253 Figura 31.1: Multicolinearidade entre variáveis candidatas em modelos de regressão multivariável. Em caso de uma proporção baixa entre o número de participantes e de variáveis, use o conhecimento prévio da literatura para selecionar um pequeno conjunto de variáveis candidatas.253 Colapse categorias com contagem nula (células com valor igual a 0) de variáveis candidatas.253 Use simulações de dados para identificar qual(is) variável(is) está(ão) causando problemas de convergência do ajuste do modelo.253 A eliminação retroativa tem sido recomendada como a abordagem de regressão gradual mais confiável entre aquelas que podem ser facilmente alcançadas com programas de computador.196 Referências "],["árvore-de-decisão.html", "Capítulo 32 Árvore de decisão 32.1 Árvore de decisão", " Capítulo 32 Árvore de decisão 32.1 Árvore de decisão 32.1.1 O que é árvore de decisão? .REF? "],["aprendizagem-de-máquina.html", "Capítulo 33 Aprendizagem de máquina 33.1 Aprendizagem de máquina 33.2 Aprendizagem supervisionada 33.3 Aprendizagem não-supervisionada 33.4 Aprendizagem por reforço 33.5 Aprendizagem profunda", " Capítulo 33 Aprendizagem de máquina 33.1 Aprendizagem de máquina 33.1.1 O que é aprendizagem de máquina? .REF? 33.1.2 Quais são os principais algoritmos de aprendizagem de máquina? Regressão linear: .REF? Árvores de decisão: .REF? Máquinas de vetores de suporte: .REF? Regressão logística: .REF? K-médias: .REF? K-vizinhos mais próximos: .REF? Redes neurais: .REF? Florestas aleatórias: .REF? Análise de componentes principais: .REF? Naive Bayes: .REF? 33.2 Aprendizagem supervisionada .REF? 33.3 Aprendizagem não-supervisionada .REF? 33.4 Aprendizagem por reforço .REF? 33.5 Aprendizagem profunda .REF? "],["parte-4---metodologia-da-pesquisa-aplicada.html", "Parte 4 - Metodologia da Pesquisa Aplicada", " Parte 4 - Metodologia da Pesquisa Aplicada "],["delineamento-de-estudos.html", "Capítulo 34 Delineamento de estudos 34.1 Critérios de delineamento 34.2 Alocação 34.3 Cegamento 34.4 Pareamento 34.5 Aleatorização 34.6 Taxonomia de estudos", " Capítulo 34 Delineamento de estudos 34.1 Critérios de delineamento 34.1.1 Quais critérios são utilizados para classificar os delineamentos de estudos? .REF? 34.2 Alocação 34.2.1 O que é alocação? .REF? 34.3 Cegamento .REF? 34.3.1 O que é cegamento? 34.4 Pareamento 34.4.1 O que é pareamento? Pareamento significa que para cada participante de um grupo (por exemplo, com alguma condição clínica), existe um (ou mais) participantes (por exemplo, grupo controle) que possui características iguais ou similares relativas a algumas variáveis de interesse.254 As variáveis escolhidas para pareamento devem ter relação com as variáveis de desfecho, mas não são de interesse elas mesmas.254 O ajuste por pareamento deve ser incluído nas análises estatísticas mesmo que as variáveis de pareamento não sejam consideradas prognósticas ou confundidores na amostra estudada.254 A ausência de evidência estatística de diferença entre grupos não é considerada pareamento.254 34.5 Aleatorização 34.5.1 O que é aleatorização? .REF? 34.6 Taxonomia de estudos 34.6.1 Como podem ser classificados os estudos científicos? Estudos científicos podem ser classificados em básicos, observacionais, experimentais, acurácia diagnóstica, propriedades psicométricas, avaliação econômica e revisões de literatura:255–264 Estudos básicos256,261 Genética Celular Experimentos com animais Desenvolvimento de métodos Estudos de simulação computacional262,264 Estudos de propriedades psicométricas257,259 Validade Concordância Confiabilidade Estudos de desempenho diagnóstico260,263 Transversal Caso-Controle Comparativo Totalmente pareado Parcialmente pareado com subgrupo aleatório Parcialmente pareado com subgrupo não aleatório Não pareado aleatório Não pareado não aleatório Estudos observacionais256,261 Descritivo Estudo de caso Série de casos Transversal Analítico Transversal Caso-Controle Caso-Controle aninhado Caso-Coorte Coorte prospectiva ou retrospectiva Estudos quase-experimentais258 Quase-aleatorizado controlado Estimação de variável instrumental Descontinuidade de regressão Série temporal interrompida controlada Série temporal interrompida Diferença Estudos experimentais256,261 Fases I a IV Aleatorizado controlado Não-aleatorizado controlado Autocontrolado Cruzado Fatorial Campo Comunitário Estudos de avaliação econômica256 Análise de custo Análise de minimização de custo Análise de custo-utilidade Análise de custo-efetividade Análise de custo-benefício Estudos de revisão255 Estado-da-arte Narrativa Crítica Mapeamento Escopo Busca e revisão sistemática Sistematizada Sistemática Meta-análise Bibliométrica.265,266 Sistemática qualitativa Mista Visão geral Rápida Guarda-chuva Referências "],["tamanho-da-amostra.html", "Capítulo 35 Tamanho da amostra 35.1 Tamanho da amostra 35.2 Cálculo do tamanho da amostra 35.3 Perdas de amostra 35.4 Ajustes no tamanho da amostra 35.5 Justificativa do tamanho da amostra 35.6 SPARKing", " Capítulo 35 Tamanho da amostra 35.1 Tamanho da amostra 35.1.1 O que é tamanho da amostra? Tamanho da amostra \\(n\\) é a quantidade de participantes (ou unidades de análise) necessárias para conduzir um estudo a fim de testar uma hipótese.267 .13 35.1.2 Por que determinar o tamanho da amostra é importante? É virtualmente impossível, devido a limitações de recursos - tempo, acesso, custo, dentre outros - coletar dados da população completa.8 Uma amostra muito pequena para o estudo pode resultar em ajuste exagerado, imprecisão e baixo poder do teste.48 35.1.3 Quais fatores devem ser considerados para determinar o tamanho da amostra? Tamanho da população (\\(N\\)): O tamanho da amostra depende parcialmente do tamanho da população de origem. Geralmente assume-se que a população tem tamanho desconhecido ou infinito. Em alguns estudos serão amostradas populações de tamanho finito (inferior a 100.000 indivíduos), geralmente em pesquisas descritivas, em que esse tamanho deve ser incorporado nos cálculos.267 Delineamento do estudo.267 Quantidade e características (dependente vs. independente) dos grupos de participantes do estudo.267 Erros tipo I (\\(\\alpha\\)) e tipo II (\\(\\beta\\)).267 Tipo de variável a ser observada (contínua, intervalo, ordinal, nominal, dicotômica).267 Tamanho de efeito mínimo a ser observado.267 Variabilidade da(s) variável(eis) coletada(s).267 Lateralidade do teste de hipótese (uni- ou bicaudais).267 Perdas de dados durante a coleta e/ou acompanhamento dos participantes do estudo.267 O pacote pwr165 fornece a função plot.power.htest para apresentar graficamente a relação entre o tamanho da amostra e o poder de testes de hipóteses. 35.1.4 Quais aspectos éticos estão envolvidos no tamanho da amostra? Determinar a priori o tamanho da amostra pode diminuir o risco de realizar testes ou intervenções desnecessários, de desperdício de recursos (tempo e dinheiro) associados e, por outro lado, de coletar dados insuficientes para testar as hipóteses do estudo.267 O tratamento ético dos participantes do estudo, portanto, não exige que se considere se o poder do estudo é inferior à meta convencional de 80% ou 90%.268 Estudos com poder &lt;80% não são necessariamente antiéticos.268 Grandes estudos podem ser desejáveis por outras razões que não as éticas.268 35.2 Cálculo do tamanho da amostra 35.2.1 Como calcular o tamanho da amostra? O tamanho amostral pode ser calculado por meio de fórmulas matemáticas que tendem a assegurar margens de erros tipos I (\\(\\alpha\\)) e II (\\(\\beta\\)) para a estimação dos parâmetros populacionais (tamanho de efeito) a partir dos dados amostrais.267 O tamanho da amostra deve ser calculado para cada um dos objetivos primários e/ou secundários, sendo escolhido o maior tamanho de amostra calculado para o estudo.267 Geralmente é recomendado ser cético em relação às regras práticas para o tamanho da amostra, tais como a proporção entre o número de variáveis (ou eventos) e de participantes.48 35.2.2 Como especificar o tamanho do efeito esperado? Estudo-piloto — realizados nas mesmas condições do estudo, mas envolvendo um tamanho de amostra limitado — pode ser útil na estimativa do tamanho da amostra a partir do tamanho do efeito estimado.267 Utilizar os limites dos intervalos de confiança de estudos-piloto de ensaios clínicos como estimativa do tamanho do efeito pode aumentar o poder estatístico da análise se comparado ao uso das estimativas pontuais obtidas no mesmo piloto.269 Embora os testes de hipótese considerem efeito nulo para a hipótese nula — ex.: dferença de média (\\(H_{0}: \\mu_{1} - \\mu_{2}=0\\)), correlação (\\(H_{0}: r=0\\)), associação (\\(H_{0}: \\beta=0\\) ou \\(H_{0}: OR=1\\)) —, em geral é improvável que os efeitos populacionais sejam de fato nulos (isto é, exatamente 0).270 O pacote pwr165 fornece a função pwr.2p.test para cálculo do tamanho da amostra para testes de proporção balanceados (2 amostras com mesmo número de participantes). O pacote pwr165 fornece a função pwr.2p2n.test para cálculo do tamanho da amostra para testes de proporção não balanceados (2 amostras com diferente número de participantes). O pacote pwr165 fornece a função pwr.anova.test para cálculo do tamanho da amostra para testes de análise de variância balanceados (3 ou mais amostras com mesmo número de participantes). O pacote pwr165 fornece a função pwr.chisq.test para cálculo do tamanho da amostra para testes de qui-quadrado \\(\\chi^2\\). O pacote pwr165 fornece a função pwr.f2.test para cálculo do tamanho da amostra para testes com modelo linear geral. O pacote pwr165 fornece a função pwr.norm.test para cálculo do tamanho da amostra para a média de uma distribuição normal com variância conhecida. O pacote pwr165 fornece a função pwr.p.test para cálculo do tamanho da amostra para testes de proporção (1 amostra). O pacote pwr165 fornece a função pwr.r.test para cálculo do tamanho da amostra para testes de correlação (1 amostra). O pacote pwr165 fornece a função pwr.t.test para cálculo do tamanho da amostra para testes t de diferença de 1 amostra, 2 amostras dependentes ou 2 amostras independentes (grupos balanceados). O pacote pwr165 fornece a função pwr.t2n.test para cálculo do tamanho da amostra para testes t de diferença de 2 amostras independentes (grupos não balanceados). O pacote longpower166 fornece a função power.mmrm para calcular o tamanho da amostra para estudos com análises por modelo de regressão linear misto. 35.3 Perdas de amostra 35.3.1 O que é perda de amostra? Perda de amostra(s) — isto é, participante(s) ou unidade(s) de análise — pode ocorrer durante a coleta e/ou acompanhamento dos participantes do estudo.267 Perda amostral pode ocorrer por: abandono ou desistência do participante, perda de contato com o participante, perda de informação, ocorrência de eventos adversos, morte do participante, entre outros.267 35.3.2 Por que a perda de amostra é um problema? A perda de amostra pode levar a uma redução do poder do estudo, aumentando a probabilidade de erro tipo II (\\(\\beta\\)).REF? A perda de amostra pode levar a um viés de seleção, pois os participantes que permanecem no estudo podem ser diferentes daqueles que foram perdidos.REF? 35.3.3 Como evitar perda de amostra? A perda de amostra pode ser evitada por meio de um planejamento cuidadoso do estudo, incluindo a definição de critérios de inclusão e exclusão claros e apropriados, bem como a definição de estratégias para minimizar a perda de amostra.REF? A perda de amostra pode ser compensada pelo aumento do tamanho da amostra, desde que o aumento seja suficiente para manter o poder do estudo.267 35.4 Ajustes no tamanho da amostra 35.4.1 Por que ajustar o tamanho da amostra? O tamanho da amostra pode ser ajustado durante o estudo para compensar a perda de amostra, desde que o aumento seja suficiente para manter o poder do estudo.267 35.4.2 Como ajustar para perda amostral? Aumentar o tamanho da amostra estimada \\(n\\) pela porcentagem \\(d\\) de perdas esperada ou prevista, para obter o tamanho da amostra efetiva \\(n&#39;\\) com base na equação (35.1):267 \\[\\begin{equation} \\tag{35.1} n&#39; = \\dfrac{n}{1-d} \\end{equation}\\] 35.5 Justificativa do tamanho da amostra 35.5.1 Como justificar o tamanho da amostra de um estudo? Em estudos que envolvem condições raras, pode ser difícil recrutar o número necessário de participantes devido à limitada disponibilidade de casos da população. Mesmo assim, é aconselhável determinar o tamanho da amostra.267 Quando um estudo deste tipo não é possível, as considerações referentes ao tamanho da amostra são justificadas de acordo com o número máximo de pacientes que podem ser recrutados no decorrer do estudo.267 35.6 SPARKing 35.6.1 O que é SPARKing? SPARKing é um acrônimo para Sample size Planning After the Results are Known.271 SPARKing é uma mal prática que envolve o ajuste do tamanho da amostra após a coleta dos dados, com o objetivo de obter resultados estatisticamente significativos.271 SPARKing é considerado antiético, pois pode levar a resultados enviesados e não confiáveis, além de violar os princípios da pesquisa científica.271 Referências "],["simulação-computacional.html", "Capítulo 36 Simulação computacional 36.1 Características 36.2 Método de Monte Carlo 36.3 Diretrizes para redação", " Capítulo 36 Simulação computacional 36.1 Características 36.1.1 Quais são as características de estudos de simulação computacional? .REF? 36.2 Método de Monte Carlo 36.2.1 O que é o método de Monte Carlo? .REF? O pacote base52 fornece a função set.seed para especificar uma semente para reprodutibilidade de computações que envolvem números aleatórios. O pacote simstudy272 fornece as funções defData e genData para criar variáveis e simular um banco de dados de acordo com o delineamento pré-especificado, respectivamente. O pacote faux273 fornece a função sim_design para simular um banco de dados de acordo com o delineamento pré-especificado. O pacote InteractionPoweR168 fornece a função generate_interaction para simular bancos de dads com efeitos de interação. 36.3 Diretrizes para redação 36.3.1 Quais são as diretrizes para redação de estudos de simulação computacional? Visite a rede Enhancing the QUAlity and Transparency Of health Research EQUATOR Network para encontrar diretrizes específicas para cada tipo de estudo de simulação computacional. Reporting Guidelines for Health Care Simulation Research: Extensions to the CONSORT and STROBE Statements:274 https://www.equator-network.org/reporting-guidelines/reporting-guidelines-for-health-care-simulation-research-extensions-to-the-consort-and-strobe-statements/ Referências "],["propriedades-psicométricas.html", "Capítulo 37 Propriedades psicométricas 37.1 Características 37.2 Análise fatorial exploratória 37.3 Análise fatorial confirmatória 37.4 Validade de conteúdo 37.5 Validade de face 37.6 Validade do construto 37.7 Validade fatorial 37.8 Validade convergente 37.9 Validade discriminante 37.10 Validade de critério 37.11 Validade concorrente 37.12 Responsividade 37.13 Concordância 37.14 Confiabilidade 37.15 Diretrizes para redação", " Capítulo 37 Propriedades psicométricas 37.1 Características 37.1.1 O que são propriedades psicométricas? .REF? Tabela 37.1: Tabela de confusão sobre propriedades psicométricas de instrumentos. Concordância alta Concordância baixa Validade alta Adequado Inadequado Validade baixa Inadequado Inadequado O pacote lavaan275 fornece a função cfa para implementar modelos de análise fatorial confirmatória. O pacote lavaan275 fornece a função modificationIndices para calcular os índices de modificação. O pacote semTools276 fornece a função reliability para analisar a confiabilidade de um instrumento. O pacote psych277 fornece a função icc para calcular a confiabilidade utilizando coeficientes de correlação intraclasse. 37.2 Análise fatorial exploratória 37.2.1 O que é análise fatorial exploratória? .REF? 37.3 Análise fatorial confirmatória 37.3.1 O que é análise fatorial confirmatória? .REF? 37.4 Validade de conteúdo 37.4.1 O que é validade interna? .278 37.4.2 O que é validade externa? .278 37.4.3 Que fatores afetam a validade? A amostragem não probabilística pode dificultar a generalização dos achados da amostra para a população, diminuindo assim a validade externa do estudo.13 Quando as características da amostra obtida por seleção não probabilística forem similares às da população, a validade externa pode ser maior.13 37.4.4 Como avaliar a validade de um estudo? As características da amostra apresentadas na Tabela 1 são úteis para interpretação da validade interna e externa dos achados do estudo.146 37.5 Validade de face 37.5.1 O que é validade de face? .[RF] 37.6 Validade do construto 37.6.1 O que é construto? .[RF] 37.7 Validade fatorial 37.7.1 O que é validade fatorial? .[RF] 37.8 Validade convergente 37.8.1 O que é validade convergente? .[RF] 37.9 Validade discriminante 37.9.1 O que é validade discriminante? .[RF] 37.10 Validade de critério 37.10.1 O que é validade de critério? .[RF] 37.11 Validade concorrente 37.11.1 O que é concorrente? .[RF] 37.11.2 O que é validade concorrente? .[RF] 37.11.3 O que é validade preditiva? .[RF] 37.12 Responsividade 37.12.1 O que é responsividade? .REF? 37.13 Concordância 37.13.1 O que é concordância? .REF? 37.13.2 Quais métodos são adequados para análise de concordância de variáveis dicotômicas? Coeficiente de Cohen \\(\\kappa\\): mede a concordância corrigida pelo acaso.279,280 Tabela 37.2: Tabela de confusão 2x2 para análise de concordância de testes e variáveis dicotômicas. Teste positivo Teste negativo Total Teste positivo \\(a\\) \\(b\\) \\(g=a+b\\) Teste negativo \\(c\\) \\(d\\) \\(h=c+d\\) Total \\(e=a+c\\) \\(f=b+d\\) \\(N=a+b+c+d\\) Coeficiente de correlação tetracórica \\(r_{tet}\\).281,282 O pacote psych277 fornece a função tetrachoric para calcular o coeficiente de correlação tetracórica (\\(r_{tet}\\)). 37.13.3 Quais métodos não são adequados para análise de concordância de variáveis dicotômicas? Concordância absoluta \\(C_{A}\\) - quantidade de casos em que examinadores concordam - não é recomendada porque não corrige a estimativa para possíveis concordâncias ao acaso.282 Concordância percentual \\(C_{\\%}\\) - proporção de casos em que examinadores concordam pela quantidade total de casos - não é recomendada porque não corrige a estimativa para possíveis concordâncias ao acaso.282 Qui-quadrado \\(\\chi^2\\) a partir da tabela de contigência não é recomendado porque tal teste analisa associação.282 A família de coeficientes de Cohen \\(\\kappa\\) não é adequada para analisar concordância quando as variáveis são aparentemente (e não originalmente) dicotômicas.282 37.13.4 Quais métodos são adequados para análise de concordância de variáveis categóricas? Coeficiente de Cohen \\(\\kappa\\): mede a concordância corrigida pelo acaso.279,280 Coeficiente de Cohen ponderado \\(\\kappa_{w}\\): mede a concordância corrigida pelo acaso.279,280 Tabela 37.3: Tabela de confusão 3x3 para análise de concordância de testes e variáveis dicotômicas. Grave Moderado Leve Total Grave \\(a\\) \\(b\\) \\(c\\) \\(j=a+b+c\\) Moderado \\(d\\) \\(e\\) \\(f\\) \\(k=d+e+f\\) Leve \\(g\\) \\(h\\) \\(i\\) \\(l=g+h+i\\) Total \\(j=a+d+g\\) \\(k=b+e+h\\) \\(l=c+f+i\\) \\(N=a+b+c+d+e+f+g+h+i\\) Coeficiente de correlação policórica \\(r_{pol}\\).282 O pacote psych277 fornece a função tetrachoric para calcular o coeficiente de correlação policórica (\\(r_{pol}\\)). 37.13.5 Quais métodos são adequados para análise de concordância de variáveis categóricas e contínuas? Coeficiente de correlação bisserial \\(r_{s}\\).282 O pacote psych277 fornece a função tetrachoric para calcular o coeficiente de correlação bisserial (\\(r_{s}\\)). 37.13.6 Quais métodos são adequados para análise de concordância de variáveis ordinais? Coeficiente de Cohen ponderado \\(\\kappa_{w}\\): mede a concordância corrigida pelo acaso.279,280 37.13.7 Quais métodos são adequados para análise de concordância de variáveis contínuas? Gráfico de dispersão com a reta de regressão.46 Gráfico de limites de concordância (média dos testes vs. diferença entre testes) com a reta de regressão do viés e respectivo no nível de significância \\(\\alpha\\) pré-estabelecido.46 O pacote BlandAltmanLeh283 fornece as funções bland.altman.stats e bland.altman.plot para calcular e apresentar, respectivamente, o gráfico com os limites de concordância entre dois métodos. 37.13.8 Quais métodos não são adequados para análise de concordância de variáveis contínuas? Comparação de médias: dois métodos apresentarem médias similares - isto é, ‘sem diferença estatística’ após um teste inferencial de hipótese nula \\(H_{0}:\\mu_{1} = \\mu_{2}\\) - não informa sobre a concordância deles. Métodos com maior erro de medida tendem a ter menos chance de rejeição da hipótese nula.46 Correlação bivariada: o coeficiente de correlação dependente tanto da variação entre indivíduos (isto é, entre os valores verdadeiros) quanto da variação intraindividual (isto é, erro de medida). Se a variância dos erros de medida de ambos os métodos não for pequena comparadas à variância dos valores verdadeiros, o tamanho do efeito da correlação será pequeno mesmo que os métodos possuam boa concordância.46 Regressão linear: o teste da hipótese nula da inclinação da reta de regressão (\\(H_{0}:\\beta = 0\\)) é equivalente a testar a correlação bivariada (\\(H_{0}:\\rho = 0\\)).46 37.13.9 Quais métodos são adequados para modelagem de concordância? Modelo log-linear.282 37.14 Confiabilidade 37.14.1 O que é confiabilidade? .REF? 37.14.2 Quais métodos são adequados para análise de confiabilidade? .REF? 37.15 Diretrizes para redação 37.15.1 Quais são as diretrizes para redação de estudos de propriedades psicométricas? Visite a rede Enhancing the QUAlity and Transparency Of health Research EQUATOR Network para encontrar diretrizes específicas para cada tipo de estudo de propriedades psicométricas. COSMIN reporting guideline for studies on measurement properties of patient-reported outcome measures:284 https://www.equator-network.org/reporting-guidelines/cosmin-reporting-guideline-for-studies-on-measurement-properties-of-patient-reported-outcome-measures/ Recommendations for reporting the results of studies of instrument and scale development and testing:285 https://www.equator-network.org/reporting-guidelines/recommendations-for-reporting-the-results-of-studies-of-instrument-and-scale-development-and-testing/ Guidelines for reporting reliability and agreement studies (GRRAS) were proposed:286 https://www.equator-network.org/reporting-guidelines/guidelines-for-reporting-reliability-and-agreement-studies-grras-were-proposed/&gt; Referências "],["desempenho-diagnóstico.html", "Capítulo 38 Desempenho diagnóstico 38.1 Características 38.2 Tabelas 2x2 38.3 Gráficos crosshair 38.4 Curvas ROC 38.5 Interpretação da validade de um teste 38.6 Diretrizes para redação", " Capítulo 38 Desempenho diagnóstico 38.1 Características 38.1.1 Quais são as características de estudos de desempenho diagnóstico? .REF? 38.2 Tabelas 2x2 38.2.1 O que é uma tabela de confusão 2x2? Tabela de confusão é uma matriz de 2 linhas por 2 colunas que permite analisar o desempenho de classificação de uma variável dicotômica (padrão-ouro ou referência) versus outra variável dicotômica (novo teste).287 38.2.2 Como analisar o desempenho diagnóstico em tabelas 2x2? Verdadeiro-positivo (\\(VP\\)): caso com a condição presente e corretamente identificado como tal.288 Falso-negativo (\\(FN\\)): caso com a condição presente e erroneamente identificado como ausente.288 Verdadeiro-negativo (\\(VN\\)): controle sem a condição presente e corretamente identificados como tal.288 Falso-positivo (\\(FP\\)): controle sem a condição presente e erroneamente identificado como presente.288 Tabela 38.1: Tabela de confusão 2x2 para análise de desempenho diagnóstico de testes e variáveis dicotômicas. Condição presente Condição ausente Total Teste positivo \\(VP\\) \\(FP\\) \\(VP+FP\\) Teste negativo \\(FN\\) \\(VN\\) \\(FN+VN\\) Total \\(VP+FN\\) \\(FP+VN\\) \\(N=VP+VN+FP+FN\\) Tabelas de confusão também podem ser visualizadas em formato de árvores de frequência.287 Figura 38.1: Árvore de frequência do desempenho diagnóstico de uma tabela de confusão 2x2 representando um método novo (dicotômico) comparado ao método padrão-ouro ou referência (dicotômico). O pacote riskyr289 fornece a função plot_prism para construir árvores de frequência a partir de diferentes cenários. 38.2.3 Quais probabilidades caracterizam o desempenho diagnóstico de um teste em tabelas 2x2? Sensibilidade (\\(SEN\\)), equação (38.1): Proporção de verdadeiro-positivos dentre aqueles com a condição.288 \\[\\begin{equation} \\tag{38.1} SEN = \\dfrac{VP}{VP+FN} \\end{equation}\\] Especificidade (\\(ESP\\)), equação (38.2): Proporção de verdadeiro-negativos dentre aqueles sem a condição.288 \\[\\begin{equation} \\tag{38.2} ESP = \\dfrac{VN}{VN+FP} \\end{equation}\\] Acurácia (\\(ACU\\)), equação (38.3): Proporção de casos e controle corretamente identificados.288 \\[\\begin{equation} \\tag{38.3} ACU = \\dfrac{VP+VN}{VP+VN+FP+FN} \\end{equation}\\] Valor preditivo positivo (\\(VPP\\)), equação (38.4): Proporção de casos corretamente identificados como verdadeiro-positivos.288 \\[\\begin{equation} \\tag{38.4} VPP = \\dfrac{VP}{VP+FP} \\end{equation}\\] Valor preditivo negativo (\\(VPN\\)), equação (38.5): Proporção de controles corretamente identificados como verdadeiro-negativos.288 \\[\\begin{equation} \\tag{38.5} VPN = \\dfrac{VN}{VN+FN} \\end{equation}\\] Tabela 38.2: Probabilidades calculados a partir da tabela de confusão 2x2 para análise de desempenho diagnóstico de testes e variáveis dicotômicas. Condição presente Condição ausente Total Probabilidades Teste positivo \\(VP\\) \\(FP\\) \\(VP+FP\\) \\(VPP = \\frac{VP}{VP+FP}\\) Teste negativo \\(FN\\) \\(VN\\) \\(FN+VN\\) \\(VPN = \\frac{VN}{VN+FN}\\) Total \\(VP+FN\\) \\(FP+VN\\) \\(N=VP+VN+FP+FN\\) Probabilidades \\(SEN = \\frac{VP}{VP+FN}\\) \\(ESP = \\frac{VN}{VN+FP}\\) \\(ACU = \\frac{VP+VN}{VP+VN+FP+FN}\\) O pacote riskyr289 fornece a função comp_prob para estimar 13 probabilidades relacionadas ao desempenho diagnóstico em tabelas 2x2. O pacote caret290 fornece a função confusionMatrix para estimar 11 probabilidades relacionadas ao desempenho diagnóstico em tabelas 2x2. 38.3 Gráficos crosshair 38.3.1 O que um gráfico crosshair? .291 O pacote mada292 fornece a função crosshair para criar um gráfico crosshair291 a partir de dados de verdadeiro-positivo, falso-positivo, verdadeiro-negativo e verdadeiro-positivo de tabelas de confusão 2x2. 38.4 Curvas ROC 38.4.1 O que é a área sob a curva (AUROC)? A área sob a curva ROC (AUC ou AUROC) quantifica o poder de discriminação ou desempenho diagnóstico na classificação de uma variável dicotômica.293 O pacote proc294 fornece a função plot.roc para criar uma curva ROC. 38.4.2 Como interpretar a área sob a curva (ROC)? A área sob a curva AUC varia no intervalo \\([0.5; 1]\\), com valores mais elevados indicando melhor discriminação ou desempenho do modelo de classificação.293 As interpretações qualitativas (isto é: pobre/fraca/baixa, moderada/razoável/aceitável, boa ou muito boa/alta/excelente) dos valores de área sob a curva são arbitrários e não devem ser considerados isoladamente.293 Modelos de classificação com valores altos de área sob a curva podem ser enganosos se os valores preditos por esses modelos não estiverem adequadamente calibrados.293 38.4.3 Como analisar o desempenho diagnóstico em desfechos com distribuição trimodal na população? Limiares duplos podem ser utilizados para análise de desempenho diagnóstico de testes com distribuição trimodal.295 38.5 Interpretação da validade de um teste 38.5.1 Que itens devem ser verificados na interpretação de um estudo de validade? O novo teste foi comparado junto ao método padrão-ouro.288 As probabilidades pontuais estimadas que caracterizam o desempenho diagnóstico do novo teste são altas e adequadas para sua aplicação clínica.288 Os intervalos de confiança estimados para as probabilidades do novo teste são estreitos e adequadas para sua aplicação clínica.288 O novo teste possui adequada confiabilidade intra/inter examinadores.288 O estudo de validação incluiu um espectro adequado da amostra.288 Todos os participantes realizaram ambos o novo teste e o padrão-ouro no estudo de validação.288 Os examinadores do novo teste estavam cegados para o resultado do teste padrão-ouro.288 38.6 Diretrizes para redação 38.6.1 Quais são as diretrizes para redação de estudos diagnósticos? Visite a rede Enhancing the QUAlity and Transparency Of health Research EQUATOR Network para encontrar diretrizes específicas para cada tipo de estudo de desempenho diagnóstico. STARD 2015: An Updated List of Essential Items for Reporting Diagnostic Accuracy Studies:296 https://www.equator-network.org/reporting-guidelines/stard/ Referências "],["estudos-observacionais.html", "Capítulo 39 Estudos observacionais 39.1 Características 39.2 Diretrizes para redação", " Capítulo 39 Estudos observacionais 39.1 Características 39.1.1 Quais são as características de estudos observacionais? .REF? 39.2 Diretrizes para redação 39.2.1 Quais são as diretrizes para redação de estudos observacionais? Visite a rede Enhancing the QUAlity and Transparency Of health Research EQUATOR Network para encontrar diretrizes específicas para cada tipo de estudo observacional. The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) Statement: guidelines for reporting observational studies:297 https://www.equator-network.org/reporting-guidelines/strobe/ Referências "],["ensaios-quase-experimentais.html", "Capítulo 40 Ensaios quase-experimentais 40.1 Características 40.2 Diretrizes para redação", " Capítulo 40 Ensaios quase-experimentais 40.1 Características 40.1.1 Quais são as características de ensaios quase-experimentais? .REF? 40.2 Diretrizes para redação 40.2.1 Quais são as diretrizes para redação de ensaios quase-experimentais? Visite a rede Enhancing the QUAlity and Transparency Of health Research EQUATOR Network para encontrar diretrizes específicas para cada tipo de estudo de ensaio quase-experimental. Guidelines for reporting non-randomised studies:298 https://www.equator-network.org/reporting-guidelines/guidelines-for-reporting-non-randomised-studies/ Referências "],["ensaios-experimentais.html", "Capítulo 41 Ensaios experimentais 41.1 Ensaio clínico aleatorizado 41.2 Modelos de análise de comparação 41.3 Comparação na linha de base 41.4 Comparação intragrupos 41.5 Comparação entre grupos 41.6 Comparação de subgrupos 41.7 Efeito de interação 41.8 Ajuste de covariáveis 41.9 Imputação de dados perdidos 41.10 Diretrizes para redação", " Capítulo 41 Ensaios experimentais 41.1 Ensaio clínico aleatorizado 41.1.1 Quais são as características de ensaios clínicos aleatorizados? A característica essencial de um ensaio clínico aleatorizado é a comparação entre grupos.299 Quanto à unidade de alocação:300 Individual Agrupado Quanto ao número de braços:300 Único* Múltiplos Quanto ao número de centros:300 Único Múltiplos Quanto ao cegamento:300 Aberto* Simples-cego Duplo-cego Tripo-cego Quádruplo-cego Quanto à alocação:300 Sem sorteio Estratificada (centro apenas) Estratificada Minimizada Estratificada e minimizada 41.2 Modelos de análise de comparação 41.2.1 Que modelos podem ser utilizados para comparações? As abordagens compreendem a comparação da variável de desfecho medida entre os momentos antes e depois ou da sua mudança (pré - pós) entre os momentos.301 Se a média da variável é igual entre grupos no início do acompanhamento, ambas abordagens estimam o mesmo efeito. Caso contrário, o efeito será influenciado pela correlação entre as medidas antes e depois. A análise da mudança não controla para desbalanços no início do estudo.301 A abordagem mais recomendada é a análise de covariância (ANCOVA) - equação (41.1) - pois ajusta os valores pós-intervenção (\\(Y_{ij}\\)) aos valores pré-intervenção (\\(X_{ij}\\)) para cada participante (\\(i\\)) de cada grupo {\\(Z_{ij}\\)}, e portanto não é afetada pelas diferenças entre grupos no início do estudo.10,301 \\[\\begin{equation} \\tag{41.1} Y_{ij} = \\beta_0 + \\beta_1 X_{ij} + \\beta_2 Z_j + \\epsilon_{ij} \\end{equation}\\] A ANCOVA modelando seja a mudança (pré - pós: \\(\\Delta = X_{ij} - Y_{ij}\\)) quando o desfecho no pós-tratamento parece ser o método mais efetivo considerando-se o viés de estimação dos parâmetros, a precisão das estimativas, a cobertura nominal (isto é, intervalo de confiança) e o poder do teste.302 Quando a ANCOVA - equação (41.2) - é utilizada com a mudança (pré - pós) com variável de desfecho (\\(Y_{ij}\\)), o coeficiente de regressão \\(\\beta_1\\) é diminuído em 1 unidade.10,303 \\[\\begin{equation} \\tag{41.2} (X_{ij} - Y_{ij}) = \\beta_0 + \\beta_1 Z_j + \\epsilon_{ij} \\end{equation}\\] Análise de variância (ANOVA) e modelos lineares mistos (MLM) são outras opções de métodos, embora apresentem maior variância, menor poder, e cobertura nominal comparados à ANCOVA.302 .304 .305 41.3 Comparação na linha de base 41.3.1 O que são dados na linha de base? Dados sociodemográficos, clínicos e funcionais são coletados na linha de base sobre cada participante no momento da aleatorização.306 Os dados de linha de base são usados para caracterizar os pacientes incluídos no estudo e para mostrar que os grupos de tratamento estão bem equilibrados.306 Dados da linha de base podem ser utilizados para a aleatorização de modo a equilíbrar ou estratificar os grupos considerando alguns fatores-chave.306 Dados da linha de base podem ser utilizados como ajuste de covariável para análise do resultado por grupo de tratamento.306 41.3.2 O que é comparação entre grupos na linha de base em ensaios clínicos aleatorizados? A comparação se refere ao teste de hipótese nula de não haver diferença (‘balanço’ ou ‘equilíbrio’) entre grupos de tratamento nas (co)variáveis na linha de base, geralmente apresentadas apenas de modo descritivo na ‘Tabela 1’.307 A interpretação isolada do P-valor da comparação entre grupos na linha de base não permite identificar as razões para eventuais diferenças.307 41.3.3 Para quê comparar grupos na linha de base em ensaios clínicos aleatorizados? Os P-valores estão relacionados à aleatorização dos participantes em grupos.308 Em ensaios clínicos aleatorizados, a comparação de (co)variáveis na linha de base é usada para avaliar se aleatorização foi ‘bem sucedida’.308 41.3.4 Quais são as razões para diferenças entre grupos de tratamento nas (co)variáveis na linha de base? Acaso.147,307 Viés.147,307 Tamanho da amostra.147,307 Má conduta científica.147 41.3.5 Quais cenários permitem a comparação entre grupos na linha de base em ensaios clínicos aleatorizados? Em ensaios clínicos aleatorizados agregados, os P-valores possuem interpretação diferente de estudos aleatorizados individualmente.308 Em ensaios clínicos com agrupamento, nos quais o recrutamento ocorreu após a aleatorização, os P-valores já não estão inteiramente relacionados ao processo de aleatorização, mas sim ao método de recrutamento, o que pode resultar na comparação de amostras não aleatórias.308 41.3.6 Por que não se deve comparar grupos na linha de base em ensaios clínicos aleatorizados? A interpretação errônea dos P-valores na comparação entre grupos, na linha de base, de um ensaio clínico aleatorizado constitui a ‘falácia da Tabela 1’.148 Quando a aleatorização é bem-sucedida, a hipótese nula de diferença entre grupos na linha de base é verdadeira.309 Testes de significância estatística na linha de base avaliam a probabilidade de que as diferenças observadas possam ter ocorrido por acaso; no entanto, já sabemos - pelo delineamento do experimento - que quaisquer diferenças são causadas pelo acaso.310 41.3.7 Quais estratégias podem ser adotadas para substituir a comparação entre grupos na linha de base em ensaios clínicos aleatorizados? Na fase de projeto: identifique as variáveis prognósticas do desfecho de acordo com a literatura.309 Na fase de análise: inclua as variáveis prognósticas nos modelos para ajuste.309 41.4 Comparação intragrupos 41.4.1 Por que não se deve comparar intragrupos (pré - pós) em ensaios clínicos aleatorizados? Testar por mudanças a partir da linha de base separadamente em cada grupos aleatorizados não permite concluir sobre diferenças entre grupos; não se pode fazer inferências a partir da comparação de P-valores.299 41.5 Comparação entre grupos 41.5.1 O que é comparação entre grupos em ensaios clínicos aleatorizados? A comparação se refere ao teste de hipótese nula de não haver diferença (‘alteração’ ou ‘mudança’) pós-tratamento entre grupos de tratamento.299 41.5.2 O que pode ser comparado entre grupos? Valores pós-tratamento; mudança entre linha de base e pós-tratamento; mudança percentual da linha de base.311 41.5.3 Qual é a comparação entre grupos mais adequada em ensaios clínicos aleatorizados? Análise de covariância (ANCOVA) que analisa o pós-tratamento com a linha de base como covariável é a opção que possui maior poder estatístico.311 Mudança entre linha de base e pós-tratamento tem poder adequado apenas quando a correlação entre linha de base e pós-tratamento é alta.311 Mudança percentual da linha de base é a opção menos eficiente em termos de poder estatístico.311 41.6 Comparação de subgrupos 41.6.1 O que é comparação de subgrupos em ensaios clínicos aleatorizados? Análises de subgrupos podem ser realizadas para avaliar se as diferenças no resultado do tratamento (ou a falta delas) dependem de algumas características na linha de base dos pacientes.306 41.6.2 Como realizar a comparação de subgrupos em ensaios clínicos aleatorizados? Testes estatísticos de interação (que avaliam se um efeito de tratamento difere entre subgrupos) devem ser usados, e não apenas a inspeção dos P-valores do subgrupo. Somente se o teste de interação estatística apoiar um efeito de subgrupo as conclusões poderão ser influenciadas.306,312 41.6.3 Como interpretar a comparação de subgrupos em ensaios clínicos aleatorizados? Análises de subgrupos devem ser consideradas de natureza exploratória e raramente elas afetam as conclusões obtidas a partir do estudo.306,312 A credibilidade das análises de subgrupos é melhor se restrita ao desfecho primário e a alguns subgrupos predefinidos e baseadas em hipóteses biologicamente plausíveis.306 Deve-se verificar se o estudo possui poder estatístico suficiente para detectar tamanhos de efeitos realistas em subgrupos e interpretar com cautela uma diferença de tratamento em um subgrupo quando a comparação global do tratamento não é significativa.306 41.7 Efeito de interação 41.7.1 Por que analisar o efeito de interação? Em ensaios clínicos aleatorizados, o principal problema de pesquisa é se há uma diferença pré - pós maior em um grupo do que em outro(s).299 A comparação de subgrupos por meio de testes de significância de hipótese nula separados é enganosa por não testar (comparar) diretamente os tamanhos dos efeitos dos tratamentos.313 .184 41.7.2 Quando usar o termo de interação? Análise de efeito de interação pode ser usada para testar se o efeito de um tratamento varia entre dois ou mais subgrupos de indivíduos, ou seja, se um efeito é modificado pelo(s) outros(s) efeito(s).185 A interação entre duas (ou mais) variáveis pode ser utilizada para comparar efeitos do tratamento em subgrupos de ensaios clínicos.314 O poder estatístico para detectar efeitos de interação é limitado.314 41.8 Ajuste de covariáveis 41.8.1 Quais variáveis devem ser utilizadas no ajuste de covariáveis? A escolha das características de linha de base pelas quais uma análise é ajustada deve ser determinada pelo conhecimento prévio de uma possível influência no resultado, em vez da evidência de desequilíbrio entre os grupos de tratamento no estudo.309 41.8.2 Quais os benefícios do ajuste de covariáveis? Ajustar por covariáveis ajuda a estimar os efeitos do tratamento para o indivíduo, assim como aumenta a eficiência dos testes para hipótese nula e a validade externa do estudo.315 Incluir a variável de desfecho medida na linha de base como covariável - independentemente de a análise ser realizada com a medida pós-tratamento da mesma variável ou a diferença para a linha de base - pode aumentar o poder estatístico do estudo.316 Incluir outras variáveis medidas na linha de base, com potencial para serem desbalanceadas entre grupos após a aleatorização, diminui a chance de afetar as estimativas de efeito dos tratamentos.316 41.8.3 Quais os riscos do ajuste de covariáveis? Incluir covariáveis que não são prognósticas do desfecho pode reduzir o poder estatístico do estudo.316 Incluir covariáveis com dados perdidos pode reduzir o tamanho amostral e consequentemente o poder estatístico do estudo (análise de casos completos) ou levar a desvios do plano de análise por exclusão de covariáveis prognósticas.316 41.9 Imputação de dados perdidos 41.9.1 Como lidar com os dados perdidos em desfechos? Em dados longitudinais com um pequeno número de ‘ondas’ (medidas repetidas) e poucas variáveis, para análise com modelos de regressão univariados, a imputação paramétrica via especificação condicional completa - também conhecido como imputação multivariada por equações encadeadas (multivariate imputation by chained equations, MICE) - é eficiente do ponto de vista computacional e possui acurácia e precisão para estimação de parâmetros.57,317 Para dados perdidos em desfechos dicotômicos, o desempenho dos métodos de imputação multivariada por equações encadeadas (multivariate imputation by chained equations, MICE)64 e por correspondência média preditiva (predictive mean matching, PMM)65,66 é similar.63 41.9.2 Como lidar com os dados perdidos em covariáveis? Imputação de dados perdidos de uma covariável pela média dos dados do respectivo grupo permite estimativas não enviesadas dos efeitos do tratamento, preserva o erro tipo I e aumenta o poder estatístico comparado à análise de dados completos.316 Os pacotes mice64 e miceadds67 fornecem funções mice e mi.anova para imputação multivariada por equações encadeadas, respectivamente, para imputação de dados. 41.10 Diretrizes para redação 41.10.1 Quais são as diretrizes para redação de ensaios experimentais? Visite a rede Enhancing the QUAlity and Transparency Of health Research EQUATOR Network para encontrar diretrizes específicas para cada tipo de ensaio experimental. CONSORT 2010 Statement: updated guidelines for reporting parallel group randomised trials:318 https://www.equator-network.org/reporting-guidelines/consort/ O pacote consort319 fornece a função consort_plot para elaboração do fluxograma de ensaios experimentais no formato padrão. Referências "],["meta-análise.html", "Capítulo 42 Meta-análise 42.1 Características 42.2 Interpretação de efeitos em meta-análise 42.3 Diretrizes para redação", " Capítulo 42 Meta-análise 42.1 Características 42.1.1 O que é meta-análise? .REF? O pacote metagear320 fornece a função plot_PRISMA para gerar o fluxograma de uma revisão sistemática de acordo com o Preferred Reporting Items for Systematic Reviews and Meta-Analyses321. O pacote PRISMA2020322 fornece a função PRISMA_flowdiagram para elaboração do fluxograma de revisões sistemáticas no formato padrão. 42.2 Interpretação de efeitos em meta-análise 42.2.1 Como avaliar a variação do tamanho do efeito? O intervalo de predição contém informação sobre a variação do tamanho do efeito.323 Se o intervalo de predição não contém a hipótese nula (\\(H_{0}\\)), podemos concluir que (a) o tratamento funciona igualmente bem em todas as populações, ou que ele funciona melhor em algumas populações do que em outras.323 Se o intervalo de predição contém a hipótese nula (\\(H_{0}\\)), podemos concluir que o tratamento pode ser benéfico em algumas populações, mas prejudicial em outras, de modo que a estimativa pontual (geralmente a média) torna-se amplamente irrelevante. Nesse caso, é recomendado investigar em que populações o tratamento seria benéfico e em quais causaria danos.323 42.2.2 Como avaliar a heterogeneidade entre os estudos? A heterogeneidade - variação não-aleatória - no efeito do tratamento entre os estudos incluídos em uma meta-análise pode ser avaliada pelo \\(I^{2}\\).323,324 \\(I^{2}\\) representa qual proporção da variância observada reflete a variância nos efeitos verdadeiros em vez do erro de amostragem.323 \\(I^{2}\\) mede a proporção da variância total que pode ser atribuída à heterogeneidade entre os estudos incluídos.324 \\(I^{2}\\) não depende da quantidade de estudos incluídos na meta-análise. Entretanto, \\(I^{2}\\) aumenta com a quantidade de participantes incluídos nos estudos meta-analisados.324 A heterogeneidade entre estudos é explicada de modo mais confiável utilizando dados de pacientes individuais, uma vez que a direção verdadeira da modificação de efeito não pode ser observada a partir de dados agregados no estudo.325 O pacote psychmeta237 fornece a função ma_d para meta-analisar valores d. O pacote psychmeta237 fornece a função ma_r para meta-analisar correlações. O pacote psychmeta237 fornece a função plot_forest para criar figuras tipo forest plot. O pacote psychmeta237 fornece a função plot_funnel para criar figuras tipo funnel plot. 42.3 Diretrizes para redação 42.3.1 Quais são as diretrizes para redação de meta-análises? Visite a rede Enhancing the QUAlity and Transparency Of health Research EQUATOR Network para encontrar diretrizes específicas para cada tipo de meta-análises. The PRISMA 2020 statement: An updated guideline for reporting systematic reviews:326 https://www.equator-network.org/reporting-guidelines/prisma/ Referências "],["redação-de-resultados.html", "Capítulo 43 Redação de resultados 43.1 Resultados da análise estatística 43.2 Diretrizes e Listas", " Capítulo 43 Redação de resultados 43.1 Resultados da análise estatística 43.1.1 Como redigir os resultados da análise estatística? .REF? O pacote report327 fornece a função report para redigir a descrição de diversas análises estatísticas. 43.2 Diretrizes e Listas 43.2.1 Quais diretrizes estão disponíveis para redação estatística? Review of guidance papers on regression modeling in statistical series of medical journals.328 Principles and recommendations for incorporating estimands into clinical study protocol templates.329 How to write statistical analysis section in medical research.226 Recommendations for Statistical Reporting in Cardiovascular Medicine: A Special Report From the American Heart Association.330 Framework for the treatment and reporting of missing data in observational studies: The Treatment And Reporting of Missing data in Observational Studies framework.331 Guidelines for reporting of figures and tables for clinical research in urology.332 Who is in this study, anyway? Guidelines for a useful Table 1.149 Guidelines for Reporting of Statistics for Clinical Research in Urology.333 Reveal, Don’t Conceal: Transforming Data Visualization to Improve Transparency.155 Guidelines for the Content of Statistical Analysis Plans in Clinical Trials.334 Basic statistical reporting for articles published in Biomedical Journals: The ‘’Statistical Analyses and Methods in the Published Literature’’ or the SAMPL Guidelines.335 Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm.336 STRengthening analytical thinking for observational studies: the STRATOS initiative.337 Research methods and reporting.338 How to ensure your paper is rejected by the statistical reviewer.339 43.2.2 Quais listas de verificação estão disponíveis para redação estatística? A CHecklist for statistical Assessment of Medical Papers (the CHAMP statement): explanation and elaboration.340 Checklist for clinical applicability of subgroup analysis.341 Evidence-based statistical analysis and methods in biomedical research (SAMBR) checklists according to design features.225 Referências "],["bibliografia.html", "Bibliografia", " Bibliografia "],["shiny-apps.html", "Shiny Apps Aplicativos por delineamento de estudo", " Shiny Apps Aplicativos por delineamento de estudo Ensaios clínicos RCTapp Figura 43.1: RCTapp: Shiny app para análise de ensaios clínicos aleatorizados. "],["fontes-externas.html", "Fontes externas Fontes de informação externas", " Fontes externas Fontes de informação externas American Heart Association Statistical Reporting Recommendations - AHA/ASA journals American Physiological Society Statistics Exploration in Statistics General Statistics Reporting Statistics American Statistical Association Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05 - The American Statistical Association British Medicine Journal Statistics - Latest from The BMJ Statistics notes - Latest from The BMJ Statistics and research methods - Latest from The BMJ Statistics at Square One Research methods &amp; reporting Enhancing the QUality And Transparency Of health Research Network Enhancing the Quality and Transparency of health research EQUATOR Network Journal of the Amercan Medical Association JAMA Guide to Statistics and Methods - JAMA Nature Publishing Group Statistics for Biologists - Nature Publising Group Oxford Reference A Dictionary of Statistics Royal Statistical Society Best Practices for Data Visualisation - Royal Statistical Society Statistics in Medicine Tutorials in Biostatistics Papers BMC Trials Design and analysis of n-of-1 trials The Journal of Applied Statistics in the Pharmaceutical Industry Tutorial Papers "],["referências.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
